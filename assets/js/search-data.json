{
  
    
        "post0": {
            "title": "Bank Loan Prediction",
            "content": "Installations . !pip install boruta . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: boruta in /usr/local/lib/python3.7/dist-packages (0.3) Requirement already satisfied: scikit-learn&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.0.2) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.7.3) Requirement already satisfied: numpy&gt;=1.10.4 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.21.6) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.17.1-&gt;boruta) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.17.1-&gt;boruta) (3.1.0) . !pip install hyperopt . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2) Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.3) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.21.6) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.7.3) Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.1.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.64.0) Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0) . !pip install catboost . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.0.6) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2022.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.4.3) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.9) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;catboost) (4.1.1) Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (8.0.1) . !pip install shap . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (0.41.0) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5) Requirement already satisfied: packaging&gt;20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6) Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap) (0.0.7) Requirement already satisfied: tqdm&gt;4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.0) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0) Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;20.9-&gt;shap) (3.0.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba-&gt;shap) (57.4.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba-&gt;shap) (0.34.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;shap) (2022.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;shap) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;shap) (1.15.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;shap) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;shap) (1.1.0) . Importing libraries . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from xgboost import XGBClassifier from boruta import BorutaPy from hyperopt import tpe, hp, fmin, STATUS_OK,Trials from hyperopt.pyll.base import scope from sklearn.model_selection import cross_val_score from sklearn.metrics import classification_report from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import f1_score from hyperopt import space_eval from catboost import CatBoostClassifier import joblib import warnings warnings.filterwarnings(&quot;ignore&quot;) import shap shap.initjs() . Data preprocessing and EDA . credit=pd.read_csv(&quot;credit_applications.csv&quot;) . cust=pd.read_csv(&quot;customers.csv&quot;) . cust.head() . Unnamed: 0 client_nr yearmonth total_nr_trx nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance CRG . 0 1 | 1 | 201401 | 97 | 50 | 6527929 | 47 | 7454863 | -7914288 | 25110651 | 1.0 | . 1 2 | 1 | 201402 | 88 | 59 | 3475918 | 29 | 1895848 | -8448513 | 25036651 | 1.0 | . 2 3 | 1 | 201403 | 96 | 62 | 31316405 | 34 | 20083583 | -10347650 | 18020151 | 1.0 | . 3 4 | 1 | 201404 | 83 | 53 | 18669967 | 30 | 1091295 | -15385039 | 13318200 | 1.0 | . 4 5 | 1 | 201405 | 94 | 54 | 2893905 | 40 | 2034075 | -15682170 | 2350000 | 1.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; cust.drop([&#39;Unnamed: 0&#39;,&#39;client_nr&#39;,&#39;yearmonth&#39;,&#39;CRG&#39;],axis=1).describe() . total_nr_trx nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance . count 29996.000000 | 29996.000000 | 2.999600e+04 | 29996.000000 | 2.999600e+04 | 2.999600e+04 | 2.999600e+04 | . mean 166.427957 | 75.785571 | 1.121290e+07 | 90.642386 | 1.126906e+07 | -5.523773e+06 | 3.752693e+06 | . std 220.947519 | 60.063496 | 1.617596e+07 | 192.244770 | 1.624998e+07 | 1.357517e+07 | 1.616937e+07 | . min 1.000000 | 0.000000 | 0.000000e+00 | 0.000000 | 0.000000e+00 | -3.467127e+08 | -2.485206e+08 | . 25% 76.000000 | 38.000000 | 3.072750e+06 | 33.000000 | 3.148068e+06 | -7.895864e+06 | -1.868002e+06 | . 50% 129.000000 | 66.000000 | 6.822769e+06 | 56.000000 | 6.934694e+06 | -2.957198e+06 | 1.040998e+06 | . 75% 205.000000 | 101.000000 | 1.386656e+07 | 102.000000 | 1.394257e+07 | 1.690275e+04 | 5.806224e+06 | . max 6341.000000 | 1590.000000 | 7.980480e+08 | 6325.000000 | 8.775321e+08 | 2.109783e+08 | 3.722319e+08 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; some extreme values prsent for transactional behavior . cust.isna().sum() . Unnamed: 0 0 client_nr 0 yearmonth 0 total_nr_trx 0 nr_debit_trx 0 volume_debit_trx 0 nr_credit_trx 0 volume_credit_trx 0 min_balance 0 max_balance 0 CRG 5537 dtype: int64 . seems like Credit risk group variable have some missing values. Instead of dropping or filling with the mode we shall assign a different category to them. . cust.groupby([&#39;client_nr&#39;])[&#39;yearmonth&#39;].nunique().describe() . count 992.000000 mean 30.237903 std 5.742178 min 1.000000 25% 32.000000 50% 32.000000 75% 32.000000 max 32.000000 Name: yearmonth, dtype: float64 . this show some clients do not have full time series. We can drop those clients but we can keep as well, its a subjective choice. . . . . full=cust.drop(&quot;Unnamed: 0&quot;,axis=1).merge(credit.drop(&quot;Unnamed: 0&quot;,axis=1),on=[&#39;yearmonth&#39;,&#39;client_nr&#39;]) . full[&#39;Date&#39;]=pd.to_datetime(full[&#39;yearmonth&#39;].astype(str), format=&#39;%Y%m&#39;) . max(full[&#39;Date&#39;]) . Timestamp(&#39;2016-08-01 00:00:00&#39;) . min(full[&#39;Date&#39;]) . Timestamp(&#39;2014-01-01 00:00:00&#39;) . full.groupby(&#39;Date&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index().sort_values(by=&#39;nr_credit_applications&#39;,ascending=False) . Date nr_credit_applications . 18 2015-07-01 | 120 | . 17 2015-06-01 | 120 | . 21 2015-10-01 | 99 | . 11 2014-12-01 | 96 | . 4 2014-05-01 | 96 | . 8 2014-09-01 | 96 | . 5 2014-06-01 | 91 | . 6 2014-07-01 | 89 | . 16 2015-05-01 | 88 | . 22 2015-11-01 | 87 | . 14 2015-03-01 | 87 | . 10 2014-11-01 | 87 | . 9 2014-10-01 | 87 | . 12 2015-01-01 | 86 | . 20 2015-09-01 | 82 | . 3 2014-04-01 | 82 | . 28 2016-05-01 | 81 | . 23 2015-12-01 | 81 | . 15 2015-04-01 | 80 | . 7 2014-08-01 | 80 | . 24 2016-01-01 | 79 | . 29 2016-06-01 | 78 | . 26 2016-03-01 | 77 | . 1 2014-02-01 | 73 | . 2 2014-03-01 | 72 | . 19 2015-08-01 | 70 | . 13 2015-02-01 | 68 | . 27 2016-04-01 | 67 | . 0 2014-01-01 | 66 | . 30 2016-07-01 | 59 | . 25 2016-02-01 | 45 | . 31 2016-08-01 | 43 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; full=full.sort_values(by=[&#39;client_nr&#39;,&#39;Date&#39;]) . full[&#39;month&#39;]=pd.DatetimeIndex(full[&#39;Date&#39;]).month . full[&#39;month&#39;]=full[&#39;month&#39;].astype(&#39;category&#39;) . full.groupby(&#39;month&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index().sort_values(by=&#39;nr_credit_applications&#39;,ascending=False) . month nr_credit_applications . 5 6 | 289 | . 6 7 | 268 | . 4 5 | 265 | . 2 3 | 236 | . 0 1 | 231 | . 3 4 | 229 | . 7 8 | 193 | . 1 2 | 186 | . 9 10 | 186 | . 8 9 | 178 | . 11 12 | 177 | . 10 11 | 174 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; generally we see that from august there is a drop in applications of loans. so we can use the variable month as it can be useful. . full[&#39;cash_flow&#39;]=full[&#39;volume_credit_trx&#39;]-full[&#39;volume_debit_trx&#39;] . we also caculate a feature called cash flow which define the net earnings of the clients for that timestamp and which keeps on getting accumalated. . full.drop([&#39;yearmonth&#39;],axis=1,inplace=True) . full[&#39;month&#39;]=full[&#39;month&#39;].astype(str) . full[&#39;CRG&#39;]=full[&#39;CRG&#39;].astype(&#39;category&#39;) full[&#39;CRG&#39;].unique() . [1.0, 4.0, 7.0, 2.0, 3.0, 5.0, NaN] Categories (6, float64): [1.0, 2.0, 3.0, 4.0, 5.0, 7.0] . full[&#39;CRG&#39;] = full[&#39;CRG&#39;].cat.add_categories(-1) full[&#39;CRG&#39;].fillna(-1, inplace=True) . we also try to inspect how crg groups behave differently for different transactions dimensions. . sns.boxplot(x=&#39;CRG&#39;,y=&#39;min_balance&#39;,data=full) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0887706d90&gt; . sns.boxplot(x=&#39;CRG&#39;,y=&#39;cash_flow&#39;,data=full) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f088746ca50&gt; . full.groupby(&#39;CRG&#39;)[&#39;client_nr&#39;].nunique().reset_index().sort_values(by=&#39;client_nr&#39;,ascending=False) . CRG client_nr . 2 3.0 | 291 | . 6 -1.0 | 194 | . 0 1.0 | 140 | . 5 7.0 | 138 | . 1 2.0 | 136 | . 3 4.0 | 68 | . 4 5.0 | 25 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; full.groupby(&#39;CRG&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index().sort_values(by=&#39;nr_credit_applications&#39;,ascending=False) . CRG nr_credit_applications . 2 3.0 | 756 | . 5 7.0 | 614 | . 1 2.0 | 415 | . 3 4.0 | 290 | . 6 -1.0 | 217 | . 0 1.0 | 186 | . 4 5.0 | 134 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; there is good amount of variation with respect to the column crg so it is worthwile to keep the variable. on a general level it might be the case that the crg group 3 must be more trustable as compared to group 7. . diff=full[full[&#39;credit_application&#39;]==1] . diff[&#39;Prev&#39;]=diff.groupby(&#39;client_nr&#39;)[&#39;Date&#39;].shift(1) . diff=diff[[&#39;client_nr&#39;,&#39;Date&#39;,&#39;Prev&#39;]].fillna(&#39;2014-01-01&#39;) . diff[&#39;consec&#39;]=(diff[&#39;Date&#39;]-diff[&#39;Prev&#39;])// np.timedelta64(1, &#39;M&#39;) . sns.distplot(diff[&#39;consec&#39;],kde=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f08875e53d0&gt; . diff.head() . client_nr Date Prev consec . 70 3 | 2014-07-01 | 2014-01-01 | 5 | . 81 3 | 2015-06-01 | 2014-07-01 | 11 | . 87 3 | 2015-12-01 | 2015-06-01 | 6 | . 95 3 | 2016-08-01 | 2015-12-01 | 8 | . 126 4 | 2016-07-01 | 2014-01-01 | 29 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; diff[&#39;consec&#39;].describe() . count 2025.000000 mean 5.841975 std 5.668782 min 0.000000 25% 2.000000 50% 4.000000 75% 8.000000 max 30.000000 Name: consec, dtype: float64 . from above we see that generally different between consective loans can be generally around 8 months. This information is useful to decide for the forecasting window that we should choose. . full[&#39;avg_local_credit&#39;]=full[&#39;volume_credit_trx&#39;]/full[&#39;nr_credit_trx&#39;] full[&#39;avg_local_debit&#39;]=full[&#39;volume_debit_trx&#39;]/full[&#39;nr_debit_trx&#39;] . full[&#39;nr_credit_applications_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].cumsum() . full[&#39;cash_flow_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;cash_flow&#39;].cumsum() . full[&#39;nr_debit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;nr_debit_trx&#39;].cumsum() full[&#39;nr_credit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_trx&#39;].cumsum() . full[&#39;volume_debit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;volume_debit_trx&#39;].cumsum() full[&#39;volume_credit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;volume_credit_trx&#39;].cumsum() full[&#39;avg_global_credit&#39;]=full[&#39;volume_credit_trx_cumsum&#39;]/full[&#39;nr_credit_trx_cumsum&#39;] full[&#39;avg_global_debit&#39;]=full[&#39;volume_debit_trx_cumsum&#39;]/full[&#39;nr_debit_trx_cumsum&#39;] . full[&#39;global_min_balance&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;min_balance&#39;].cummin() . full[&#39;global_max_balance&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;max_balance&#39;].cummax() . def create_shifted_lables(data,n): data[&#39;T+&#39;+str(n)+&#39;_nr_credit_applications&#39;]=data.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].shift(-n) . we calcalate some variables as above on a cumulative value so that at each time stamp we can also have historical information with use which will help us in predicting the future actions of the clients. . for i in range(1,13): create_shifted_lables(full,i) . we manipulate data to create target variable such as number of loans taken in the future at t+n timestamp which will be use to predict. . full.columns . Index([&#39;client_nr&#39;, &#39;total_nr_trx&#39;, &#39;nr_debit_trx&#39;, &#39;volume_debit_trx&#39;, &#39;nr_credit_trx&#39;, &#39;volume_credit_trx&#39;, &#39;min_balance&#39;, &#39;max_balance&#39;, &#39;CRG&#39;, &#39;credit_application&#39;, &#39;nr_credit_applications&#39;, &#39;Date&#39;, &#39;month&#39;, &#39;cash_flow&#39;, &#39;avg_local_credit&#39;, &#39;avg_local_debit&#39;, &#39;nr_credit_applications_cumsum&#39;, &#39;cash_flow_cumsum&#39;, &#39;nr_debit_trx_cumsum&#39;, &#39;nr_credit_trx_cumsum&#39;, &#39;volume_debit_trx_cumsum&#39;, &#39;volume_credit_trx_cumsum&#39;, &#39;avg_global_credit&#39;, &#39;avg_global_debit&#39;, &#39;global_min_balance&#39;, &#39;global_max_balance&#39;, &#39;T+1_nr_credit_applications&#39;, &#39;T+2_nr_credit_applications&#39;, &#39;T+3_nr_credit_applications&#39;, &#39;T+4_nr_credit_applications&#39;, &#39;T+5_nr_credit_applications&#39;, &#39;T+6_nr_credit_applications&#39;, &#39;T+7_nr_credit_applications&#39;, &#39;T+8_nr_credit_applications&#39;, &#39;T+9_nr_credit_applications&#39;, &#39;T+10_nr_credit_applications&#39;, &#39;T+11_nr_credit_applications&#39;, &#39;T+12_nr_credit_applications&#39;], dtype=&#39;object&#39;) . full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].sum().describe() . count 992.000000 mean 2.633065 std 3.412808 min 0.000000 25% 0.000000 50% 1.000000 75% 4.000000 max 27.000000 Name: nr_credit_applications, dtype: float64 . we see that moslty people took number of loans between 1 to 4. We will take two such customers to see if we can find reason or pattern why they took specified number of loans. . clients=full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index() . clients[clients[&#39;nr_credit_applications&#39;]==4] . client_nr nr_credit_applications . 40 41 | 4 | . 46 47 | 4 | . 60 61 | 4 | . 73 74 | 4 | . 82 83 | 4 | . ... ... | ... | . 890 895 | 4 | . 909 914 | 4 | . 912 917 | 4 | . 939 945 | 4 | . 965 973 | 4 | . 78 rows × 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; cleints_one_loan=clients[clients[&#39;nr_credit_applications&#39;]==1] . cleints_one_loan.head() . client_nr nr_credit_applications . 3 4 | 1 | . 5 6 | 1 | . 7 8 | 1 | . 24 25 | 1 | . 30 31 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Client with only 1 loan . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==6], x=&quot;Date&quot;, y=&#39;cash_flow_cumsum&#39;,hue=&#39;credit_application&#39;) plt.show() . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==6], x=&quot;Date&quot;, y=&#39;min_balance&#39;,hue=&#39;credit_application&#39;) plt.show() . client with 4 loans . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==47], x=&quot;Date&quot;, y=&#39;cash_flow_cumsum&#39;,hue=&#39;credit_application&#39;) plt.show() . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==47], x=&quot;Date&quot;, y=&#39;min_balance&#39;,hue=&#39;credit_application&#39;) plt.show() . one interesting pattern to see here is that the users who took more loans are the one for whome cash flow cumulative increase which means for whome income increase more. . full[[&#39;T+1_nr_credit_applications&#39;, &#39;T+2_nr_credit_applications&#39;, &#39;T+3_nr_credit_applications&#39;, &#39;T+4_nr_credit_applications&#39;, &#39;T+5_nr_credit_applications&#39;, &#39;T+6_nr_credit_applications&#39;, &#39;T+7_nr_credit_applications&#39;, &#39;T+8_nr_credit_applications&#39;, &#39;T+9_nr_credit_applications&#39;, &#39;T+10_nr_credit_applications&#39;, &#39;T+11_nr_credit_applications&#39;, &#39;T+12_nr_credit_applications&#39;]]=full[[&#39;T+1_nr_credit_applications&#39;, &#39;T+2_nr_credit_applications&#39;, &#39;T+3_nr_credit_applications&#39;, &#39;T+4_nr_credit_applications&#39;, &#39;T+5_nr_credit_applications&#39;, &#39;T+6_nr_credit_applications&#39;, &#39;T+7_nr_credit_applications&#39;, &#39;T+8_nr_credit_applications&#39;, &#39;T+9_nr_credit_applications&#39;, &#39;T+10_nr_credit_applications&#39;, &#39;T+11_nr_credit_applications&#39;, &#39;T+12_nr_credit_applications&#39;]].fillna(0) . full[[&#39;avg_local_credit&#39;,&#39;avg_local_debit&#39;,&#39;avg_global_credit&#39;,&#39;avg_global_debit&#39;]]=full[[&#39;avg_local_credit&#39;,&#39;avg_local_debit&#39;,&#39;avg_global_credit&#39;,&#39;avg_global_debit&#39;]].fillna(0) . full.isna().sum() . client_nr 0 total_nr_trx 0 nr_debit_trx 0 volume_debit_trx 0 nr_credit_trx 0 volume_credit_trx 0 min_balance 0 max_balance 0 CRG 0 credit_application 0 nr_credit_applications 0 Date 0 month 0 cash_flow 0 avg_local_credit 0 avg_local_debit 0 nr_credit_applications_cumsum 0 cash_flow_cumsum 0 nr_debit_trx_cumsum 0 nr_credit_trx_cumsum 0 volume_debit_trx_cumsum 0 volume_credit_trx_cumsum 0 avg_global_credit 0 avg_global_debit 0 global_min_balance 0 global_max_balance 0 T+1_nr_credit_applications 0 T+2_nr_credit_applications 0 T+3_nr_credit_applications 0 T+4_nr_credit_applications 0 T+5_nr_credit_applications 0 T+6_nr_credit_applications 0 T+7_nr_credit_applications 0 T+8_nr_credit_applications 0 T+9_nr_credit_applications 0 T+10_nr_credit_applications 0 T+11_nr_credit_applications 0 T+12_nr_credit_applications 0 dtype: int64 . full[&#39;CRG&#39;]=full[&#39;CRG&#39;].astype(str) . instead of predicting number of exact loans at t+n timestep we instead use number of loans in next month, next 3 months, next 6 month and next 12 months for better predictibility. . full[&#39;next_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;] . full[&#39;next_3_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;]+full[&#39;T+2_nr_credit_applications&#39;]+full[&#39;T+3_nr_credit_applications&#39;] . full[&#39;next_6_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;]+full[&#39;T+2_nr_credit_applications&#39;]+full[&#39;T+3_nr_credit_applications&#39;]+full[&#39;T+4_nr_credit_applications&#39;]+full[&#39;T+5_nr_credit_applications&#39;]+full[&#39;T+6_nr_credit_applications&#39;] . full[&#39;next_12_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;]+full[&#39;T+2_nr_credit_applications&#39;]+full[&#39;T+3_nr_credit_applications&#39;]+full[&#39;T+4_nr_credit_applications&#39;]+full[&#39;T+5_nr_credit_applications&#39;]+full[&#39;T+6_nr_credit_applications&#39;]+full[&#39;T+7_nr_credit_applications&#39;]+full[&#39;T+8_nr_credit_applications&#39;]+full[&#39;T+9_nr_credit_applications&#39;]+full[&#39;T+10_nr_credit_applications&#39;]+full[&#39;T+11_nr_credit_applications&#39;]+full[&#39;T+12_nr_credit_applications&#39;] . max(full[&#39;Date&#39;]) . Timestamp(&#39;2016-08-01 00:00:00&#39;) . to showcase the thought process we will predict for future 6 months. . full=full[full[&#39;Date&#39;]&lt;=&#39;2016-02-01&#39;] . full.head() . client_nr total_nr_trx nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance CRG credit_application ... T+7_nr_credit_applications T+8_nr_credit_applications T+9_nr_credit_applications T+10_nr_credit_applications T+11_nr_credit_applications T+12_nr_credit_applications next_month_nr_credit_applications next_3_month_nr_credit_applications next_6_month_nr_credit_applications next_12_month_nr_credit_applications . 0 1 | 97 | 50 | 6527929 | 47 | 7454863 | -7914288 | 25110651 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 1 | 88 | 59 | 3475918 | 29 | 1895848 | -8448513 | 25036651 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1 | 96 | 62 | 31316405 | 34 | 20083583 | -10347650 | 18020151 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1 | 83 | 53 | 18669967 | 30 | 1091295 | -15385039 | 13318200 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1 | 94 | 54 | 2893905 | 40 | 2034075 | -15682170 | 2350000 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 42 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X=full.drop([&#39;T+1_nr_credit_applications&#39;,&#39;T+2_nr_credit_applications&#39;,&#39;T+3_nr_credit_applications&#39;,&#39;T+4_nr_credit_applications&#39;,&#39;T+5_nr_credit_applications&#39;,&#39;T+6_nr_credit_applications&#39;,&#39;T+7_nr_credit_applications&#39;,&#39;T+8_nr_credit_applications&#39;,&#39;T+9_nr_credit_applications&#39;,&#39;T+10_nr_credit_applications&#39;,&#39;T+11_nr_credit_applications&#39;,&#39;T+12_nr_credit_applications&#39;],axis=1) . X.columns . Index([&#39;client_nr&#39;, &#39;total_nr_trx&#39;, &#39;nr_debit_trx&#39;, &#39;volume_debit_trx&#39;, &#39;nr_credit_trx&#39;, &#39;volume_credit_trx&#39;, &#39;min_balance&#39;, &#39;max_balance&#39;, &#39;CRG&#39;, &#39;credit_application&#39;, &#39;nr_credit_applications&#39;, &#39;Date&#39;, &#39;month&#39;, &#39;cash_flow&#39;, &#39;avg_local_credit&#39;, &#39;avg_local_debit&#39;, &#39;nr_credit_applications_cumsum&#39;, &#39;cash_flow_cumsum&#39;, &#39;nr_debit_trx_cumsum&#39;, &#39;nr_credit_trx_cumsum&#39;, &#39;volume_debit_trx_cumsum&#39;, &#39;volume_credit_trx_cumsum&#39;, &#39;avg_global_credit&#39;, &#39;avg_global_debit&#39;, &#39;global_min_balance&#39;, &#39;global_max_balance&#39;, &#39;next_month_nr_credit_applications&#39;, &#39;next_3_month_nr_credit_applications&#39;, &#39;next_6_month_nr_credit_applications&#39;, &#39;next_12_month_nr_credit_applications&#39;], dtype=&#39;object&#39;) . X=X.drop(columns= [&#39;next_month_nr_credit_applications&#39;,&#39;next_3_month_nr_credit_applications&#39;,&#39;next_6_month_nr_credit_applications&#39;,&#39;next_12_month_nr_credit_applications&#39;,&#39;client_nr&#39;,&#39;Date&#39;,&#39;credit_application&#39;, &#39;total_nr_trx&#39;,&#39;volume_debit_trx_cumsum&#39;,&#39;nr_credit_trx_cumsum&#39; ],axis=1) . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 24370 entries, 0 to 29989 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 nr_debit_trx 24370 non-null int64 1 volume_debit_trx 24370 non-null int64 2 nr_credit_trx 24370 non-null int64 3 volume_credit_trx 24370 non-null int64 4 min_balance 24370 non-null int64 5 max_balance 24370 non-null int64 6 CRG 24370 non-null object 7 nr_credit_applications 24370 non-null int64 8 month 24370 non-null object 9 cash_flow 24370 non-null int64 10 avg_local_credit 24370 non-null float64 11 avg_local_debit 24370 non-null float64 12 nr_credit_applications_cumsum 24370 non-null int64 13 cash_flow_cumsum 24370 non-null int64 14 nr_debit_trx_cumsum 24370 non-null int64 15 volume_credit_trx_cumsum 24370 non-null int64 16 avg_global_credit 24370 non-null float64 17 avg_global_debit 24370 non-null float64 18 global_min_balance 24370 non-null int64 19 global_max_balance 24370 non-null int64 dtypes: float64(4), int64(14), object(2) memory usage: 3.9+ MB . we binarise the variable . y=(full[[&#39;next_6_month_nr_credit_applications&#39;]]&gt;=1).astype(int) . X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1) . we create a train dataset, validation set ( for hyperparameter tuning) and a test set. . X_val=X_train[-1000:] y_val=y_train[-1000:] X_train=X_train[:-1000] y_train=y_train[:-1000] . Modelling . rf=RandomForestClassifier(n_estimators=500,max_depth=10,class_weight=&#39;balanced&#39;) rf.fit(X_train,y_train) %time print(classification_report(y_test, rf.predict_proba(X_test)[:,1]&gt;=0.5, target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 3 µs, sys: 0 ns, total: 3 µs Wall time: 6.2 µs precision recall f1-score support 0 0.89 0.78 0.83 1685 1 0.61 0.79 0.69 752 accuracy 0.78 2437 macro avg 0.75 0.78 0.76 2437 weighted avg 0.80 0.78 0.78 2437 . above we casually fit a model lets see how much score we can improve with further hyperparameter tuning and ensembling. . space_RF = { &quot;n_estimators&quot;: hp.choice(&quot;n_estimators&quot;, [300, 400,500,600,700,800,900,1000]), &quot;max_depth&quot;: hp.quniform(&quot;max_depth&quot;, 5, 15,1) } . def parameter_tuning_RF(params): clf = RandomForestClassifier(**params,class_weight=&#39;balanced&#39;,n_jobs=-1) f1 = cross_val_score(clf, X_train, y_train,scoring=&quot;f1_weighted&quot;,cv=3).mean() return {&quot;loss&quot;: -f1, &quot;status&quot;: STATUS_OK} . trials = Trials() best_RF = fmin( fn=parameter_tuning_RF, space = space_RF, algo=tpe.suggest, max_evals=30, trials=trials ) print(&quot;Best: {}&quot;.format(best_RF)) . 100%|██████████| 30/30 [27:21&lt;00:00, 54.73s/it, best loss: -0.8248557350549816] Best: {&#39;max_depth&#39;: 15.0, &#39;n_estimators&#39;: 3} . hyperparams = space_eval(space_RF,best_RF) . hyperparams . {&#39;max_depth&#39;: 15.0, &#39;n_estimators&#39;: 600} . we also perform feature selection and feature ranking. . def feature_selector(clf,x,y): boruta = BorutaPy(estimator = clf, n_estimators = &#39;auto&#39;, max_iter = 50) boruta.fit(x.values, y.values) important = list(X.columns[boruta.support_]) print(f&quot;important: {important}&quot;) tentative = list(X.columns[boruta.support_weak_]) print(f&quot;unconfirmed: {tentative}&quot;) unimportant = list(X.columns[~(boruta.support_ | boruta.support_weak_)]) print(f&quot;unimportant: {unimportant}&quot;) important.extend(tentative) return important,boruta.ranking_ . clf=RandomForestClassifier(**hyperparams,class_weight=&#39;balanced&#39;,n_jobs=-1) . featuresrf,ranking=feature_selector(clf,X_train,y_train) . important: [&#39;nr_debit_trx&#39;, &#39;volume_debit_trx&#39;, &#39;nr_credit_trx&#39;, &#39;volume_credit_trx&#39;, &#39;min_balance&#39;, &#39;max_balance&#39;, &#39;CRG&#39;, &#39;avg_local_credit&#39;, &#39;avg_local_debit&#39;, &#39;nr_credit_applications_cumsum&#39;, &#39;cash_flow_cumsum&#39;, &#39;nr_debit_trx_cumsum&#39;, &#39;volume_credit_trx_cumsum&#39;, &#39;avg_global_credit&#39;, &#39;avg_global_debit&#39;, &#39;global_min_balance&#39;, &#39;global_max_balance&#39;] unconfirmed: [&#39;cash_flow&#39;] unimportant: [&#39;nr_credit_applications&#39;, &#39;month&#39;] . X_train_rf=X_train.loc[:,featuresrf] X_val_rf=X_val.loc[:,featuresrf] X_test_rf=X_test.loc[:,featuresrf] . clf=RandomForestClassifier(**hyperparams,class_weight=&#39;balanced&#39;,n_jobs=-1) . clf.fit(X_train_rf,y_train) . RandomForestClassifier(class_weight=&#39;balanced&#39;, max_depth=15.0, n_estimators=600, n_jobs=-1) . we also try to find the best threshold which will emphasise on the weighted f1 score. . maxargrf=0.4 max_score=-1 for i in np.arange(0.4, 0.7, 0.01): score=f1_score(y_val, clf.predict_proba(X_val_rf)[:,1]&gt;=i, average=&#39;weighted&#39;) if(score&gt;max_score): max_score=score maxargrf=i . maxargrf . 0.5400000000000001 . %time print(classification_report(y_test, clf.predict_proba(X_test_rf)[:,1]&gt;=maxargrf, target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 5 µs, sys: 0 ns, total: 5 µs Wall time: 9.78 µs precision recall f1-score support 0 0.89 0.91 0.90 1685 1 0.78 0.76 0.77 752 accuracy 0.86 2437 macro avg 0.84 0.83 0.83 2437 weighted avg 0.86 0.86 0.86 2437 . len(y[y[&#39;next_6_month_nr_credit_applications&#39;]==1]) . 7523 . len(y[y[&#39;next_6_month_nr_credit_applications&#39;]==0]) . 16847 . for catboost classifier we enable early stopping and scale pos parameter which takes care of imbalanced classes. . clf2 = CatBoostClassifier(iterations=10000, learning_rate= 0.01,use_best_model=True,od_type=&#39;Iter&#39;,od_wait=50,scale_pos_weight=2.3) . clf2.fit( X_train, y_train, cat_features=[6,8], plot=True, eval_set=(X_val ,y_val) ) . maxargcat=0.4 max_score=-1 for i in np.arange(0.4, 0.7, 0.01): score=f1_score(y_val, clf2.predict_proba(X_val)[:,1]&gt;=i, average=&#39;weighted&#39;) if(score&gt;max_score): max_score=score maxargcat=i . %time print(classification_report(y_test, clf2.predict(X_test), target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 5 µs, sys: 1 µs, total: 6 µs Wall time: 9.78 µs precision recall f1-score support 0 0.94 0.86 0.90 1685 1 0.74 0.87 0.80 752 accuracy 0.86 2437 macro avg 0.84 0.86 0.85 2437 weighted avg 0.87 0.86 0.87 2437 . we optimise for a mix paramter which optimises the mix of random forest and catboost on validation set. . scoree=-1 mix=0 for i in np.arange(0, 1, 0.1): j=1-i y_pred=(i*clf.predict_proba(X_val_rf)[:,1]+ j*(clf2.predict_proba(X_val)[:,1]))/(i+j) score=f1_score(y_val, (y_pred&gt;=0.5).astype(int), average=&#39;weighted&#39;) if(score&gt;scoree): scoree=score mix=i . mix . 0.30000000000000004 . y_pred=(mix*clf.predict_proba(X_test_rf)[:,1]+ (1-mix)*(clf2.predict_proba(X_test)[:,1])) . %time print(classification_report(y_test, (y_pred&gt;=0.5).astype(int), target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs Wall time: 5.72 µs precision recall f1-score support 0 0.94 0.87 0.90 1685 1 0.75 0.87 0.80 752 accuracy 0.87 2437 macro avg 0.84 0.87 0.85 2437 weighted avg 0.88 0.87 0.87 2437 . so our model seems to be good just the precision is a bit less as compared to other metrics which can be optimised by raising the threshold as per business needs rest looks good. . Result Analysis . explainer = shap.TreeExplainer(clf2) shap_values = explainer.shap_values(X_train) . shap.summary_plot(shap_values, X_train) . above shapley plot shows us that how variables affect the output. Its interesting to see the relationship of min/max balance, past credit applications on the output which seems to make sense. . for implementation we can rank the customers based on how high the probablity is to apply for a loans in the coming months, and give attention accordingly. . explainer = shap.TreeExplainer(clf2) shap_values = explainer.shap_values(X_test) shap.summary_plot(shap_values, X_test) . type1=[] type2=[] . from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay . cm=confusion_matrix(y_test, (y_pred&gt;=0.5).astype(int)) . ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf2.classes_).plot() . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f087e42ad50&gt; . type2=[] . for p, a in zip(y_test.values.squeeze(), (y_pred&gt;=0.5).astype(int)): if((p==0) &amp; (a==1)): type2.append(True) else: type2.append(False) . X_test.loc[type2,:].describe() . nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance nr_credit_applications cash_flow avg_local_credit avg_local_debit nr_credit_applications_cumsum cash_flow_cumsum nr_debit_trx_cumsum volume_credit_trx_cumsum avg_global_credit avg_global_debit global_min_balance global_max_balance . count 222.000000 | 2.220000e+02 | 222.000000 | 2.220000e+02 | 2.220000e+02 | 2.220000e+02 | 222.000000 | 2.220000e+02 | 222.000000 | 222.000000 | 222.000000 | 2.220000e+02 | 222.000000 | 2.220000e+02 | 222.000000 | 222.000000 | 2.220000e+02 | 2.220000e+02 | . mean 82.707207 | 8.441915e+06 | 74.828829 | 8.622403e+06 | -8.890885e+06 | -7.355832e+05 | 0.220721 | 1.804874e+05 | 123149.532152 | 104135.663538 | 2.139640 | -7.015935e+05 | 1010.144144 | 1.100454e+08 | 125585.619152 | 108206.186392 | -1.261369e+07 | 2.174478e+06 | . std 38.614932 | 6.607415e+06 | 56.483510 | 7.500837e+06 | 8.986808e+06 | 8.609833e+06 | 0.504212 | 3.455147e+06 | 94909.954476 | 72395.136775 | 2.405378 | 5.918188e+06 | 862.287869 | 1.156250e+08 | 75136.408686 | 70961.034486 | 1.395708e+07 | 1.120575e+07 | . min 14.000000 | 6.147450e+05 | 15.000000 | 9.090280e+05 | -5.078090e+07 | -4.457249e+07 | 0.000000 | -6.689391e+06 | 28733.409836 | 12404.829787 | 0.000000 | -3.749371e+07 | 29.000000 | 1.591174e+06 | 26002.477612 | 17515.647655 | -7.685539e+07 | -3.702260e+07 | . 25% 51.250000 | 3.651970e+06 | 37.250000 | 3.671908e+06 | -1.231199e+07 | -3.946260e+06 | 0.000000 | -6.404925e+05 | 69125.290164 | 56458.889923 | 0.000000 | -1.532136e+06 | 342.250000 | 2.207029e+07 | 74317.129575 | 59643.994994 | -1.535338e+07 | -2.244168e+06 | . 50% 73.500000 | 7.283650e+06 | 59.500000 | 6.851706e+06 | -7.074746e+06 | 5.875950e+04 | 0.000000 | 7.080000e+03 | 102942.428150 | 78772.354288 | 1.500000 | -2.734825e+05 | 771.000000 | 6.878834e+07 | 112778.034193 | 88494.956764 | -9.051172e+06 | 8.519360e+05 | . 75% 111.000000 | 1.138326e+07 | 95.000000 | 1.139022e+07 | -3.187168e+06 | 1.479230e+06 | 0.000000 | 5.373035e+05 | 143889.871111 | 134017.040762 | 3.000000 | 5.858578e+05 | 1406.750000 | 1.528945e+08 | 145546.242877 | 141165.532783 | -4.013094e+06 | 4.973174e+06 | . max 217.000000 | 4.332326e+07 | 414.000000 | 5.253854e+07 | 4.357142e+06 | 4.048669e+07 | 3.000000 | 4.358051e+07 | 929674.941176 | 378763.500000 | 15.000000 | 4.263622e+07 | 4328.000000 | 5.971536e+08 | 504657.909091 | 425501.725962 | 5.007770e+05 | 5.993425e+07 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; correct_labe1=[] for p, a in zip(y_test.values.squeeze(), (y_pred&gt;=0.5).astype(int)): if((p==0) &amp; (a==0)): correct_labe1.append(True) else: correct_labe1.append(False) . X_test.loc[correct_labe1,:].describe()[[&#39;cash_flow_cumsum&#39;,&#39;min_balance&#39;]] . cash_flow_cumsum min_balance . count 1.463000e+03 | 1.463000e+03 | . mean -2.712885e+05 | -3.102227e+06 | . std 1.504404e+07 | 1.337984e+07 | . min -3.400017e+08 | -2.512930e+08 | . 25% -2.483418e+06 | -5.124114e+06 | . 50% -6.646800e+04 | -3.054610e+05 | . 75% 1.921716e+06 | 1.480795e+05 | . max 1.272466e+08 | 1.551133e+08 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_test.loc[type2,:].describe()[[&#39;cash_flow_cumsum&#39;,&#39;min_balance&#39;]] . cash_flow_cumsum min_balance . count 2.220000e+02 | 2.220000e+02 | . mean -7.015935e+05 | -8.890885e+06 | . std 5.918188e+06 | 8.986808e+06 | . min -3.749371e+07 | -5.078090e+07 | . 25% -1.532136e+06 | -1.231199e+07 | . 50% -2.734825e+05 | -7.074746e+06 | . 75% 5.858578e+05 | -3.187168e+06 | . max 4.263622e+07 | 4.357142e+06 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; sns.distplot(X_test.loc[type2,:].describe()[[&#39;cash_flow_cumsum&#39;]]) sns.distplot(X_test.loc[correct_labe1,:].describe()[[&#39;cash_flow_cumsum&#39;]]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f087fa244d0&gt; . we can see that one possible reason for misclassification (type2) error is that in the test set it was difficult to predict correctly for some examples since distribution of cash_flow_cumsum, min_balance was very wide which kind of hints they were tricky to classify. We can similarly check for other dimensions to inspect why some types of error occured. . . .",
            "url": "https://ashish244co.github.io/blog/jupyter/2022/07/28/bank_loan_prediction.html",
            "relUrl": "/jupyter/2022/07/28/bank_loan_prediction.html",
            "date": " • Jul 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "HPC Machine Learning Ensemble",
            "content": "!pip install mpi4py . Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/dist-packages (3.0.3) . %%writefile comm.py from mpi4py import MPI from typing import Tuple from sklearn.linear_model import LogisticRegression from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier from sklearn.preprocessing import StandardScaler import numpy as np import pandas as pd import copy from sklearn.metrics import classification_report def pre_processing(df_: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]: &quot;&quot;&quot;Pre-processing step for classification task This function takes the data frame loaded from above and returns a tuple of the NumPy array. The tuple&#39;s first element is the independent variables, or feature vectors, having a shape of (N, d) where N is the number of observations and d is the number of variables (or columns). The tuple&#39;s second element is that the vector represents the dependent variable or label with the shape in (N,). Finalize this function to pre-process the data frame to be fit in the output spec. Beyond the mechanical conversion between input data-type and output data-type, apply any content-wise pre-processing that is necessary. &quot;&quot;&quot; df=df_.copy(deep=True) categorical=[&#39;job&#39;,&#39;marital&#39;,&#39;education&#39;,&#39;default&#39;,&#39;housing&#39;,&#39;loan&#39;,&#39;poutcome&#39;] corr_matrix = df.corr().abs() upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) dropcols = [column for column in upper.columns if any(upper[column] &gt; 0.7)] dropcols=dropcols+[&#39;contact&#39;] day={&#39;thu&#39;:3,&#39;tue&#39;:1,&#39;wed&#39;:2,&#39;mon&#39;:0,&#39;fri&#39;:4} df[&#39;day_of_week&#39;]=df[&#39;day_of_week&#39;].map(day) month={&#39;may&#39;:1,&#39;jun&#39;:2,&#39;jul&#39;:3,&#39;aug&#39;:4,&#39;sep&#39;:5,&#39;nov&#39;:6} df[&#39;month&#39;]=df[&#39;month&#39;].map(month) df[&#39;week_sin&#39;] = np.sin((df.day_of_week)*(2.*np.pi/5)) df[&#39;wee_cos&#39;] = np.cos((df.day_of_week)*(2.*np.pi/5)) df[&#39;age&#39;] = df[&#39;age&#39;].apply(lambda x: x.replace(&#39;unknown&#39;,&#39;39&#39;)) #str(df[df[&#39;age&#39;]!=&#39;unknown&#39;][&#39;age&#39;].astype(int).median())) df[&#39;age&#39;]=df[&#39;age&#39;].astype(int) df.drop(columns=dropcols,axis=1,inplace=True) df=pd.get_dummies(df,drop_first=True,columns=categorical) return df.drop(&#39;y&#39;,axis=1).values,df[&#39;y&#39;].map({&#39;no&#39;:0,&#39;yes&#39;:1}).values,df.drop(&#39;y&#39;,axis=1).columns def main(): comm = MPI.COMM_WORLD rank = comm.Get_rank() size = comm.Get_size() #print(size) X, y ,columns= pre_processing(pd.read_csv(&#39;moro14_synth.csv&#39;, index_col=0)) n_samples, n_features = X.shape sc=StandardScaler() # split the dataset into train / test rnd_idx = np.random.permutation(n_samples) bound = int(n_samples / 5) newbound=n_samples-bound x_train = X[rnd_idx[:newbound]] y_train = y[rnd_idx[:newbound]] x_test = X[rnd_idx[newbound:]] y_test = y[rnd_idx[newbound:]] xtrn=copy.deepcopy(x_train) xtst=copy.deepcopy(x_test) sc.fit(xtrn) xtrn=sc.transform(xtrn) xtst=sc.transform(xtst) x_train1 = xtrn[:800,:12] y_train1 = y_train[:800] x_train2 = xtrn[800:1600,12:24] y_train2 = y_train[800:1600] x_train3 = xtrn[1600:2200,24:] y_train3 = y_train[1600:2200] clfs = { &#39;LR&#39;: LogisticRegression(), &#39;QDA&#39;: QuadraticDiscriminantAnalysis(), &#39;GNB&#39;: GaussianNB(), &#39;DT&#39;: DecisionTreeClassifier() } # root process if rank == 0: # root process sends data to all other processes data1={ &#39;x&#39;:x_train1, &#39;y&#39;:y_train1, &#39;xt&#39;:xtst[:,:12], &#39;yt&#39;:y_test } data2={ &#39;x&#39;:x_train2, &#39;y&#39;:y_train2, &#39;xt&#39;:xtst[:,12:24], &#39;yt&#39;:y_test } data3={ &#39;x&#39;:x_train3, &#39;y&#39;:y_train3, &#39;xt&#39;:xtst[:,24:], &#39;yt&#39;:y_test, } comm.send(data1, dest=1, tag=1) print(&#39;Process {} sent data:&#39;.format(rank), data1) comm.send(data2, dest=2, tag=2) print(&#39;Process {} sent data:&#39;.format(rank), data2) comm.send(data3, dest=3, tag=3) print(&#39;Process {} sent data:&#39;.format(rank), data3) result1=comm.recv(source=1, tag=5) print(&#39;Process rec data 1&#39;) result2=comm.recv(source=2, tag=6) print(&#39;Process rec data 2&#39;) result3=comm.recv(source=3, tag=7) print(&#39;Process rec data 3&#39;) print(&#39;Mean Result&#39;,classification_report(y_test,np.round((result1+result2+result3)/3))) # non-root processes elif rank==1: # each non-root process receives data from root process data1 = comm.recv(source=0, tag=rank) print(&#39;Process {} received data:&#39;.format(rank), data1) clfs[&#39;LR&#39;].fit(data1[&#39;x&#39;], data1[&#39;y&#39;]) print(&#39;LR&#39;,classification_report(y_test,clfs[&#39;LR&#39;].predict(data1[&#39;xt&#39;]))) comm.send(clfs[&#39;LR&#39;].predict_proba(data1[&#39;xt&#39;])[:, 1], dest=0, tag=5) print(&#39;done 1&#39;) elif rank==2: # each non-root process receives data from root process data1 = comm.recv(source=0, tag=rank) print(&#39;Process {} received data:&#39;.format(rank), data1) clfs[&#39;QDA&#39;].fit(data1[&#39;x&#39;], data1[&#39;y&#39;]) print(&#39;QDA&#39;,classification_report(y_test,clfs[&#39;QDA&#39;].predict(data1[&#39;xt&#39;]))) comm.send(clfs[&#39;QDA&#39;].predict_proba(data1[&#39;xt&#39;])[:, 1], dest=0, tag=6) print(&#39;done 2&#39;) elif rank==3: # each non-root process receives data from root process data1 = comm.recv(source=0, tag=rank) print(&#39;Process {} received data:&#39;.format(rank), data1) clfs[&#39;GNB&#39;].fit(data1[&#39;x&#39;], data1[&#39;y&#39;]) print(&#39;GNB&#39;,classification_report(y_test,clfs[&#39;GNB&#39;].predict(data1[&#39;xt&#39;]))) comm.send(clfs[&#39;GNB&#39;].predict_proba(data1[&#39;xt&#39;])[:, 1], dest=0, tag=7) print(&#39;done 3&#39;) main() . Overwriting comm.py . !mpirun --allow-run-as-root -np 4 python comm.py . 4 4 4 4 Process 0 sent data: {&#39;x&#39;: array([[ 0.75350917, 0.02882345, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.22391057, -1.6862909 , 1.66618377, ..., -1.52816275, 0.5815288 , -0.25264558], [ 0.81867049, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-0.68003979, -1.6862909 , -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 0.55802523, -1.11458612, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.02842662, 0.60052823, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558]]), &#39;y&#39;: array([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]), &#39;xt&#39;: array([[ 0.10189601, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-0.94068506, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 1.07931576, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-1.39681427, 0.60052823, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-1.39681427, 1.17223302, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558], [ 0.16705733, 0.60052823, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process 2 received data: {&#39;x&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], ..., [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285]]), &#39;y&#39;: array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]), &#39;xt&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], ..., [-0.1143927 , 5.28352748, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process 0 sent data: {&#39;x&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], ..., [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285]]), &#39;y&#39;: array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]), &#39;xt&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], ..., [-0.1143927 , 5.28352748, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process 0 sent data: {&#39;x&#39;: array([[-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., 5.64594654, 0.86192772, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;y&#39;: array([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, QDA precision recall f1-score support 0 0.46 0.29 0.35 299 1 0.48 0.66 0.56 301 accuracy 0.47 600 macro avg 0.47 0.47 0.45 600 weighted avg 0.47 0.47 0.45 600 done 2 Process 3 received data: {&#39;x&#39;: array([[-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., 5.64594654, 0.86192772, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;y&#39;: array([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1]), &#39;xt&#39;: array([[ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,Process 1 received data: {&#39;x&#39;: array([[ 0.75350917, 0.02882345, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.22391057, -1.6862909 , 1.66618377, ..., -1.52816275, 0.5815288 , -0.25264558], [ 0.81867049, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-0.68003979, -1.6862909 , -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 0.55802523, -1.11458612, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.02842662, 0.60052823, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558]]), &#39;y&#39;: array([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]), &#39;xt&#39;: array([[ 0.10189601, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-0.94068506, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 1.07931576, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-1.39681427, 0.60052823, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-1.39681427, 1.17223302, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558], [ 0.16705733, 0.60052823, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1]), &#39;xt&#39;: array([[ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process rec data 1 Process rec data 2 Process rec data 3 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} LR precision recall f1-score support 0 0.52 0.58 0.55 309 1 0.49 0.44 0.46 291 accuracy 0.51 600 macro avg 0.51 0.51 0.51 600 weighted avg 0.51 0.51 0.51 600 done 1 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} GNB precision recall f1-score support 0 0.52 0.87 0.65 309 1 0.51 0.14 0.22 291 accuracy 0.52 600 macro avg 0.51 0.51 0.44 600 weighted avg 0.51 0.52 0.44 600 done 3 Mean Result precision recall f1-score support 0 0.57 0.65 0.61 289 1 0.63 0.55 0.59 311 accuracy 0.60 600 macro avg 0.60 0.60 0.60 600 weighted avg 0.60 0.60 0.60 600 .",
            "url": "https://ashish244co.github.io/blog/jupyter/2021/06/01/HPC.html",
            "relUrl": "/jupyter/2021/06/01/HPC.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Sentiment classification",
            "content": "Data download . !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz !tar -xzf aclImdb_v1.tar.gz !ls . --2020-12-04 12:11:02-- https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10 Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 84125825 (80M) [application/x-gzip] Saving to: ‘aclImdb_v1.tar.gz’ aclImdb_v1.tar.gz 100%[===================&gt;] 80.23M 20.3MB/s in 7.1s 2020-12-04 12:11:09 (11.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825] aclImdb aclImdb_v1.tar.gz sample_data . Alternative with tf.datasets . I imported data in my own way since i thought it would be a good idea to do semi supervised traiing also using the unsupervised text. . def read_file(path): with open(path, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file: lines = file.readlines() text = &quot; &quot;.join(lines) return text . import glob import os . def getData(): path_posttrain = os.path.join(&#39;aclImdb/train/&#39;, &quot;pos&quot;, &quot;*.txt&quot;) path_negtrain = os.path.join(&#39;aclImdb/train/&#39;, &quot;neg&quot;, &quot;*.txt&quot;) postrain = glob.glob(path_posttrain) negtrain = glob.glob(path_negtrain) data_pos_train = [read_file(path) for path in postrain] data_neg_train = [read_file(path) for path in negtrain] path_unsup = os.path.join(&#39;aclImdb/train/&#39;, &quot;unsup&quot;, &quot;*.txt&quot;) unsup = glob.glob(path_unsup) unsup_train = [read_file(path) for path in unsup] path_postest = os.path.join(&#39;aclImdb/test/&#39;, &quot;pos&quot;, &quot;*.txt&quot;) path_negtest = os.path.join(&#39;aclImdb/test/&#39;, &quot;neg&quot;, &quot;*.txt&quot;) postest = glob.glob(path_postest) negtest = glob.glob(path_negtest) data_pos_test = [read_file(path) for path in postest] data_neg_test = [read_file(path) for path in negtest] return data_pos_train+data_neg_train,[1.0] * len(data_pos_train) + [0.0] * len(data_neg_train),data_pos_test+data_neg_test,[1.0] * len(data_pos_test) + [0.0] * len(data_neg_test),unsup_train . trainX,trainY,testX,testY,unsup=getData() . trainX[0],trainY[0] . (&#34;A couple(Janet and Richard) go camping out in the woods near a giant swamp. After camping and enjoying nature, the couple takes shelter in what they think is an abandoned farm house. Soon, a pair of escaped convicts show up and, after much delaying of the inevitable, they proceed to rape Janet and lock Richard in a birdcage.&lt;br /&gt;&lt;br /&gt;This LAST HOUSE ON THE LEFT-like film has to be one of the most underrated horror films ever made. It&#39;s one of the more sick and twisted early 70s shockers. Moreover, I found this to be quite enchanting and beautiful in it&#39;s perverse tone. I love CAGED TERROR. The music definitely helps lend a sense of personality to the film as well as a lot of beauty. I found the film to be quite creepy.&lt;br /&gt;&lt;br /&gt;The flaws mainly have to do with the pacing of the film, which is to say that the film is rather slow and meandering. While I didn&#39;t mind the pacing due to the beauty and suspense of the film in question, I do think that it will both most people. The acting isn&#39;t too good nor is the dialogue, at least in the early scenes. This film takes a little more patience than usual, and it&#39;s really not for everyone.&lt;br /&gt;&lt;br /&gt;In short, this was a good film. Not the greatest horror film I&#39;ve ever seen, but it is certainly a lot of fun. It&#39;s not exactly the easiest film to find. It&#39;s possible to find it in the USED section of a lot of stores if you look hard enough. It&#39;s not for everyone, but if you&#39;re a fan of trash cinema then it&#39;s definitely worth checking out.&#34;, 1.0) . testX[0],testY[0] . (&#34;This Metro film is episodic, but nearly a constant series of chases, mainly trying to escape police, whether real or imagined, as Buster is mistaken for an escaped criminal. It is consistently inventive and entertaining. Its greatest value is in its documenting what Hollywood looked like in the early twenties, since 95% of it is shot outside among the streets and building exteriors of the time. One gem moment and one gem sequence are present here.&lt;br /&gt;&lt;br /&gt;The great moment is when a train at a great distance quickly approaches the camera and finally stops just short of it - with Buster glumly sitting on the cowcatcher and thus moving from a long shot to a close-up within seconds.&lt;br /&gt;&lt;br /&gt;The great sequence is with the phone booth next to the elevator - one constantly being mistaken for the other with races from floor to floor - one of the great Keaton gags.&lt;br /&gt;&lt;br /&gt;Kino&#39;s print is sharp and clear - almost pristine. There is a violin/piano score accompaniment. This is one to seek out and enjoy.&#34;, 1.0) . import pandas as pd . unsupDf=pd.DataFrame(unsup,columns=[&#39;Text&#39;]) . trainDf=pd.DataFrame(trainX,columns=[&#39;Text&#39;]) . testDf=pd.DataFrame(testX,columns=[&#39;Text&#39;]) . trainDf[&#39;Label&#39;]=trainY . testDf[&#39;Label&#39;]=testY . trainDf=trainDf.sample(frac=1).reset_index(drop=True) . testDf=testDf.sample(frac=1).reset_index(drop=True) . trainDf.to_csv(&#39;train.csv&#39;) testDf.to_csv(&#39;test.csv&#39;) . trainPlusUnsupDf=pd.concat([unsupDf[&#39;Text&#39;],trainDf[&#39;Text&#39;]]) . trainPlusUnsupDf=pd.DataFrame(trainPlusUnsupDf,columns=[&#39;Text&#39;]) . I use spacy here for text preprocessing for bag of words based model which i use as a tokenizer in the input of tfidf vectorizer. So spacy here toenizes and cleans as well. . %%capture !pip install spacy !pip install en_core_web_sm !pip install optuna . import spacy . Diabled tagger and other part of pipelines since they take it takes a lot of time to run or else. . nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=[&#39;parser&#39;, &#39;ner&#39;,&#39;tagger&#39;]) . nlp.Defaults.stop_words.update([&#39;from&#39;, &#39;subject&#39;, &#39;re&#39;, &#39;edu&#39;, &#39;use&#39;, &#39;not&#39;, &#39;would&#39;, &#39;say&#39;, &#39;could&#39;, &#39;_&#39;, &#39;be&#39;, &#39;know&#39;, &#39;good&#39;, &#39;go&#39;, &#39;get&#39;, &#39;do&#39;, &#39;done&#39;, &#39;try&#39;, &#39;many&#39;, &#39;some&#39;, &#39;nice&#39;, &#39;thank&#39;, &#39;think&#39;, &#39;see&#39;, &#39;rather&#39;, &#39;easy&#39;, &#39;easily&#39;, &#39;lot&#39;, &#39;lack&#39;, &#39;make&#39;, &#39;want&#39;, &#39;seem&#39;, &#39;run&#39;, &#39;need&#39;, &#39;even&#39;, &#39;right&#39;, &#39;line&#39;, &#39;even&#39;, &#39;also&#39;, &#39;may&#39;, &#39;take&#39;, &#39;come&#39;]) . import string . also using regex for basic text preprocessing for deep learning and fastext models. which i just clearing html texts ensuring only alphanumeric characters are there and replacing number with num tag. . from bs4 import BeautifulSoup . Cleaning . import re def alpha_num(text): return re.sub(r&#39;[^A-Za-z0-9 ]&#39;, &#39;&#39;, text) #a good idea to replace all the numbers with a special token def replace_num(text): return re.sub(r&#39;[0-9]&#39;, &#39;__NUM__&#39;, text) . trainPlusUnsupDf[&#39;Text&#39;] = trainPlusUnsupDf[&#39;Text&#39;].str.lower() trainPlusUnsupDf[&#39;Text&#39;]= trainPlusUnsupDf[&#39;Text&#39;].apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) trainPlusUnsupDf[&#39;Text&#39;] = trainPlusUnsupDf[&#39;Text&#39;].apply(alpha_num) trainPlusUnsupDf[&#39;Text&#39;] = trainPlusUnsupDf[&#39;Text&#39;].apply(replace_num) . def spacyTokenize(text): doc = nlp(text) text_words = [token for token in doc] return text_words . def tokenizerBOW(text): doc = nlp(text) text_words = [token for token in doc if token.is_alpha and not token.is_stop and not token.is_digit and token.text not in string.punctuation] text_words = [ token.lemma_.lower().strip() if token.lemma_ != &quot;-PRON-&quot; else token.lower_ for token in text_words ] return text_words . The tokenizer also automatically lemmatise and removes stop words and puncuations and other basic checks using spacy. . %%time trainPlusUnsupDf[&#39;TextTOK&#39;]=trainPlusUnsupDf[&#39;Text&#39;].apply(spacyTokenize) trainPlusUnsupDf[&#39;length&#39;]=trainPlusUnsupDf[&#39;TextTOK&#39;].apply(lambda x:len(x)) . CPU times: user 1min 12s, sys: 681 ms, total: 1min 13s Wall time: 1min 13s . trainPlusUnsupDf[&#39;length&#39;].describe() . count 75000.000000 mean 237.455867 std 175.447057 min 9.000000 25% 129.000000 50% 178.000000 75% 288.000000 max 2503.000000 Name: length, dtype: float64 . import seaborn as sns . sns.distplot(trainPlusUnsupDf[&#39;length&#39;],kde=False) . /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0f383a8c18&gt; . here we can see the distribution of length of input text mostly here i guess maxlen of 300 will be a good idea which kind of cover approx till 75 percentile of all the texts. . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.preprocessing import StandardScaler . from scipy import sparse . def pipelineBOW(data): tfidfVectorizer = TfidfVectorizer(tokenizer = tokenizerBOW,ngram_range=(1,3),sublinear_tf=1,strip_accents=&#39;unicode&#39;,max_features = 40000) data=data.apply(lambda x:x.lower()) data=data.apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) tfidfVectorizer.fit(data.values) print(&#39;TFIDF Done&#39;) return tfidfVectorizer . def processData(data,tfidfVectorizer): data=data.apply(lambda x:x.lower()) data=tfidfVectorizer.transform(data.values) return data . Modelling . from sklearn import model_selection . from sklearn.model_selection import cross_val_score . import numpy as np . trainPlusUnsupDf.head() . Text TextTOK length . 0 oil industrialist leonard dawson had a __NUM__... | [oil, industrialist, leonard, dawson, had, a, ... | 213 | . 1 having recently revisited my old van damme col... | [having, recently, revisited, my, old, van, da... | 940 | . 2 i thought this might be funny going in and tay... | [i, thought, this, might, be, funny, going, in... | 210 | . 3 one of the many movies that mistakes profanity... | [one, of, the, many, movies, that, mistakes, p... | 132 | . 4 god will forgive them etc this is the best lin... | [god, will, forgive, them, etc, this, is, the,... | 245 | . %%time tfidfVectorizer=pipelineBOW(trainPlusUnsupDf[&#39;Text&#39;]) . /usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter &#39;token_pattern&#39; will not be used since &#39;tokenizer&#39; is not None&#39; warnings.warn(&#34;The parameter &#39;token_pattern&#39; will not be used&#34; . TFIDF Done CPU times: user 3min 11s, sys: 366 ms, total: 3min 11s Wall time: 3min 11s . %%time train=processData(trainDf[&#39;Text&#39;],tfidfVectorizer) . CPU times: user 45.7 s, sys: 0 ns, total: 45.7 s Wall time: 45.7 s . %%time test=processData(testDf[&#39;Text&#39;],tfidfVectorizer) . CPU times: user 43.9 s, sys: 0 ns, total: 43.9 s Wall time: 43.9 s . from sklearn.metrics import classification_report from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import LinearSVC . import optuna . Instead of handpickin parameter by myselfi automated the whole process using opuna where he choice of models are linear support vector machines, random forest and logistic regression. . Optuna by default uses TPE sampler. . According to the documentation: . This sampler is based on independent sampling. See also BaseSampler for more details of ‘independent sampling’. . On each trial, for each parameter, TPE fits one Gaussian Mixture Model (GMM) l(x) to the set of parameter values associated with the best objective values, and another GMM g(x) to the remaining parameter values. It chooses the parameter value x that maximizes the ratio l(x)/g(x). . %%time def objectiveSklearn(trial): classifier_name = trial.suggest_categorical(&#39;classifier&#39;, [&#39;LogReg&#39;,&#39;SVC&#39;,&#39;RandomForest&#39;]) if classifier_name == &#39;LogReg&#39;: loss=trial.suggest_categorical(&quot;loss&quot;, [&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;]) classifier_obj = LogisticRegression(class_weight=&#39;balanced&#39;,n_jobs=-1) if (classifier_name == &#39;SVC&#39;): svc_c = trial.suggest_float(&quot;c&quot;, 1e-10, 1e10, log=True) classifier_obj = LinearSVC(C=svc_c) else: rf_max_depth = int(trial.suggest_int(&#39;rf_max_depth&#39;, 3, 15)) n_est=int(trial.suggest_categorical(&quot;n_estimator&quot;, [100,200,300,400,500])) classifier_obj = RandomForestClassifier(max_depth=rf_max_depth, n_estimators=n_est,n_jobs=-1) accuracy=cross_val_score(classifier_obj, train, trainDf[&#39;Label&#39;].values, cv=3,n_jobs=-1) return np.mean(accuracy) studySK = optuna.create_study(direction=&#39;maximize&#39;) studySK.optimize(objectiveSklearn, n_trials=30) . [I 2020-12-04 13:28:48,002] A new study created in memory with name: no-name-1b77f598-6c55-4079-b4cf-c6288f590dbe [I 2020-12-04 13:29:14,304] Trial 0 finished with value: 0.8346396922831557 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;l2&#39;, &#39;rf_max_depth&#39;: 12, &#39;n_estimator&#39;: 400}. Best is trial 0 with value: 0.8346396922831557. [I 2020-12-04 13:29:22,459] Trial 1 finished with value: 0.8660798555772325 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 1112.8121932926088}. Best is trial 1 with value: 0.8660798555772325. [I 2020-12-04 13:29:24,040] Trial 2 finished with value: 0.870639869991248 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 6.091001448466234}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:29:37,624] Trial 3 finished with value: 0.8310396010752837 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;l1&#39;, &#39;rf_max_depth&#39;: 6, &#39;n_estimator&#39;: 400}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:29:43,109] Trial 4 finished with value: 0.8146395882233177 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 4, &#39;n_estimator&#39;: 200}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:11,168] Trial 5 finished with value: 0.8359996170905152 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 11, &#39;n_estimator&#39;: 500}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:12,967] Trial 6 finished with value: 0.8660398539771684 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 34653.80363906542}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:14,739] Trial 7 finished with value: 0.8660398539771684 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 6089723.822115924}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:28,199] Trial 8 finished with value: 0.827719765858069 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 6, &#39;n_estimator&#39;: 400}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:41,732] Trial 9 finished with value: 0.8342797162810438 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;elasticnet&#39;, &#39;rf_max_depth&#39;: 13, &#39;n_estimator&#39;: 200}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:42,019] Trial 10 finished with value: 0.821519594645077 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 3.640261501310243e-07}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:42,450] Trial 11 finished with value: 0.8631596859746721 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.019657026783969394}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:44,891] Trial 12 finished with value: 0.8689198827852321 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 11.204722563404262}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:45,325] Trial 13 finished with value: 0.8796797996229905 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.08395483786035728}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:45,637] Trial 14 finished with value: 0.8316796266763077 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.00016477035976404718}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:45,876] Trial 15 finished with value: 0.820759588242901 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 1.6781273218095836e-10}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:47,688] Trial 16 finished with value: 0.8660398539771684 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 3470240422.013036}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:48,115] Trial 17 finished with value: 0.8726396620059665 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.041590138298862295}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:48,454] Trial 18 finished with value: 0.8322796026791875 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 3.794433800671928e-05}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:48,881] Trial 19 finished with value: 0.8632396747753758 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.02024212363284256}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:57,113] Trial 20 finished with value: 0.8250796650536528 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 15, &#39;n_estimator&#39;: 100}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:58,166] Trial 21 finished with value: 0.8745198428047513 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 2.8141429666154796}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:58,588] Trial 22 finished with value: 0.8779197484194065 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.06403159117739417}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:06,631] Trial 23 finished with value: 0.8662398619774884 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 322.3759012574662}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:06,967] Trial 24 finished with value: 0.8316396010772037 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 6.562613218751375e-05}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:19,668] Trial 25 finished with value: 0.8295197706636367 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;l2&#39;, &#39;rf_max_depth&#39;: 8, &#39;n_estimator&#39;: 300}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:20,207] Trial 26 finished with value: 0.8853198188402698 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.46782324148727283}. Best is trial 26 with value: 0.8853198188402698. [I 2020-12-04 13:31:20,442] Trial 27 finished with value: 0.820759593042709 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 5.226694743474181e-08}. Best is trial 26 with value: 0.8853198188402698. [I 2020-12-04 13:31:20,802] Trial 28 finished with value: 0.8414796443069626 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.003193234097835833}. Best is trial 26 with value: 0.8853198188402698. [I 2020-12-04 13:31:32,070] Trial 29 finished with value: 0.8235597290462292 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;elasticnet&#39;, &#39;rf_max_depth&#39;: 3, &#39;n_estimator&#39;: 500}. Best is trial 26 with value: 0.8853198188402698. . CPU times: user 1.49 s, sys: 136 ms, total: 1.62 s Wall time: 2min 44s . studySK.best_params . {&#39;c&#39;: 0.46782324148727283, &#39;classifier&#39;: &#39;SVC&#39;} . svc=LinearSVC(C=studySK.best_params[&#39;c&#39;]) svc.fit(train, trainDf[&#39;Label&#39;].values) . LinearSVC(C=0.46782324148727283, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001, verbose=0) . print(classification_report(svc.predict(test),testDf[&#39;Label&#39;])) . precision recall f1-score support 0.0 0.88 0.87 0.87 12646 1.0 0.87 0.88 0.87 12354 accuracy 0.87 25000 macro avg 0.87 0.87 0.87 25000 weighted avg 0.87 0.87 0.87 25000 . The BOW models performs well infact one could use the unsupervised data also for TFIDF which gives us good result. Another imporvement could have been using truncated SVD in the pipeline to make it even more easier for models to fit. Since fitting models was not time taking much we could do an exhaustive search for hyperparameters and find good one for these models. . FAST TEXT . Fast text expect inout data to be of a certain form hence we need to preprocess our data accordingly. It is a method for cpu utilisation and seems to be quite robust and compact . !pip install fasttext . Collecting fasttext Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB) |████████████████████████████████| 71kB 3.8MB/s Requirement already satisfied: pybind11&gt;=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1) Requirement already satisfied: setuptools&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5) Building wheels for collected packages: fasttext Building wheel for fasttext (setup.py) ... done Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3038678 sha256=402134e5718634d7967fe276865bdb4c7c97360bb7bb1ebc28bc25e836f1e89c Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592 Successfully built fasttext Installing collected packages: fasttext Successfully installed fasttext-0.9.2 . We clean minimally for deep learning models. . trainDf[&#39;Text&#39;]= trainDf[&#39;Text&#39;].apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) trainDf[&#39;Text&#39;] = trainDf[&#39;Text&#39;].str.lower() trainDf[&#39;Text&#39;] = trainDf[&#39;Text&#39;].apply(alpha_num) trainDf[&#39;Text&#39;] = trainDf[&#39;Text&#39;].apply(replace_num) . testDf[&#39;Text&#39;]= testDf[&#39;Text&#39;].apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) testDf[&#39;Text&#39;] = testDf[&#39;Text&#39;].str.lower() testDf[&#39;Text&#39;] = testDf[&#39;Text&#39;].apply(alpha_num) testDf[&#39;Text&#39;] = testDf[&#39;Text&#39;].apply(replace_num) . def makeFastextData(data,dataset): with open(&#39;fasttext_input_imdb_&#39;+dataset +&#39;.txt&#39;, &#39;w&#39;) as f: for text,label in zip(data[&#39;Text&#39;],data[&#39;Label&#39;]): f.writelines(f&#39;__label__{label} {text} n&#39;) . makeFastextData(trainDf,&#39;train&#39;) . makeFastextData(testDf,&#39;test&#39;) . !wc fasttext_input_imdb_train.txt . 25000 5744161 32330565 fasttext_input_imdb_train.txt . !wc fasttext_input_imdb_test.txt . 25000 5615443 31564214 fasttext_input_imdb_test.txt . Some extra preprocessing . !cat fasttext_input_imdb_train.txt | sed -e &quot;s/ ([. !?,&#39;/()] )/ 1 /g&quot; | tr &quot;[:upper:]&quot; &quot;[:lower:]&quot; &gt; fasttext_input_imdb_train_proc.txt . !cat fasttext_input_imdb_test.txt | sed -e &quot;s/ ([. !?,&#39;/()] )/ 1 /g&quot; | tr &quot;[:upper:]&quot; &quot;[:lower:]&quot; &gt; fasttext_input_imdb_test_proc.txt . !head -n 24000 fasttext_input_imdb_train_proc.txt &gt; imdb_train.bin . !tail -n 1000 fasttext_input_imdb_train_proc.txt &gt; imdb_valid.bin . !head -n 25000 fasttext_input_imdb_test_proc.txt &gt; imdb_test.bin . import fasttext . Here we use autotune funcionality of fasttext to find a perfect model for us by doing changes based on validation data. limited the duration to 100 for fast execution and also limit the size of the output model. For faster convergence we use hierarchical softmax as the loss. . %%time model = fasttext.train_supervised(input=&#39;imdb_train.bin&#39;,autotuneValidationFile=&#39;imdb_valid.bin&#39;,autotuneModelSize=&quot;2M&quot;, autotuneDuration=100, loss=&#39;hs&#39;) . CPU times: user 2min 22s, sys: 31.1 ms, total: 2min 22s Wall time: 2min 22s . _,precision,recall=model.test(&#39;imdb_test.bin&#39;) . print(&#39;Precision:&#39;,precision,&#39;recall:&#39;,recall) . Precision: 0.87892 recall: 0.87892 . saved the model here in compact quantized form so that it can be use later. . model.quantize(retrain=False) model.save_model(&quot;model_filename.ftz&quot;) . Fast text is a very good easy to use and very well engineered solution which give great results too. i tried to search for a way to learn representation present in text and use it as a start for classification but could not find that bridge although one can learn the representation easily by using function learn_unsupervised . Sequence RNN . we use keras tokenizer trained on the unsupervised+training test and then transform test data using it as well as train. i experiment with 2 layer lstm structure for which we select hyperparameters using optuna again. used a 2 layer lstm structure for which we find optimal hyperparameter due to resource content and time needed by models to run we dont choose many trials. . import tensorflow . from tensorflow.keras.preprocessing.text import Tokenizer . t = Tokenizer() t.fit_on_texts(trainPlusUnsupDf[&#39;Text&#39;].values) . trainText = t.texts_to_sequences(trainDf[&#39;Text&#39;].values) . testText = t.texts_to_sequences(testDf[&#39;Text&#39;].values) . trainTextPadded=tensorflow.keras.preprocessing.sequence.pad_sequences(trainText, maxlen=300, dtype=&#39;int32&#39;, padding=&#39;post&#39;, truncating=&#39;post&#39;) . testTextPadded=tensorflow.keras.preprocessing.sequence.pad_sequences(testText, maxlen=300, dtype=&#39;int32&#39;, padding=&#39;post&#39;, truncating=&#39;post&#39;) . from sklearn.model_selection import train_test_split . from tensorflow.keras.layers import LSTM,Embedding,GRU,Dense,Input,Dropout,Bidirectional from tensorflow.keras import Model from tensorflow.keras.optimizers import RMSprop,Adam . X_train, X_test, y_train, y_test = train_test_split(trainTextPadded, trainDf[&#39;Label&#39;], test_size=0.2, random_state=42) . %%time def objectiveLSTM(trial): inpt = Input(shape = (300,)) embedding = Embedding(len(t.word_index) + 1,output_dim=trial.suggest_categorical(&quot;embedding&quot;, [50,100]), input_length=300, mask_zero=True)(inpt) LSTM1=Bidirectional(LSTM(units=trial.suggest_categorical(&quot;units1&quot;, [64,128,256]),return_sequences=True))(embedding) dropout=Dropout(rate=trial.suggest_loguniform(&#39;dropout&#39;, 1e-1, 5e-1))(LSTM1) LSTM2=Bidirectional(LSTM(units=trial.suggest_categorical(&quot;units2&quot;, [32,64,128]),return_sequences=False))(dropout) output=Dense(1, activation=&#39;sigmoid&#39;)(LSTM2) model=Model(inpt,output) lr = trial.suggest_loguniform(&#39;lr&#39;, 1e-5, 1e-2) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=RMSprop(lr=lr), metrics=[&#39;accuracy&#39;]) model.fit(X_train,y_train,validation_split=0.1,shuffle=True,batch_size=512,epochs=10,verbose=True) accuracy=model.evaluate(X_test,y_test) return accuracy[0] studyDL = optuna.create_study(direction=&#39;minimize&#39;) studyDL.optimize(objectiveLSTM, n_trials=5) . [I 2020-12-04 15:56:48,108] A new study created in memory with name: no-name-63df6f6a-b44a-48ca-9ae5-105f9e61f3a7 . Epoch 1/10 36/36 [==============================] - 14s 377ms/step - loss: 0.6842 - accuracy: 0.6080 - val_loss: 0.6049 - val_accuracy: 0.6760 Epoch 2/10 36/36 [==============================] - 9s 259ms/step - loss: 0.5117 - accuracy: 0.7650 - val_loss: 0.4550 - val_accuracy: 0.7850 Epoch 3/10 36/36 [==============================] - 9s 259ms/step - loss: 0.3579 - accuracy: 0.8429 - val_loss: 0.5821 - val_accuracy: 0.7550 Epoch 4/10 36/36 [==============================] - 9s 259ms/step - loss: 0.2589 - accuracy: 0.8945 - val_loss: 0.5419 - val_accuracy: 0.7770 Epoch 5/10 36/36 [==============================] - 9s 258ms/step - loss: 0.1753 - accuracy: 0.9327 - val_loss: 0.6102 - val_accuracy: 0.7345 Epoch 6/10 36/36 [==============================] - 9s 259ms/step - loss: 0.1415 - accuracy: 0.9468 - val_loss: 0.5000 - val_accuracy: 0.8165 Epoch 7/10 36/36 [==============================] - 9s 259ms/step - loss: 0.0930 - accuracy: 0.9638 - val_loss: 0.9224 - val_accuracy: 0.6855 Epoch 8/10 36/36 [==============================] - 9s 260ms/step - loss: 0.1016 - accuracy: 0.9628 - val_loss: 0.6156 - val_accuracy: 0.7845 Epoch 9/10 36/36 [==============================] - 9s 258ms/step - loss: 0.0511 - accuracy: 0.9852 - val_loss: 0.5967 - val_accuracy: 0.7960 Epoch 10/10 36/36 [==============================] - 9s 259ms/step - loss: 0.0460 - accuracy: 0.9824 - val_loss: 0.7120 - val_accuracy: 0.8050 157/157 [==============================] - 3s 22ms/step - loss: 0.6811 - accuracy: 0.8076 . [I 2020-12-04 15:58:59,365] Trial 0 finished with value: 0.6811148524284363 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.22504036672028457, &#39;units2&#39;: 32, &#39;lr&#39;: 0.008674885296861211}. Best is trial 0 with value: 0.6811148524284363. . Epoch 1/10 36/36 [==============================] - 14s 385ms/step - loss: 0.6923 - accuracy: 0.5211 - val_loss: 0.6897 - val_accuracy: 0.5410 Epoch 2/10 36/36 [==============================] - 10s 274ms/step - loss: 0.6286 - accuracy: 0.6923 - val_loss: 0.4985 - val_accuracy: 0.7650 Epoch 3/10 36/36 [==============================] - 10s 274ms/step - loss: 0.4178 - accuracy: 0.8182 - val_loss: 0.4259 - val_accuracy: 0.8110 Epoch 4/10 36/36 [==============================] - 10s 273ms/step - loss: 0.3308 - accuracy: 0.8665 - val_loss: 0.3624 - val_accuracy: 0.8455 Epoch 5/10 36/36 [==============================] - 10s 273ms/step - loss: 0.2694 - accuracy: 0.8977 - val_loss: 0.3391 - val_accuracy: 0.8675 Epoch 6/10 36/36 [==============================] - 10s 272ms/step - loss: 0.2275 - accuracy: 0.9151 - val_loss: 0.3360 - val_accuracy: 0.8775 Epoch 7/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1878 - accuracy: 0.9328 - val_loss: 0.4527 - val_accuracy: 0.8445 Epoch 8/10 36/36 [==============================] - 10s 274ms/step - loss: 0.1630 - accuracy: 0.9429 - val_loss: 0.3379 - val_accuracy: 0.8790 Epoch 9/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1361 - accuracy: 0.9536 - val_loss: 0.3432 - val_accuracy: 0.8775 Epoch 10/10 36/36 [==============================] - 10s 275ms/step - loss: 0.1099 - accuracy: 0.9637 - val_loss: 0.3591 - val_accuracy: 0.8590 157/157 [==============================] - 4s 23ms/step - loss: 0.3803 - accuracy: 0.8502 . [I 2020-12-04 16:01:04,098] Trial 1 finished with value: 0.38028624653816223 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.3706454194859867, &#39;units2&#39;: 64, &#39;lr&#39;: 0.00016189924268387887}. Best is trial 1 with value: 0.38028624653816223. . Epoch 1/10 36/36 [==============================] - 13s 374ms/step - loss: 0.6136 - accuracy: 0.6732 - val_loss: 0.4087 - val_accuracy: 0.8200 Epoch 2/10 36/36 [==============================] - 10s 274ms/step - loss: 0.3540 - accuracy: 0.8510 - val_loss: 0.4048 - val_accuracy: 0.8260 Epoch 3/10 36/36 [==============================] - 10s 274ms/step - loss: 0.2343 - accuracy: 0.9084 - val_loss: 0.3642 - val_accuracy: 0.8550 Epoch 4/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1784 - accuracy: 0.9336 - val_loss: 0.3657 - val_accuracy: 0.8660 Epoch 5/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1308 - accuracy: 0.9510 - val_loss: 0.4816 - val_accuracy: 0.8545 Epoch 6/10 36/36 [==============================] - 10s 272ms/step - loss: 0.0933 - accuracy: 0.9664 - val_loss: 0.4857 - val_accuracy: 0.7950 Epoch 7/10 36/36 [==============================] - 10s 273ms/step - loss: 0.0546 - accuracy: 0.9820 - val_loss: 0.5534 - val_accuracy: 0.8635 Epoch 8/10 36/36 [==============================] - 10s 274ms/step - loss: 0.0463 - accuracy: 0.9835 - val_loss: 0.5407 - val_accuracy: 0.8580 Epoch 9/10 36/36 [==============================] - 10s 273ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 0.5886 - val_accuracy: 0.8555 Epoch 10/10 36/36 [==============================] - 10s 274ms/step - loss: 0.0321 - accuracy: 0.9884 - val_loss: 0.5884 - val_accuracy: 0.8580 157/157 [==============================] - 4s 22ms/step - loss: 0.5836 - accuracy: 0.8582 . [I 2020-12-04 16:03:08,472] Trial 2 finished with value: 0.5836058855056763 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.1275607257989057, &#39;units2&#39;: 64, &#39;lr&#39;: 0.0020301932674781486}. Best is trial 1 with value: 0.38028624653816223. . Epoch 1/10 36/36 [==============================] - 19s 514ms/step - loss: 0.6922 - accuracy: 0.5332 - val_loss: 0.6908 - val_accuracy: 0.5860 Epoch 2/10 36/36 [==============================] - 15s 415ms/step - loss: 0.6754 - accuracy: 0.6570 - val_loss: 0.5943 - val_accuracy: 0.7025 Epoch 3/10 36/36 [==============================] - 15s 414ms/step - loss: 0.5075 - accuracy: 0.7626 - val_loss: 0.4557 - val_accuracy: 0.7970 Epoch 4/10 36/36 [==============================] - 15s 415ms/step - loss: 0.3980 - accuracy: 0.8305 - val_loss: 0.4485 - val_accuracy: 0.8050 Epoch 5/10 36/36 [==============================] - 15s 412ms/step - loss: 0.3251 - accuracy: 0.8691 - val_loss: 0.4432 - val_accuracy: 0.8120 Epoch 6/10 36/36 [==============================] - 15s 417ms/step - loss: 0.2710 - accuracy: 0.8966 - val_loss: 0.3585 - val_accuracy: 0.8470 Epoch 7/10 36/36 [==============================] - 15s 415ms/step - loss: 0.2351 - accuracy: 0.9135 - val_loss: 0.3566 - val_accuracy: 0.8560 Epoch 8/10 36/36 [==============================] - 15s 414ms/step - loss: 0.1995 - accuracy: 0.9301 - val_loss: 0.4250 - val_accuracy: 0.8350 Epoch 9/10 36/36 [==============================] - 15s 415ms/step - loss: 0.1723 - accuracy: 0.9423 - val_loss: 0.3400 - val_accuracy: 0.8590 Epoch 10/10 36/36 [==============================] - 15s 415ms/step - loss: 0.1463 - accuracy: 0.9514 - val_loss: 0.4038 - val_accuracy: 0.8635 157/157 [==============================] - 4s 24ms/step - loss: 0.4163 - accuracy: 0.8548 . [I 2020-12-04 16:06:05,257] Trial 3 finished with value: 0.4163428843021393 and parameters: {&#39;embedding&#39;: 100, &#39;units1&#39;: 128, &#39;dropout&#39;: 0.2709290810082003, &#39;units2&#39;: 64, &#39;lr&#39;: 9.041356047814194e-05}. Best is trial 1 with value: 0.38028624653816223. . Epoch 1/10 36/36 [==============================] - 16s 451ms/step - loss: 0.6749 - accuracy: 0.6149 - val_loss: 0.5397 - val_accuracy: 0.7425 Epoch 2/10 36/36 [==============================] - 12s 338ms/step - loss: 0.4680 - accuracy: 0.7923 - val_loss: 0.7116 - val_accuracy: 0.6825 Epoch 3/10 36/36 [==============================] - 12s 338ms/step - loss: 0.4096 - accuracy: 0.8179 - val_loss: 0.4056 - val_accuracy: 0.8115 Epoch 4/10 36/36 [==============================] - 12s 338ms/step - loss: 0.2572 - accuracy: 0.8961 - val_loss: 0.4227 - val_accuracy: 0.8235 Epoch 5/10 36/36 [==============================] - 12s 338ms/step - loss: 0.1988 - accuracy: 0.9237 - val_loss: 0.7829 - val_accuracy: 0.7080 Epoch 6/10 36/36 [==============================] - 12s 338ms/step - loss: 0.1731 - accuracy: 0.9328 - val_loss: 0.4729 - val_accuracy: 0.8015 Epoch 7/10 36/36 [==============================] - 12s 337ms/step - loss: 0.1406 - accuracy: 0.9492 - val_loss: 0.8127 - val_accuracy: 0.7070 Epoch 8/10 36/36 [==============================] - 12s 336ms/step - loss: 0.1125 - accuracy: 0.9569 - val_loss: 0.5266 - val_accuracy: 0.7965 Epoch 9/10 36/36 [==============================] - 12s 338ms/step - loss: 0.0567 - accuracy: 0.9808 - val_loss: 0.5345 - val_accuracy: 0.8400 Epoch 10/10 36/36 [==============================] - 12s 337ms/step - loss: 0.0791 - accuracy: 0.9735 - val_loss: 1.2125 - val_accuracy: 0.7055 157/157 [==============================] - 4s 23ms/step - loss: 1.2458 - accuracy: 0.7060 . [I 2020-12-04 16:08:33,779] Trial 4 finished with value: 1.2457996606826782 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 128, &#39;dropout&#39;: 0.3363530997614701, &#39;units2&#39;: 32, &#39;lr&#39;: 0.003724474987378308}. Best is trial 1 with value: 0.38028624653816223. . CPU times: user 7min 47s, sys: 28.5 s, total: 8min 16s Wall time: 11min 45s . studyDL.best_params . {&#39;dropout&#39;: 0.3706454194859867, &#39;embedding&#39;: 50, &#39;lr&#39;: 0.00016189924268387887, &#39;units1&#39;: 64, &#39;units2&#39;: 64} . studyDL.best_trial . FrozenTrial(number=1, value=0.38028624653816223, datetime_start=datetime.datetime(2020, 12, 4, 15, 58, 59, 366599), datetime_complete=datetime.datetime(2020, 12, 4, 16, 1, 4, 97783), params={&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.3706454194859867, &#39;units2&#39;: 64, &#39;lr&#39;: 0.00016189924268387887}, distributions={&#39;embedding&#39;: CategoricalDistribution(choices=(50, 100)), &#39;units1&#39;: CategoricalDistribution(choices=(64, 128, 256)), &#39;dropout&#39;: LogUniformDistribution(high=0.5, low=0.1), &#39;units2&#39;: CategoricalDistribution(choices=(32, 64, 128)), &#39;lr&#39;: LogUniformDistribution(high=0.01, low=1e-05)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=1, state=TrialState.COMPLETE) . So for using pretrained embedding we used here glove and fasttext embeddign which initiliases the weights of our embedding layer and we set it trainable to false . !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip . --2020-12-04 14:32:47-- http://nlp.stanford.edu/data/glove.twitter.27B.zip Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140 Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected. HTTP request sent, awaiting response... 302 Found Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following] --2020-12-04 14:32:48-- https://nlp.stanford.edu/data/glove.twitter.27B.zip Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following] --2020-12-04 14:32:48-- http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22 Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 1520408563 (1.4G) [application/zip] Saving to: ‘glove.twitter.27B.zip’ glove.twitter.27B.z 100%[===================&gt;] 1.42G 2.18MB/s in 11m 43s 2020-12-04 14:44:31 (2.06 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563] . !unzip glove.twitter.27B.zip . Archive: glove.twitter.27B.zip inflating: glove.twitter.27B.25d.txt inflating: glove.twitter.27B.50d.txt replace glove.twitter.27B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: glove.twitter.27B.100d.txt inflating: glove.twitter.27B.200d.txt . !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip . --2020-12-04 14:48:01-- https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ... Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 681808098 (650M) [application/zip] Saving to: ‘wiki-news-300d-1M.vec.zip’ wiki-news-300d-1M.v 100%[===================&gt;] 650.22M 12.5MB/s in 53s 2020-12-04 14:48:56 (12.2 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098] . !unzip wiki-news-300d-1M.vec.zip . Archive: wiki-news-300d-1M.vec.zip inflating: wiki-news-300d-1M.vec . import numpy as np . def embeddingWeights(embeddingType,word_index): embeddings_index = {} if(embeddingType==&#39;glove&#39;): f = open(&#39;glove.twitter.27B.50d.txt&#39;) EMBEDDING_DIM=50 else: f = open(&#39;wiki-news-300d-1M.vec&#39;) EMBEDDING_DIM=300 for line in f: values = line.split() word = values[0] coefs = np.asarray(values[1:], dtype=&#39;float32&#39;) embeddings_index[word] = coefs f.close() embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) for word, i in word_index.items(): embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector return embedding_matrix,EMBEDDING_DIM . %%time glove,emd=embeddingWeights(&#39;glove&#39;,t.word_index) . CPU times: user 17.1 s, sys: 958 ms, total: 18 s Wall time: 18 s . %%time fasttext,embd=embeddingWeights(&#39;fasttext&#39;,t.word_index) . CPU times: user 1min 5s, sys: 3.01 s, total: 1min 8s Wall time: 1min 8s . def makeLSTM(embedding,dropout,lr,units1,units2,usePretrained,embedding_matrix): inpt = Input(shape = (300,)) if(not usePretrained): embedding = Embedding(len(t.word_index) + 1,output_dim=embedding, input_length=300, mask_zero=True)(inpt) else: embedding = Embedding(len(t.word_index) + 1,output_dim=embedding, input_length=300, mask_zero=True,weights=[embedding_matrix],trainable=False)(inpt) LSTM1=Bidirectional(LSTM(units=units1,return_sequences=True))(embedding) drop=Dropout(rate=dropout)(LSTM1) LSTM2=Bidirectional(LSTM(units=units2,return_sequences=False))(drop) output=Dense(1, activation=&#39;sigmoid&#39;)(LSTM2) model=Model(inpt,output) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=RMSprop(lr=lr), metrics=[&#39;accuracy&#39;]) model.fit(trainTextPadded,trainDf[&#39;Label&#39;].values,validation_split=0.1,shuffle=True,batch_size=512,epochs=10,verbose=True) return model . Now we use the params we found using optuna for lstm based models. . {&#39;dropout&#39;: 0.3706454194859867, &#39;embedding&#39;: 50, &#39;lr&#39;: 0.00016189924268387887, &#39;units1&#39;: 64, &#39;units2&#39;: 64} . modelLSTM=makeLSTM(50,0.37,0.00016,64,64,False,None) . Epoch 1/10 44/44 [==============================] - 16s 370ms/step - loss: 0.6898 - accuracy: 0.5742 - val_loss: 0.6753 - val_accuracy: 0.6944 Epoch 2/10 44/44 [==============================] - 12s 283ms/step - loss: 0.5134 - accuracy: 0.7696 - val_loss: 0.4666 - val_accuracy: 0.7824 Epoch 3/10 44/44 [==============================] - 12s 283ms/step - loss: 0.3553 - accuracy: 0.8519 - val_loss: 0.4220 - val_accuracy: 0.8084 Epoch 4/10 44/44 [==============================] - 12s 283ms/step - loss: 0.2882 - accuracy: 0.8859 - val_loss: 0.3599 - val_accuracy: 0.8428 Epoch 5/10 44/44 [==============================] - 12s 282ms/step - loss: 0.2342 - accuracy: 0.9120 - val_loss: 0.3435 - val_accuracy: 0.8568 Epoch 6/10 44/44 [==============================] - 12s 284ms/step - loss: 0.1997 - accuracy: 0.9261 - val_loss: 0.3219 - val_accuracy: 0.8772 Epoch 7/10 44/44 [==============================] - 12s 284ms/step - loss: 0.1648 - accuracy: 0.9422 - val_loss: 0.3309 - val_accuracy: 0.8776 Epoch 8/10 44/44 [==============================] - 12s 283ms/step - loss: 0.1391 - accuracy: 0.9528 - val_loss: 0.3400 - val_accuracy: 0.8784 Epoch 9/10 44/44 [==============================] - 12s 283ms/step - loss: 0.1175 - accuracy: 0.9609 - val_loss: 0.4453 - val_accuracy: 0.8628 Epoch 10/10 44/44 [==============================] - 12s 283ms/step - loss: 0.0993 - accuracy: 0.9683 - val_loss: 0.3774 - val_accuracy: 0.8764 . modelLSTM.evaluate(testTextPadded,testDf[&#39;Label&#39;]) . 782/782 [==============================] - 18s 22ms/step - loss: 0.4298 - accuracy: 0.8522 . [0.42976242303848267, 0.8522400259971619] . modelLSTMGlove=makeLSTM(50,0.37,0.00016,64,64,True,glove) . Epoch 1/10 44/44 [==============================] - 14s 317ms/step - loss: 0.6733 - accuracy: 0.5908 - val_loss: 0.6399 - val_accuracy: 0.6396 Epoch 2/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5996 - accuracy: 0.6828 - val_loss: 0.5484 - val_accuracy: 0.7224 Epoch 3/10 44/44 [==============================] - 10s 235ms/step - loss: 0.5605 - accuracy: 0.7183 - val_loss: 0.5308 - val_accuracy: 0.7412 Epoch 4/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5487 - accuracy: 0.7259 - val_loss: 0.5198 - val_accuracy: 0.7500 Epoch 5/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5344 - accuracy: 0.7363 - val_loss: 0.5922 - val_accuracy: 0.6804 Epoch 6/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5264 - accuracy: 0.7410 - val_loss: 0.5494 - val_accuracy: 0.7336 Epoch 7/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5217 - accuracy: 0.7460 - val_loss: 0.5292 - val_accuracy: 0.7356 Epoch 8/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5115 - accuracy: 0.7504 - val_loss: 0.5728 - val_accuracy: 0.7120 Epoch 9/10 44/44 [==============================] - 10s 235ms/step - loss: 0.5113 - accuracy: 0.7527 - val_loss: 0.4952 - val_accuracy: 0.7668 Epoch 10/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5005 - accuracy: 0.7584 - val_loss: 0.4992 - val_accuracy: 0.7672 . modelLSTMGlove.evaluate(testTextPadded,testDf[&#39;Label&#39;]) . 782/782 [==============================] - 18s 23ms/step - loss: 0.5002 - accuracy: 0.7660 . [0.5001837015151978, 0.766040027141571] . Keeping all the parameters same as that of simple lstm(flexible embedding layers)vs glove embedding layer we see the drop in performance which is quite explainable by the fact that text used train glove is different from the text here (even tho its text from twitter). Moreover can also see that the model takes more time to fit (train accuracy) but this could improve with more epoch or using high dimensional embedding of glove. could also have more complicated achitechtures and more intensive hyperparameter search but for now due to resource contraint we limit ourselves. . Since the embedding size offered by fasttext is bigger it might be good idea to use more units of lstm. . modelLSTMfasttext=makeLSTM(300,0.37,0.00016,128,64,True,fasttext) . Epoch 1/10 44/44 [==============================] - 23s 517ms/step - loss: 0.6563 - accuracy: 0.6223 - val_loss: 0.7108 - val_accuracy: 0.5560 Epoch 2/10 44/44 [==============================] - 19s 439ms/step - loss: 0.5511 - accuracy: 0.7198 - val_loss: 0.4735 - val_accuracy: 0.7880 Epoch 3/10 44/44 [==============================] - 19s 438ms/step - loss: 0.4919 - accuracy: 0.7630 - val_loss: 0.4836 - val_accuracy: 0.7688 Epoch 4/10 44/44 [==============================] - 19s 437ms/step - loss: 0.4570 - accuracy: 0.7896 - val_loss: 0.5255 - val_accuracy: 0.7464 Epoch 5/10 44/44 [==============================] - 19s 438ms/step - loss: 0.4453 - accuracy: 0.7941 - val_loss: 0.5087 - val_accuracy: 0.7656 Epoch 6/10 44/44 [==============================] - 19s 437ms/step - loss: 0.4302 - accuracy: 0.8054 - val_loss: 0.4884 - val_accuracy: 0.7596 Epoch 7/10 44/44 [==============================] - 19s 436ms/step - loss: 0.4207 - accuracy: 0.8088 - val_loss: 0.4005 - val_accuracy: 0.8200 Epoch 8/10 44/44 [==============================] - 19s 438ms/step - loss: 0.4111 - accuracy: 0.8140 - val_loss: 0.3988 - val_accuracy: 0.8300 Epoch 9/10 44/44 [==============================] - 19s 438ms/step - loss: 0.3991 - accuracy: 0.8220 - val_loss: 0.3714 - val_accuracy: 0.8384 Epoch 10/10 44/44 [==============================] - 19s 437ms/step - loss: 0.3999 - accuracy: 0.8192 - val_loss: 0.3683 - val_accuracy: 0.8460 . modelLSTMfasttext.evaluate(testTextPadded,testDf[&#39;Label&#39;]) . 782/782 [==============================] - 21s 27ms/step - loss: 0.3705 - accuracy: 0.8396 . [0.37046363949775696, 0.8396000266075134] . Here we get reasonable accuracy but we had to use a slightly bigger model. But since its a bigger size vectors it can accomodate somehow. But we can see still wiki news contains corpus which is different from casual commenting on imdb by people. . contrasting performance of deep learning models with bag of words (fits faster to data aswell) model its interesting to notice that maybe indivisual words might play more role in determing the sentiment as compared to the full sequential structure. but one cannot say that with full certainity since didnt train our models for long enough neither explored more trials of hyperparameters. . BERT BASED . So we try 2 variants of bert based models first by using distil bert from hugging face an then using ktrain to train the bigger bert. . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB) |████████████████████████████████| 1.4MB 5.6MB/s Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 19.9MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Collecting tokenizers==0.9.4 Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB) |████████████████████████████████| 2.9MB 28.2MB/s Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (0.17.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers) (2.4.7) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2020.11.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (3.0.4) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a1ec9e5910bcec29fdd40a087b5167a0a7839887e10711dce208e8319dfc82e2 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, transformers Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0 . from transformers import DistilBertTokenizerFast, TFDistilBertModel . We almost used unprocessed data due to the nature of bert tokenizer and also we set do lower case to true which convert data to lower case since we use distilbert base uncased . tokenizer = DistilBertTokenizerFast.from_pretrained(&#39;distilbert-base-uncased&#39;, do_lower_case=True) . . from tqdm.notebook import tqdm . bert = TFDistilBertModel.from_pretrained(&#39;distilbert-base-uncased&#39;) . . Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: [&#39;vocab_projector&#39;, &#39;activation_13&#39;, &#39;vocab_transform&#39;, &#39;vocab_layer_norm&#39;] - This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training. . bert.summary() . Model: &#34;tf_distil_bert_model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= distilbert (TFDistilBertMain multiple 66362880 ================================================================= Total params: 66,362,880 Trainable params: 66,362,880 Non-trainable params: 0 _________________________________________________________________ . For tokenisation we use fast encoder batch for distil bert which works very fast and automatically adds special tokens and does padding and truncation according to the max length. . import numpy as np . def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512): all_ids = [] for i in tqdm(range(0, len(texts), chunk_size)): text_chunk = texts[i:i+chunk_size].tolist() encs = tokenizer.batch_encode_plus(text_chunk,add_special_tokens=True,max_length=maxlen,padding=True,truncation=True) all_ids.extend(np.asarray(encs[&#39;input_ids&#39;])) return all_ids . x_train = fast_encode(trainDf[&#39;Text&#39;].astype(str), tokenizer, maxlen=300) x_train=np.stack(x_train, axis=0) . . x_test = fast_encode(testDf[&#39;Text&#39;].astype(str), tokenizer, maxlen=300) x_test=np.stack(x_test, axis=0) . . y_train = trainDf[&#39;Label&#39;].values y_test = testDf[&#39;Label&#39;].values . from transformers import AdamWeightDecay,get_cosine_schedule_with_warmup . def modelDBERT(): input = Input(shape=(300,),dtype=&#39;int32&#39;) bert.trainable=True bertOp = bert(input)[0] cls_token = bertOp[:, 0, :] dropout=Dropout(0.1)(cls_token) dense1=Dense(128,activation=&#39;relu&#39;)(dropout) out = Dense(1, activation=&#39;sigmoid&#39;)(dense1) model = Model(inputs=input, outputs=out) model.compile(Adam(lr=3e-5), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model . Here we just add a bit of dropout to somehow avoid overfiting and a dense layer to fine tune. . modelbert=modelDBERT() . modelbert.summary() . Model: &#34;functional_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 300)] 0 _________________________________________________________________ tf_distil_bert_model (TFDist multiple 66362880 _________________________________________________________________ tf_op_layer_strided_slice_2 [(None, 768)] 0 _________________________________________________________________ dropout_21 (Dropout) (None, 768) 0 _________________________________________________________________ dense_4 (Dense) (None, 128) 98432 _________________________________________________________________ dense_5 (Dense) (None, 1) 129 ================================================================= Total params: 66,461,441 Trainable params: 66,461,441 Non-trainable params: 0 _________________________________________________________________ . modelbert.fit(x_train,y_train,validation_split=0.1,epochs=1,batch_size=16,verbose=1,shuffle=True) . WARNING:tensorflow:Model was constructed with shape (None, 300) for input Tensor(&#34;input_3:0&#34;, shape=(None, 300), dtype=int32), but it was called on an input with incompatible shape (None, 400). WARNING:tensorflow:Model was constructed with shape (None, 300) for input Tensor(&#34;input_3:0&#34;, shape=(None, 300), dtype=int32), but it was called on an input with incompatible shape (None, 400). 1407/1407 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.8800WARNING:tensorflow:Model was constructed with shape (None, 300) for input Tensor(&#34;input_3:0&#34;, shape=(None, 300), dtype=int32), but it was called on an input with incompatible shape (None, 400). 1407/1407 [==============================] - 587s 418ms/step - loss: 0.2798 - accuracy: 0.8800 - val_loss: 0.1992 - val_accuracy: 0.9228 . &lt;tensorflow.python.keras.callbacks.History at 0x7f1180c0efd0&gt; . modelbert.evaluate(x_test,y_test) . 782/782 [==============================] - 196s 250ms/step - loss: 0.1960 - accuracy: 0.9227 . [0.19601084291934967, 0.9226800203323364] . Accuracy wise we get the best results for bert. But one thing which i noticed was that pure fine tuning (freezing the bert layer) and tuning extra added layers doesnt work very well. what works is we update the whole model with a small learning rate so that the weights of bert are not disturbed much .",
            "url": "https://ashish244co.github.io/blog/jupyter/2020/12/30/sentiment-classification-IMDB.html",
            "relUrl": "/jupyter/2020/12/30/sentiment-classification-IMDB.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ashish244co.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ashish244co.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Ashish Kashav. . Find more details here: LinkedIn [^1]. .",
          "url": "https://ashish244co.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashish244co.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}