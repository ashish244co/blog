{
  
    
        "post0": {
            "title": "Bank Loan Prediction",
            "content": "Installations . !pip install boruta . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: boruta in /usr/local/lib/python3.7/dist-packages (0.3) Requirement already satisfied: scikit-learn&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.0.2) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.7.3) Requirement already satisfied: numpy&gt;=1.10.4 in /usr/local/lib/python3.7/dist-packages (from boruta) (1.21.6) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.17.1-&gt;boruta) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.17.1-&gt;boruta) (3.1.0) . !pip install hyperopt . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2) Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.3) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.21.6) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.7.3) Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.1.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.64.0) Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0) . !pip install catboost . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.0.6) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2022.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.4.3) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.9) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;catboost) (4.1.1) Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (8.0.1) . !pip install shap . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: shap in /usr/local/lib/python3.7/dist-packages (0.41.0) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5) Requirement already satisfied: packaging&gt;20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6) Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.7/dist-packages (from shap) (0.0.7) Requirement already satisfied: tqdm&gt;4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.0) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0) Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;20.9-&gt;shap) (3.0.9) Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba-&gt;shap) (57.4.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba-&gt;shap) (0.34.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;shap) (2022.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;shap) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;shap) (1.15.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;shap) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;shap) (1.1.0) . Importing libraries . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from xgboost import XGBClassifier from boruta import BorutaPy from hyperopt import tpe, hp, fmin, STATUS_OK,Trials from hyperopt.pyll.base import scope from sklearn.model_selection import cross_val_score from sklearn.metrics import classification_report from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import f1_score from hyperopt import space_eval from catboost import CatBoostClassifier import joblib import warnings warnings.filterwarnings(&quot;ignore&quot;) import shap shap.initjs() . Data preprocessing and EDA . credit=pd.read_csv(&quot;credit_applications.csv&quot;) . cust=pd.read_csv(&quot;customers.csv&quot;) . cust.head() . Unnamed: 0 client_nr yearmonth total_nr_trx nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance CRG . 0 1 | 1 | 201401 | 97 | 50 | 6527929 | 47 | 7454863 | -7914288 | 25110651 | 1.0 | . 1 2 | 1 | 201402 | 88 | 59 | 3475918 | 29 | 1895848 | -8448513 | 25036651 | 1.0 | . 2 3 | 1 | 201403 | 96 | 62 | 31316405 | 34 | 20083583 | -10347650 | 18020151 | 1.0 | . 3 4 | 1 | 201404 | 83 | 53 | 18669967 | 30 | 1091295 | -15385039 | 13318200 | 1.0 | . 4 5 | 1 | 201405 | 94 | 54 | 2893905 | 40 | 2034075 | -15682170 | 2350000 | 1.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; cust.drop([&#39;Unnamed: 0&#39;,&#39;client_nr&#39;,&#39;yearmonth&#39;,&#39;CRG&#39;],axis=1).describe() . total_nr_trx nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance . count 29996.000000 | 29996.000000 | 2.999600e+04 | 29996.000000 | 2.999600e+04 | 2.999600e+04 | 2.999600e+04 | . mean 166.427957 | 75.785571 | 1.121290e+07 | 90.642386 | 1.126906e+07 | -5.523773e+06 | 3.752693e+06 | . std 220.947519 | 60.063496 | 1.617596e+07 | 192.244770 | 1.624998e+07 | 1.357517e+07 | 1.616937e+07 | . min 1.000000 | 0.000000 | 0.000000e+00 | 0.000000 | 0.000000e+00 | -3.467127e+08 | -2.485206e+08 | . 25% 76.000000 | 38.000000 | 3.072750e+06 | 33.000000 | 3.148068e+06 | -7.895864e+06 | -1.868002e+06 | . 50% 129.000000 | 66.000000 | 6.822769e+06 | 56.000000 | 6.934694e+06 | -2.957198e+06 | 1.040998e+06 | . 75% 205.000000 | 101.000000 | 1.386656e+07 | 102.000000 | 1.394257e+07 | 1.690275e+04 | 5.806224e+06 | . max 6341.000000 | 1590.000000 | 7.980480e+08 | 6325.000000 | 8.775321e+08 | 2.109783e+08 | 3.722319e+08 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; some extreme values prsent for transactional behavior . cust.isna().sum() . Unnamed: 0 0 client_nr 0 yearmonth 0 total_nr_trx 0 nr_debit_trx 0 volume_debit_trx 0 nr_credit_trx 0 volume_credit_trx 0 min_balance 0 max_balance 0 CRG 5537 dtype: int64 . seems like Credit risk group variable have some missing values. Instead of dropping or filling with the mode we shall assign a different category to them. . cust.groupby([&#39;client_nr&#39;])[&#39;yearmonth&#39;].nunique().describe() . count 992.000000 mean 30.237903 std 5.742178 min 1.000000 25% 32.000000 50% 32.000000 75% 32.000000 max 32.000000 Name: yearmonth, dtype: float64 . this show some clients do not have full time series. We can drop those clients but we can keep as well, its a subjective choice. . . . . full=cust.drop(&quot;Unnamed: 0&quot;,axis=1).merge(credit.drop(&quot;Unnamed: 0&quot;,axis=1),on=[&#39;yearmonth&#39;,&#39;client_nr&#39;]) . full[&#39;Date&#39;]=pd.to_datetime(full[&#39;yearmonth&#39;].astype(str), format=&#39;%Y%m&#39;) . max(full[&#39;Date&#39;]) . Timestamp(&#39;2016-08-01 00:00:00&#39;) . min(full[&#39;Date&#39;]) . Timestamp(&#39;2014-01-01 00:00:00&#39;) . full.groupby(&#39;Date&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index().sort_values(by=&#39;nr_credit_applications&#39;,ascending=False) . Date nr_credit_applications . 18 2015-07-01 | 120 | . 17 2015-06-01 | 120 | . 21 2015-10-01 | 99 | . 11 2014-12-01 | 96 | . 4 2014-05-01 | 96 | . 8 2014-09-01 | 96 | . 5 2014-06-01 | 91 | . 6 2014-07-01 | 89 | . 16 2015-05-01 | 88 | . 22 2015-11-01 | 87 | . 14 2015-03-01 | 87 | . 10 2014-11-01 | 87 | . 9 2014-10-01 | 87 | . 12 2015-01-01 | 86 | . 20 2015-09-01 | 82 | . 3 2014-04-01 | 82 | . 28 2016-05-01 | 81 | . 23 2015-12-01 | 81 | . 15 2015-04-01 | 80 | . 7 2014-08-01 | 80 | . 24 2016-01-01 | 79 | . 29 2016-06-01 | 78 | . 26 2016-03-01 | 77 | . 1 2014-02-01 | 73 | . 2 2014-03-01 | 72 | . 19 2015-08-01 | 70 | . 13 2015-02-01 | 68 | . 27 2016-04-01 | 67 | . 0 2014-01-01 | 66 | . 30 2016-07-01 | 59 | . 25 2016-02-01 | 45 | . 31 2016-08-01 | 43 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; full=full.sort_values(by=[&#39;client_nr&#39;,&#39;Date&#39;]) . full[&#39;month&#39;]=pd.DatetimeIndex(full[&#39;Date&#39;]).month . full[&#39;month&#39;]=full[&#39;month&#39;].astype(&#39;category&#39;) . full.groupby(&#39;month&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index().sort_values(by=&#39;nr_credit_applications&#39;,ascending=False) . month nr_credit_applications . 5 6 | 289 | . 6 7 | 268 | . 4 5 | 265 | . 2 3 | 236 | . 0 1 | 231 | . 3 4 | 229 | . 7 8 | 193 | . 1 2 | 186 | . 9 10 | 186 | . 8 9 | 178 | . 11 12 | 177 | . 10 11 | 174 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; generally we see that from august there is a drop in applications of loans. so we can use the variable month as it can be useful. . full[&#39;cash_flow&#39;]=full[&#39;volume_credit_trx&#39;]-full[&#39;volume_debit_trx&#39;] . we also caculate a feature called cash flow which define the net earnings of the clients for that timestamp and which keeps on getting accumalated. . full.drop([&#39;yearmonth&#39;],axis=1,inplace=True) . full[&#39;month&#39;]=full[&#39;month&#39;].astype(str) . full[&#39;CRG&#39;]=full[&#39;CRG&#39;].astype(&#39;category&#39;) full[&#39;CRG&#39;].unique() . [1.0, 4.0, 7.0, 2.0, 3.0, 5.0, NaN] Categories (6, float64): [1.0, 2.0, 3.0, 4.0, 5.0, 7.0] . full[&#39;CRG&#39;] = full[&#39;CRG&#39;].cat.add_categories(-1) full[&#39;CRG&#39;].fillna(-1, inplace=True) . we also try to inspect how crg groups behave differently for different transactions dimensions. . sns.boxplot(x=&#39;CRG&#39;,y=&#39;min_balance&#39;,data=full) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0887706d90&gt; . sns.boxplot(x=&#39;CRG&#39;,y=&#39;cash_flow&#39;,data=full) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f088746ca50&gt; . full.groupby(&#39;CRG&#39;)[&#39;client_nr&#39;].nunique().reset_index().sort_values(by=&#39;client_nr&#39;,ascending=False) . CRG client_nr . 2 3.0 | 291 | . 6 -1.0 | 194 | . 0 1.0 | 140 | . 5 7.0 | 138 | . 1 2.0 | 136 | . 3 4.0 | 68 | . 4 5.0 | 25 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; full.groupby(&#39;CRG&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index().sort_values(by=&#39;nr_credit_applications&#39;,ascending=False) . CRG nr_credit_applications . 2 3.0 | 756 | . 5 7.0 | 614 | . 1 2.0 | 415 | . 3 4.0 | 290 | . 6 -1.0 | 217 | . 0 1.0 | 186 | . 4 5.0 | 134 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; there is good amount of variation with respect to the column crg so it is worthwile to keep the variable. on a general level it might be the case that the crg group 3 must be more trustable as compared to group 7. . diff=full[full[&#39;credit_application&#39;]==1] . diff[&#39;Prev&#39;]=diff.groupby(&#39;client_nr&#39;)[&#39;Date&#39;].shift(1) . diff=diff[[&#39;client_nr&#39;,&#39;Date&#39;,&#39;Prev&#39;]].fillna(&#39;2014-01-01&#39;) . diff[&#39;consec&#39;]=(diff[&#39;Date&#39;]-diff[&#39;Prev&#39;])// np.timedelta64(1, &#39;M&#39;) . sns.distplot(diff[&#39;consec&#39;],kde=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f08875e53d0&gt; . diff.head() . client_nr Date Prev consec . 70 3 | 2014-07-01 | 2014-01-01 | 5 | . 81 3 | 2015-06-01 | 2014-07-01 | 11 | . 87 3 | 2015-12-01 | 2015-06-01 | 6 | . 95 3 | 2016-08-01 | 2015-12-01 | 8 | . 126 4 | 2016-07-01 | 2014-01-01 | 29 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; diff[&#39;consec&#39;].describe() . count 2025.000000 mean 5.841975 std 5.668782 min 0.000000 25% 2.000000 50% 4.000000 75% 8.000000 max 30.000000 Name: consec, dtype: float64 . from above we see that generally different between consective loans can be generally around 8 months. This information is useful to decide for the forecasting window that we should choose. . full[&#39;avg_local_credit&#39;]=full[&#39;volume_credit_trx&#39;]/full[&#39;nr_credit_trx&#39;] full[&#39;avg_local_debit&#39;]=full[&#39;volume_debit_trx&#39;]/full[&#39;nr_debit_trx&#39;] . full[&#39;nr_credit_applications_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].cumsum() . full[&#39;cash_flow_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;cash_flow&#39;].cumsum() . full[&#39;nr_debit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;nr_debit_trx&#39;].cumsum() full[&#39;nr_credit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_trx&#39;].cumsum() . full[&#39;volume_debit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;volume_debit_trx&#39;].cumsum() full[&#39;volume_credit_trx_cumsum&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;volume_credit_trx&#39;].cumsum() full[&#39;avg_global_credit&#39;]=full[&#39;volume_credit_trx_cumsum&#39;]/full[&#39;nr_credit_trx_cumsum&#39;] full[&#39;avg_global_debit&#39;]=full[&#39;volume_debit_trx_cumsum&#39;]/full[&#39;nr_debit_trx_cumsum&#39;] . full[&#39;global_min_balance&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;min_balance&#39;].cummin() . full[&#39;global_max_balance&#39;]=full.groupby(&#39;client_nr&#39;)[&#39;max_balance&#39;].cummax() . def create_shifted_lables(data,n): data[&#39;T+&#39;+str(n)+&#39;_nr_credit_applications&#39;]=data.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].shift(-n) . we calcalate some variables as above on a cumulative value so that at each time stamp we can also have historical information with use which will help us in predicting the future actions of the clients. . for i in range(1,13): create_shifted_lables(full,i) . we manipulate data to create target variable such as number of loans taken in the future at t+n timestamp which will be use to predict. . full.columns . Index([&#39;client_nr&#39;, &#39;total_nr_trx&#39;, &#39;nr_debit_trx&#39;, &#39;volume_debit_trx&#39;, &#39;nr_credit_trx&#39;, &#39;volume_credit_trx&#39;, &#39;min_balance&#39;, &#39;max_balance&#39;, &#39;CRG&#39;, &#39;credit_application&#39;, &#39;nr_credit_applications&#39;, &#39;Date&#39;, &#39;month&#39;, &#39;cash_flow&#39;, &#39;avg_local_credit&#39;, &#39;avg_local_debit&#39;, &#39;nr_credit_applications_cumsum&#39;, &#39;cash_flow_cumsum&#39;, &#39;nr_debit_trx_cumsum&#39;, &#39;nr_credit_trx_cumsum&#39;, &#39;volume_debit_trx_cumsum&#39;, &#39;volume_credit_trx_cumsum&#39;, &#39;avg_global_credit&#39;, &#39;avg_global_debit&#39;, &#39;global_min_balance&#39;, &#39;global_max_balance&#39;, &#39;T+1_nr_credit_applications&#39;, &#39;T+2_nr_credit_applications&#39;, &#39;T+3_nr_credit_applications&#39;, &#39;T+4_nr_credit_applications&#39;, &#39;T+5_nr_credit_applications&#39;, &#39;T+6_nr_credit_applications&#39;, &#39;T+7_nr_credit_applications&#39;, &#39;T+8_nr_credit_applications&#39;, &#39;T+9_nr_credit_applications&#39;, &#39;T+10_nr_credit_applications&#39;, &#39;T+11_nr_credit_applications&#39;, &#39;T+12_nr_credit_applications&#39;], dtype=&#39;object&#39;) . full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].sum().describe() . count 992.000000 mean 2.633065 std 3.412808 min 0.000000 25% 0.000000 50% 1.000000 75% 4.000000 max 27.000000 Name: nr_credit_applications, dtype: float64 . we see that moslty people took number of loans between 1 to 4. We will take two such customers to see if we can find reason or pattern why they took specified number of loans. . clients=full.groupby(&#39;client_nr&#39;)[&#39;nr_credit_applications&#39;].sum().reset_index() . clients[clients[&#39;nr_credit_applications&#39;]==4] . client_nr nr_credit_applications . 40 41 | 4 | . 46 47 | 4 | . 60 61 | 4 | . 73 74 | 4 | . 82 83 | 4 | . ... ... | ... | . 890 895 | 4 | . 909 914 | 4 | . 912 917 | 4 | . 939 945 | 4 | . 965 973 | 4 | . 78 rows × 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; cleints_one_loan=clients[clients[&#39;nr_credit_applications&#39;]==1] . cleints_one_loan.head() . client_nr nr_credit_applications . 3 4 | 1 | . 5 6 | 1 | . 7 8 | 1 | . 24 25 | 1 | . 30 31 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Client with only 1 loan . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==6], x=&quot;Date&quot;, y=&#39;cash_flow_cumsum&#39;,hue=&#39;credit_application&#39;) plt.show() . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==6], x=&quot;Date&quot;, y=&#39;min_balance&#39;,hue=&#39;credit_application&#39;) plt.show() . client with 4 loans . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==47], x=&quot;Date&quot;, y=&#39;cash_flow_cumsum&#39;,hue=&#39;credit_application&#39;) plt.show() . sns.scatterplot(data=full[full[&#39;client_nr&#39;]==47], x=&quot;Date&quot;, y=&#39;min_balance&#39;,hue=&#39;credit_application&#39;) plt.show() . one interesting pattern to see here is that the users who took more loans are the one for whome cash flow cumulative increase which means for whome income increase more. . full[[&#39;T+1_nr_credit_applications&#39;, &#39;T+2_nr_credit_applications&#39;, &#39;T+3_nr_credit_applications&#39;, &#39;T+4_nr_credit_applications&#39;, &#39;T+5_nr_credit_applications&#39;, &#39;T+6_nr_credit_applications&#39;, &#39;T+7_nr_credit_applications&#39;, &#39;T+8_nr_credit_applications&#39;, &#39;T+9_nr_credit_applications&#39;, &#39;T+10_nr_credit_applications&#39;, &#39;T+11_nr_credit_applications&#39;, &#39;T+12_nr_credit_applications&#39;]]=full[[&#39;T+1_nr_credit_applications&#39;, &#39;T+2_nr_credit_applications&#39;, &#39;T+3_nr_credit_applications&#39;, &#39;T+4_nr_credit_applications&#39;, &#39;T+5_nr_credit_applications&#39;, &#39;T+6_nr_credit_applications&#39;, &#39;T+7_nr_credit_applications&#39;, &#39;T+8_nr_credit_applications&#39;, &#39;T+9_nr_credit_applications&#39;, &#39;T+10_nr_credit_applications&#39;, &#39;T+11_nr_credit_applications&#39;, &#39;T+12_nr_credit_applications&#39;]].fillna(0) . full[[&#39;avg_local_credit&#39;,&#39;avg_local_debit&#39;,&#39;avg_global_credit&#39;,&#39;avg_global_debit&#39;]]=full[[&#39;avg_local_credit&#39;,&#39;avg_local_debit&#39;,&#39;avg_global_credit&#39;,&#39;avg_global_debit&#39;]].fillna(0) . full.isna().sum() . client_nr 0 total_nr_trx 0 nr_debit_trx 0 volume_debit_trx 0 nr_credit_trx 0 volume_credit_trx 0 min_balance 0 max_balance 0 CRG 0 credit_application 0 nr_credit_applications 0 Date 0 month 0 cash_flow 0 avg_local_credit 0 avg_local_debit 0 nr_credit_applications_cumsum 0 cash_flow_cumsum 0 nr_debit_trx_cumsum 0 nr_credit_trx_cumsum 0 volume_debit_trx_cumsum 0 volume_credit_trx_cumsum 0 avg_global_credit 0 avg_global_debit 0 global_min_balance 0 global_max_balance 0 T+1_nr_credit_applications 0 T+2_nr_credit_applications 0 T+3_nr_credit_applications 0 T+4_nr_credit_applications 0 T+5_nr_credit_applications 0 T+6_nr_credit_applications 0 T+7_nr_credit_applications 0 T+8_nr_credit_applications 0 T+9_nr_credit_applications 0 T+10_nr_credit_applications 0 T+11_nr_credit_applications 0 T+12_nr_credit_applications 0 dtype: int64 . full[&#39;CRG&#39;]=full[&#39;CRG&#39;].astype(str) . instead of predicting number of exact loans at t+n timestep we instead use number of loans in next month, next 3 months, next 6 month and next 12 months for better predictibility. . full[&#39;next_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;] . full[&#39;next_3_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;]+full[&#39;T+2_nr_credit_applications&#39;]+full[&#39;T+3_nr_credit_applications&#39;] . full[&#39;next_6_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;]+full[&#39;T+2_nr_credit_applications&#39;]+full[&#39;T+3_nr_credit_applications&#39;]+full[&#39;T+4_nr_credit_applications&#39;]+full[&#39;T+5_nr_credit_applications&#39;]+full[&#39;T+6_nr_credit_applications&#39;] . full[&#39;next_12_month_nr_credit_applications&#39;]=full[&#39;T+1_nr_credit_applications&#39;]+full[&#39;T+2_nr_credit_applications&#39;]+full[&#39;T+3_nr_credit_applications&#39;]+full[&#39;T+4_nr_credit_applications&#39;]+full[&#39;T+5_nr_credit_applications&#39;]+full[&#39;T+6_nr_credit_applications&#39;]+full[&#39;T+7_nr_credit_applications&#39;]+full[&#39;T+8_nr_credit_applications&#39;]+full[&#39;T+9_nr_credit_applications&#39;]+full[&#39;T+10_nr_credit_applications&#39;]+full[&#39;T+11_nr_credit_applications&#39;]+full[&#39;T+12_nr_credit_applications&#39;] . max(full[&#39;Date&#39;]) . Timestamp(&#39;2016-08-01 00:00:00&#39;) . to showcase the thought process we will predict for future 6 months. . full=full[full[&#39;Date&#39;]&lt;=&#39;2016-02-01&#39;] . full.head() . client_nr total_nr_trx nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance CRG credit_application ... T+7_nr_credit_applications T+8_nr_credit_applications T+9_nr_credit_applications T+10_nr_credit_applications T+11_nr_credit_applications T+12_nr_credit_applications next_month_nr_credit_applications next_3_month_nr_credit_applications next_6_month_nr_credit_applications next_12_month_nr_credit_applications . 0 1 | 97 | 50 | 6527929 | 47 | 7454863 | -7914288 | 25110651 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 1 | 88 | 59 | 3475918 | 29 | 1895848 | -8448513 | 25036651 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1 | 96 | 62 | 31316405 | 34 | 20083583 | -10347650 | 18020151 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1 | 83 | 53 | 18669967 | 30 | 1091295 | -15385039 | 13318200 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1 | 94 | 54 | 2893905 | 40 | 2034075 | -15682170 | 2350000 | 1.0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 42 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X=full.drop([&#39;T+1_nr_credit_applications&#39;,&#39;T+2_nr_credit_applications&#39;,&#39;T+3_nr_credit_applications&#39;,&#39;T+4_nr_credit_applications&#39;,&#39;T+5_nr_credit_applications&#39;,&#39;T+6_nr_credit_applications&#39;,&#39;T+7_nr_credit_applications&#39;,&#39;T+8_nr_credit_applications&#39;,&#39;T+9_nr_credit_applications&#39;,&#39;T+10_nr_credit_applications&#39;,&#39;T+11_nr_credit_applications&#39;,&#39;T+12_nr_credit_applications&#39;],axis=1) . X.columns . Index([&#39;client_nr&#39;, &#39;total_nr_trx&#39;, &#39;nr_debit_trx&#39;, &#39;volume_debit_trx&#39;, &#39;nr_credit_trx&#39;, &#39;volume_credit_trx&#39;, &#39;min_balance&#39;, &#39;max_balance&#39;, &#39;CRG&#39;, &#39;credit_application&#39;, &#39;nr_credit_applications&#39;, &#39;Date&#39;, &#39;month&#39;, &#39;cash_flow&#39;, &#39;avg_local_credit&#39;, &#39;avg_local_debit&#39;, &#39;nr_credit_applications_cumsum&#39;, &#39;cash_flow_cumsum&#39;, &#39;nr_debit_trx_cumsum&#39;, &#39;nr_credit_trx_cumsum&#39;, &#39;volume_debit_trx_cumsum&#39;, &#39;volume_credit_trx_cumsum&#39;, &#39;avg_global_credit&#39;, &#39;avg_global_debit&#39;, &#39;global_min_balance&#39;, &#39;global_max_balance&#39;, &#39;next_month_nr_credit_applications&#39;, &#39;next_3_month_nr_credit_applications&#39;, &#39;next_6_month_nr_credit_applications&#39;, &#39;next_12_month_nr_credit_applications&#39;], dtype=&#39;object&#39;) . X=X.drop(columns= [&#39;next_month_nr_credit_applications&#39;,&#39;next_3_month_nr_credit_applications&#39;,&#39;next_6_month_nr_credit_applications&#39;,&#39;next_12_month_nr_credit_applications&#39;,&#39;client_nr&#39;,&#39;Date&#39;,&#39;credit_application&#39;, &#39;total_nr_trx&#39;,&#39;volume_debit_trx_cumsum&#39;,&#39;nr_credit_trx_cumsum&#39; ],axis=1) . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 24370 entries, 0 to 29989 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 nr_debit_trx 24370 non-null int64 1 volume_debit_trx 24370 non-null int64 2 nr_credit_trx 24370 non-null int64 3 volume_credit_trx 24370 non-null int64 4 min_balance 24370 non-null int64 5 max_balance 24370 non-null int64 6 CRG 24370 non-null object 7 nr_credit_applications 24370 non-null int64 8 month 24370 non-null object 9 cash_flow 24370 non-null int64 10 avg_local_credit 24370 non-null float64 11 avg_local_debit 24370 non-null float64 12 nr_credit_applications_cumsum 24370 non-null int64 13 cash_flow_cumsum 24370 non-null int64 14 nr_debit_trx_cumsum 24370 non-null int64 15 volume_credit_trx_cumsum 24370 non-null int64 16 avg_global_credit 24370 non-null float64 17 avg_global_debit 24370 non-null float64 18 global_min_balance 24370 non-null int64 19 global_max_balance 24370 non-null int64 dtypes: float64(4), int64(14), object(2) memory usage: 3.9+ MB . we binarise the variable . y=(full[[&#39;next_6_month_nr_credit_applications&#39;]]&gt;=1).astype(int) . X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1) . we create a train dataset, validation set ( for hyperparameter tuning) and a test set. . X_val=X_train[-1000:] y_val=y_train[-1000:] X_train=X_train[:-1000] y_train=y_train[:-1000] . Modelling . rf=RandomForestClassifier(n_estimators=500,max_depth=10,class_weight=&#39;balanced&#39;) rf.fit(X_train,y_train) %time print(classification_report(y_test, rf.predict_proba(X_test)[:,1]&gt;=0.5, target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 3 µs, sys: 0 ns, total: 3 µs Wall time: 6.2 µs precision recall f1-score support 0 0.89 0.78 0.83 1685 1 0.61 0.79 0.69 752 accuracy 0.78 2437 macro avg 0.75 0.78 0.76 2437 weighted avg 0.80 0.78 0.78 2437 . above we casually fit a model lets see how much score we can improve with further hyperparameter tuning and ensembling. . space_RF = { &quot;n_estimators&quot;: hp.choice(&quot;n_estimators&quot;, [300, 400,500,600,700,800,900,1000]), &quot;max_depth&quot;: hp.quniform(&quot;max_depth&quot;, 5, 15,1) } . def parameter_tuning_RF(params): clf = RandomForestClassifier(**params,class_weight=&#39;balanced&#39;,n_jobs=-1) f1 = cross_val_score(clf, X_train, y_train,scoring=&quot;f1_weighted&quot;,cv=3).mean() return {&quot;loss&quot;: -f1, &quot;status&quot;: STATUS_OK} . trials = Trials() best_RF = fmin( fn=parameter_tuning_RF, space = space_RF, algo=tpe.suggest, max_evals=30, trials=trials ) print(&quot;Best: {}&quot;.format(best_RF)) . 100%|██████████| 30/30 [27:21&lt;00:00, 54.73s/it, best loss: -0.8248557350549816] Best: {&#39;max_depth&#39;: 15.0, &#39;n_estimators&#39;: 3} . hyperparams = space_eval(space_RF,best_RF) . hyperparams . {&#39;max_depth&#39;: 15.0, &#39;n_estimators&#39;: 600} . we also perform feature selection and feature ranking. . def feature_selector(clf,x,y): boruta = BorutaPy(estimator = clf, n_estimators = &#39;auto&#39;, max_iter = 50) boruta.fit(x.values, y.values) important = list(X.columns[boruta.support_]) print(f&quot;important: {important}&quot;) tentative = list(X.columns[boruta.support_weak_]) print(f&quot;unconfirmed: {tentative}&quot;) unimportant = list(X.columns[~(boruta.support_ | boruta.support_weak_)]) print(f&quot;unimportant: {unimportant}&quot;) important.extend(tentative) return important,boruta.ranking_ . clf=RandomForestClassifier(**hyperparams,class_weight=&#39;balanced&#39;,n_jobs=-1) . featuresrf,ranking=feature_selector(clf,X_train,y_train) . important: [&#39;nr_debit_trx&#39;, &#39;volume_debit_trx&#39;, &#39;nr_credit_trx&#39;, &#39;volume_credit_trx&#39;, &#39;min_balance&#39;, &#39;max_balance&#39;, &#39;CRG&#39;, &#39;avg_local_credit&#39;, &#39;avg_local_debit&#39;, &#39;nr_credit_applications_cumsum&#39;, &#39;cash_flow_cumsum&#39;, &#39;nr_debit_trx_cumsum&#39;, &#39;volume_credit_trx_cumsum&#39;, &#39;avg_global_credit&#39;, &#39;avg_global_debit&#39;, &#39;global_min_balance&#39;, &#39;global_max_balance&#39;] unconfirmed: [&#39;cash_flow&#39;] unimportant: [&#39;nr_credit_applications&#39;, &#39;month&#39;] . X_train_rf=X_train.loc[:,featuresrf] X_val_rf=X_val.loc[:,featuresrf] X_test_rf=X_test.loc[:,featuresrf] . clf=RandomForestClassifier(**hyperparams,class_weight=&#39;balanced&#39;,n_jobs=-1) . clf.fit(X_train_rf,y_train) . RandomForestClassifier(class_weight=&#39;balanced&#39;, max_depth=15.0, n_estimators=600, n_jobs=-1) . we also try to find the best threshold which will emphasise on the weighted f1 score. . maxargrf=0.4 max_score=-1 for i in np.arange(0.4, 0.7, 0.01): score=f1_score(y_val, clf.predict_proba(X_val_rf)[:,1]&gt;=i, average=&#39;weighted&#39;) if(score&gt;max_score): max_score=score maxargrf=i . maxargrf . 0.5400000000000001 . %time print(classification_report(y_test, clf.predict_proba(X_test_rf)[:,1]&gt;=maxargrf, target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 5 µs, sys: 0 ns, total: 5 µs Wall time: 9.78 µs precision recall f1-score support 0 0.89 0.91 0.90 1685 1 0.78 0.76 0.77 752 accuracy 0.86 2437 macro avg 0.84 0.83 0.83 2437 weighted avg 0.86 0.86 0.86 2437 . len(y[y[&#39;next_6_month_nr_credit_applications&#39;]==1]) . 7523 . len(y[y[&#39;next_6_month_nr_credit_applications&#39;]==0]) . 16847 . for catboost classifier we enable early stopping and scale pos parameter which takes care of imbalanced classes. . clf2 = CatBoostClassifier(iterations=10000, learning_rate= 0.01,use_best_model=True,od_type=&#39;Iter&#39;,od_wait=50,scale_pos_weight=2.3) . clf2.fit( X_train, y_train, cat_features=[6,8], plot=True, eval_set=(X_val ,y_val) ) . maxargcat=0.4 max_score=-1 for i in np.arange(0.4, 0.7, 0.01): score=f1_score(y_val, clf2.predict_proba(X_val)[:,1]&gt;=i, average=&#39;weighted&#39;) if(score&gt;max_score): max_score=score maxargcat=i . %time print(classification_report(y_test, clf2.predict(X_test), target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 5 µs, sys: 1 µs, total: 6 µs Wall time: 9.78 µs precision recall f1-score support 0 0.94 0.86 0.90 1685 1 0.74 0.87 0.80 752 accuracy 0.86 2437 macro avg 0.84 0.86 0.85 2437 weighted avg 0.87 0.86 0.87 2437 . we optimise for a mix paramter which optimises the mix of random forest and catboost on validation set. . scoree=-1 mix=0 for i in np.arange(0, 1, 0.1): j=1-i y_pred=(i*clf.predict_proba(X_val_rf)[:,1]+ j*(clf2.predict_proba(X_val)[:,1]))/(i+j) score=f1_score(y_val, (y_pred&gt;=0.5).astype(int), average=&#39;weighted&#39;) if(score&gt;scoree): scoree=score mix=i . mix . 0.30000000000000004 . y_pred=(mix*clf.predict_proba(X_test_rf)[:,1]+ (1-mix)*(clf2.predict_proba(X_test)[:,1])) . %time print(classification_report(y_test, (y_pred&gt;=0.5).astype(int), target_names=[&quot;0&quot;,&quot;1&quot;])) . CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs Wall time: 5.72 µs precision recall f1-score support 0 0.94 0.87 0.90 1685 1 0.75 0.87 0.80 752 accuracy 0.87 2437 macro avg 0.84 0.87 0.85 2437 weighted avg 0.88 0.87 0.87 2437 . so our model seems to be good just the precision is a bit less as compared to other metrics which can be optimised by raising the threshold as per business needs rest looks good. . Result Analysis . explainer = shap.TreeExplainer(clf2) shap_values = explainer.shap_values(X_train) . shap.summary_plot(shap_values, X_train) . above shapley plot shows us that how variables affect the output. Its interesting to see the relationship of min/max balance, past credit applications on the output which seems to make sense. . for implementation we can rank the customers based on how high the probablity is to apply for a loans in the coming months, and give attention accordingly. . explainer = shap.TreeExplainer(clf2) shap_values = explainer.shap_values(X_test) shap.summary_plot(shap_values, X_test) . type1=[] type2=[] . from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay . cm=confusion_matrix(y_test, (y_pred&gt;=0.5).astype(int)) . ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf2.classes_).plot() . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f087e42ad50&gt; . type2=[] . for p, a in zip(y_test.values.squeeze(), (y_pred&gt;=0.5).astype(int)): if((p==0) &amp; (a==1)): type2.append(True) else: type2.append(False) . X_test.loc[type2,:].describe() . nr_debit_trx volume_debit_trx nr_credit_trx volume_credit_trx min_balance max_balance nr_credit_applications cash_flow avg_local_credit avg_local_debit nr_credit_applications_cumsum cash_flow_cumsum nr_debit_trx_cumsum volume_credit_trx_cumsum avg_global_credit avg_global_debit global_min_balance global_max_balance . count 222.000000 | 2.220000e+02 | 222.000000 | 2.220000e+02 | 2.220000e+02 | 2.220000e+02 | 222.000000 | 2.220000e+02 | 222.000000 | 222.000000 | 222.000000 | 2.220000e+02 | 222.000000 | 2.220000e+02 | 222.000000 | 222.000000 | 2.220000e+02 | 2.220000e+02 | . mean 82.707207 | 8.441915e+06 | 74.828829 | 8.622403e+06 | -8.890885e+06 | -7.355832e+05 | 0.220721 | 1.804874e+05 | 123149.532152 | 104135.663538 | 2.139640 | -7.015935e+05 | 1010.144144 | 1.100454e+08 | 125585.619152 | 108206.186392 | -1.261369e+07 | 2.174478e+06 | . std 38.614932 | 6.607415e+06 | 56.483510 | 7.500837e+06 | 8.986808e+06 | 8.609833e+06 | 0.504212 | 3.455147e+06 | 94909.954476 | 72395.136775 | 2.405378 | 5.918188e+06 | 862.287869 | 1.156250e+08 | 75136.408686 | 70961.034486 | 1.395708e+07 | 1.120575e+07 | . min 14.000000 | 6.147450e+05 | 15.000000 | 9.090280e+05 | -5.078090e+07 | -4.457249e+07 | 0.000000 | -6.689391e+06 | 28733.409836 | 12404.829787 | 0.000000 | -3.749371e+07 | 29.000000 | 1.591174e+06 | 26002.477612 | 17515.647655 | -7.685539e+07 | -3.702260e+07 | . 25% 51.250000 | 3.651970e+06 | 37.250000 | 3.671908e+06 | -1.231199e+07 | -3.946260e+06 | 0.000000 | -6.404925e+05 | 69125.290164 | 56458.889923 | 0.000000 | -1.532136e+06 | 342.250000 | 2.207029e+07 | 74317.129575 | 59643.994994 | -1.535338e+07 | -2.244168e+06 | . 50% 73.500000 | 7.283650e+06 | 59.500000 | 6.851706e+06 | -7.074746e+06 | 5.875950e+04 | 0.000000 | 7.080000e+03 | 102942.428150 | 78772.354288 | 1.500000 | -2.734825e+05 | 771.000000 | 6.878834e+07 | 112778.034193 | 88494.956764 | -9.051172e+06 | 8.519360e+05 | . 75% 111.000000 | 1.138326e+07 | 95.000000 | 1.139022e+07 | -3.187168e+06 | 1.479230e+06 | 0.000000 | 5.373035e+05 | 143889.871111 | 134017.040762 | 3.000000 | 5.858578e+05 | 1406.750000 | 1.528945e+08 | 145546.242877 | 141165.532783 | -4.013094e+06 | 4.973174e+06 | . max 217.000000 | 4.332326e+07 | 414.000000 | 5.253854e+07 | 4.357142e+06 | 4.048669e+07 | 3.000000 | 4.358051e+07 | 929674.941176 | 378763.500000 | 15.000000 | 4.263622e+07 | 4328.000000 | 5.971536e+08 | 504657.909091 | 425501.725962 | 5.007770e+05 | 5.993425e+07 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; correct_labe1=[] for p, a in zip(y_test.values.squeeze(), (y_pred&gt;=0.5).astype(int)): if((p==0) &amp; (a==0)): correct_labe1.append(True) else: correct_labe1.append(False) . X_test.loc[correct_labe1,:].describe()[[&#39;cash_flow_cumsum&#39;,&#39;min_balance&#39;]] . cash_flow_cumsum min_balance . count 1.463000e+03 | 1.463000e+03 | . mean -2.712885e+05 | -3.102227e+06 | . std 1.504404e+07 | 1.337984e+07 | . min -3.400017e+08 | -2.512930e+08 | . 25% -2.483418e+06 | -5.124114e+06 | . 50% -6.646800e+04 | -3.054610e+05 | . 75% 1.921716e+06 | 1.480795e+05 | . max 1.272466e+08 | 1.551133e+08 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_test.loc[type2,:].describe()[[&#39;cash_flow_cumsum&#39;,&#39;min_balance&#39;]] . cash_flow_cumsum min_balance . count 2.220000e+02 | 2.220000e+02 | . mean -7.015935e+05 | -8.890885e+06 | . std 5.918188e+06 | 8.986808e+06 | . min -3.749371e+07 | -5.078090e+07 | . 25% -1.532136e+06 | -1.231199e+07 | . 50% -2.734825e+05 | -7.074746e+06 | . 75% 5.858578e+05 | -3.187168e+06 | . max 4.263622e+07 | 4.357142e+06 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; sns.distplot(X_test.loc[type2,:].describe()[[&#39;cash_flow_cumsum&#39;]]) sns.distplot(X_test.loc[correct_labe1,:].describe()[[&#39;cash_flow_cumsum&#39;]]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f087fa244d0&gt; . we can see that one possible reason for misclassification (type2) error is that in the test set it was difficult to predict correctly for some examples since distribution of cash_flow_cumsum, min_balance was very wide which kind of hints they were tricky to classify. We can similarly check for other dimensions to inspect why some types of error occured. . . .",
            "url": "https://ashish244co.github.io/blog/machine%20learning/deep%20learning/time%20series/jupyter/2022/07/28/bank_loan_prediction.html",
            "relUrl": "/machine%20learning/deep%20learning/time%20series/jupyter/2022/07/28/bank_loan_prediction.html",
            "date": " • Jul 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "HPC Machine Learning Ensemble",
            "content": "!pip install mpi4py . Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/dist-packages (3.0.3) . %%writefile comm.py from mpi4py import MPI from typing import Tuple from sklearn.linear_model import LogisticRegression from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier from sklearn.preprocessing import StandardScaler import numpy as np import pandas as pd import copy from sklearn.metrics import classification_report def pre_processing(df_: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]: &quot;&quot;&quot;Pre-processing step for classification task This function takes the data frame loaded from above and returns a tuple of the NumPy array. The tuple&#39;s first element is the independent variables, or feature vectors, having a shape of (N, d) where N is the number of observations and d is the number of variables (or columns). The tuple&#39;s second element is that the vector represents the dependent variable or label with the shape in (N,). Finalize this function to pre-process the data frame to be fit in the output spec. Beyond the mechanical conversion between input data-type and output data-type, apply any content-wise pre-processing that is necessary. &quot;&quot;&quot; df=df_.copy(deep=True) categorical=[&#39;job&#39;,&#39;marital&#39;,&#39;education&#39;,&#39;default&#39;,&#39;housing&#39;,&#39;loan&#39;,&#39;poutcome&#39;] corr_matrix = df.corr().abs() upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) dropcols = [column for column in upper.columns if any(upper[column] &gt; 0.7)] dropcols=dropcols+[&#39;contact&#39;] day={&#39;thu&#39;:3,&#39;tue&#39;:1,&#39;wed&#39;:2,&#39;mon&#39;:0,&#39;fri&#39;:4} df[&#39;day_of_week&#39;]=df[&#39;day_of_week&#39;].map(day) month={&#39;may&#39;:1,&#39;jun&#39;:2,&#39;jul&#39;:3,&#39;aug&#39;:4,&#39;sep&#39;:5,&#39;nov&#39;:6} df[&#39;month&#39;]=df[&#39;month&#39;].map(month) df[&#39;week_sin&#39;] = np.sin((df.day_of_week)*(2.*np.pi/5)) df[&#39;wee_cos&#39;] = np.cos((df.day_of_week)*(2.*np.pi/5)) df[&#39;age&#39;] = df[&#39;age&#39;].apply(lambda x: x.replace(&#39;unknown&#39;,&#39;39&#39;)) #str(df[df[&#39;age&#39;]!=&#39;unknown&#39;][&#39;age&#39;].astype(int).median())) df[&#39;age&#39;]=df[&#39;age&#39;].astype(int) df.drop(columns=dropcols,axis=1,inplace=True) df=pd.get_dummies(df,drop_first=True,columns=categorical) return df.drop(&#39;y&#39;,axis=1).values,df[&#39;y&#39;].map({&#39;no&#39;:0,&#39;yes&#39;:1}).values,df.drop(&#39;y&#39;,axis=1).columns def main(): comm = MPI.COMM_WORLD rank = comm.Get_rank() size = comm.Get_size() #print(size) X, y ,columns= pre_processing(pd.read_csv(&#39;moro14_synth.csv&#39;, index_col=0)) n_samples, n_features = X.shape sc=StandardScaler() # split the dataset into train / test rnd_idx = np.random.permutation(n_samples) bound = int(n_samples / 5) newbound=n_samples-bound x_train = X[rnd_idx[:newbound]] y_train = y[rnd_idx[:newbound]] x_test = X[rnd_idx[newbound:]] y_test = y[rnd_idx[newbound:]] xtrn=copy.deepcopy(x_train) xtst=copy.deepcopy(x_test) sc.fit(xtrn) xtrn=sc.transform(xtrn) xtst=sc.transform(xtst) x_train1 = xtrn[:800,:12] y_train1 = y_train[:800] x_train2 = xtrn[800:1600,12:24] y_train2 = y_train[800:1600] x_train3 = xtrn[1600:2200,24:] y_train3 = y_train[1600:2200] clfs = { &#39;LR&#39;: LogisticRegression(), &#39;QDA&#39;: QuadraticDiscriminantAnalysis(), &#39;GNB&#39;: GaussianNB(), &#39;DT&#39;: DecisionTreeClassifier() } # root process if rank == 0: # root process sends data to all other processes data1={ &#39;x&#39;:x_train1, &#39;y&#39;:y_train1, &#39;xt&#39;:xtst[:,:12], &#39;yt&#39;:y_test } data2={ &#39;x&#39;:x_train2, &#39;y&#39;:y_train2, &#39;xt&#39;:xtst[:,12:24], &#39;yt&#39;:y_test } data3={ &#39;x&#39;:x_train3, &#39;y&#39;:y_train3, &#39;xt&#39;:xtst[:,24:], &#39;yt&#39;:y_test, } comm.send(data1, dest=1, tag=1) print(&#39;Process {} sent data:&#39;.format(rank), data1) comm.send(data2, dest=2, tag=2) print(&#39;Process {} sent data:&#39;.format(rank), data2) comm.send(data3, dest=3, tag=3) print(&#39;Process {} sent data:&#39;.format(rank), data3) result1=comm.recv(source=1, tag=5) print(&#39;Process rec data 1&#39;) result2=comm.recv(source=2, tag=6) print(&#39;Process rec data 2&#39;) result3=comm.recv(source=3, tag=7) print(&#39;Process rec data 3&#39;) print(&#39;Mean Result&#39;,classification_report(y_test,np.round((result1+result2+result3)/3))) # non-root processes elif rank==1: # each non-root process receives data from root process data1 = comm.recv(source=0, tag=rank) print(&#39;Process {} received data:&#39;.format(rank), data1) clfs[&#39;LR&#39;].fit(data1[&#39;x&#39;], data1[&#39;y&#39;]) print(&#39;LR&#39;,classification_report(y_test,clfs[&#39;LR&#39;].predict(data1[&#39;xt&#39;]))) comm.send(clfs[&#39;LR&#39;].predict_proba(data1[&#39;xt&#39;])[:, 1], dest=0, tag=5) print(&#39;done 1&#39;) elif rank==2: # each non-root process receives data from root process data1 = comm.recv(source=0, tag=rank) print(&#39;Process {} received data:&#39;.format(rank), data1) clfs[&#39;QDA&#39;].fit(data1[&#39;x&#39;], data1[&#39;y&#39;]) print(&#39;QDA&#39;,classification_report(y_test,clfs[&#39;QDA&#39;].predict(data1[&#39;xt&#39;]))) comm.send(clfs[&#39;QDA&#39;].predict_proba(data1[&#39;xt&#39;])[:, 1], dest=0, tag=6) print(&#39;done 2&#39;) elif rank==3: # each non-root process receives data from root process data1 = comm.recv(source=0, tag=rank) print(&#39;Process {} received data:&#39;.format(rank), data1) clfs[&#39;GNB&#39;].fit(data1[&#39;x&#39;], data1[&#39;y&#39;]) print(&#39;GNB&#39;,classification_report(y_test,clfs[&#39;GNB&#39;].predict(data1[&#39;xt&#39;]))) comm.send(clfs[&#39;GNB&#39;].predict_proba(data1[&#39;xt&#39;])[:, 1], dest=0, tag=7) print(&#39;done 3&#39;) main() . Overwriting comm.py . !mpirun --allow-run-as-root -np 4 python comm.py . 4 4 4 4 Process 0 sent data: {&#39;x&#39;: array([[ 0.75350917, 0.02882345, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.22391057, -1.6862909 , 1.66618377, ..., -1.52816275, 0.5815288 , -0.25264558], [ 0.81867049, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-0.68003979, -1.6862909 , -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 0.55802523, -1.11458612, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.02842662, 0.60052823, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558]]), &#39;y&#39;: array([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]), &#39;xt&#39;: array([[ 0.10189601, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-0.94068506, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 1.07931576, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-1.39681427, 0.60052823, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-1.39681427, 1.17223302, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558], [ 0.16705733, 0.60052823, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process 2 received data: {&#39;x&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], ..., [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285]]), &#39;y&#39;: array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]), &#39;xt&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], ..., [-0.1143927 , 5.28352748, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process 0 sent data: {&#39;x&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], ..., [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, 1.41953351, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285]]), &#39;y&#39;: array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]), &#39;xt&#39;: array([[-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], ..., [-0.1143927 , 5.28352748, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, -0.26636529, ..., -0.12852402, -0.70445678, -0.65595285], [-0.1143927 , -0.18926749, 3.75424288, ..., -0.12852402, -0.70445678, 1.52449983]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process 0 sent data: {&#39;x&#39;: array([[-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., 5.64594654, 0.86192772, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;y&#39;: array([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, QDA precision recall f1-score support 0 0.46 0.29 0.35 299 1 0.48 0.66 0.56 301 accuracy 0.47 600 macro avg 0.47 0.47 0.45 600 weighted avg 0.47 0.47 0.45 600 done 2 Process 3 received data: {&#39;x&#39;: array([[-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., 5.64594654, 0.86192772, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;y&#39;: array([1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1]), &#39;xt&#39;: array([[ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,Process 1 received data: {&#39;x&#39;: array([[ 0.75350917, 0.02882345, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.22391057, -1.6862909 , 1.66618377, ..., -1.52816275, 0.5815288 , -0.25264558], [ 0.81867049, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-0.68003979, -1.6862909 , -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 0.55802523, -1.11458612, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558], [-0.02842662, 0.60052823, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558]]), &#39;y&#39;: array([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]), &#39;xt&#39;: array([[ 0.10189601, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-0.94068506, 0.02882345, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [ 1.07931576, -1.6862909 , 0.87527375, ..., -1.00712355, -1.01384828, -0.25264558], ..., [-1.39681427, 0.60052823, -0.70654628, ..., 1.20003392, 0.5815288 , -0.25264558], [-1.39681427, 1.17223302, 0.08436374, ..., 0.67899472, -1.01384828, -0.25264558], [ 0.16705733, 0.60052823, -1.4974563 , ..., -0.16406442, 1.56752607, -0.25264558]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1]), &#39;xt&#39;: array([[ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], ..., [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996], [ 1.52449983, -0.12340351, -0.17711822, ..., -0.17711822, -1.16019009, -0.20959996], [-0.65595285, -0.12340351, -0.17711822, ..., -0.17711822, 0.86192772, -0.20959996]]), &#39;yt&#39;: array([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} Process rec data 1 Process rec data 2 Process rec data 3 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} LR precision recall f1-score support 0 0.52 0.58 0.55 309 1 0.49 0.44 0.46 291 accuracy 0.51 600 macro avg 0.51 0.51 0.51 600 weighted avg 0.51 0.51 0.51 600 done 1 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0])} GNB precision recall f1-score support 0 0.52 0.87 0.65 309 1 0.51 0.14 0.22 291 accuracy 0.52 600 macro avg 0.51 0.51 0.44 600 weighted avg 0.51 0.52 0.44 600 done 3 Mean Result precision recall f1-score support 0 0.57 0.65 0.61 289 1 0.63 0.55 0.59 311 accuracy 0.60 600 macro avg 0.60 0.60 0.60 600 weighted avg 0.60 0.60 0.60 600 .",
            "url": "https://ashish244co.github.io/blog/hpc/machine%20learning/jupyter/2021/06/01/HPC.html",
            "relUrl": "/hpc/machine%20learning/jupyter/2021/06/01/HPC.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fashion Style Recommendations",
            "content": "from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . import warnings warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning) . !nvidia-smi . Tue Mar 2 13:12:58 2021 +--+ | NVIDIA-SMI 460.39 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 32C P8 28W / 149W | 0MiB / 11441MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . INTRO . First we divide the data into 4 types: . counts , product and user | user info | product info | mix of user and product info | . import os . os.chdir(&#39;drive/My Drive&#39;) . import pandas as pd . data=pd.read_csv(&#39;use_case.rpt&#39;,sep=&#39;;&#39;) . data.head() . ProductID MEMBERID Colour matherial product_group target_group features Category Cluster gender age . 0 010CA1O304 | 147522.0 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.0 | 43.0 | . 1 010CA1O304 | 3755899.0 | NAVY | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.0 | 58.0 | . 2 010CA1O304 | 6326114.0 | BLUE | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.0 | 52.0 | . 3 010CA1Q302 | 9539278.0 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.0 | 44.0 | . 4 010CA1S301 | 1621111.0 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.0 | 52.0 | . collabFiltering=data.groupby([&#39;MEMBERID&#39;,&#39;ProductID&#39;]).count().reset_index()[[&#39;MEMBERID&#39;,&#39;ProductID&#39;,&#39;Colour&#39;]].rename(columns={&quot;Colour&quot;: &quot;Count&quot;}) . userData=data.drop_duplicates(&#39;MEMBERID&#39;)[[&#39;gender&#39;,&#39;age&#39;,&#39;MEMBERID&#39;]].dropna() . We extract two extra metrics like number of buys and unique buys for product and user data both. . userData[&#39;Buys&#39;] = userData[&#39;MEMBERID&#39;].map(data.groupby(&#39;MEMBERID&#39;)[&#39;ProductID&#39;].count()) userData[&#39;UniqueBuys&#39;] = userData[&#39;MEMBERID&#39;].map(data.groupby(&#39;MEMBERID&#39;)[&#39;ProductID&#39;].nunique()) . userData.head() . gender age MEMBERID Buys UniqueBuys . 0 2.0 | 43.0 | 147522.0 | 24 | 8 | . 1 2.0 | 58.0 | 3755899.0 | 10 | 2 | . 2 2.0 | 52.0 | 6326114.0 | 8 | 3 | . 3 2.0 | 44.0 | 9539278.0 | 4 | 2 | . 4 2.0 | 52.0 | 1621111.0 | 11 | 4 | . productData=data.drop_duplicates(&#39;ProductID&#39;).drop(columns=[&#39;gender&#39;,&#39;age&#39;,&#39;MEMBERID&#39;],axis=1).dropna() . productData[&#39;Buys&#39;] = productData[&#39;ProductID&#39;].map(data.groupby(&#39;ProductID&#39;)[&#39;MEMBERID&#39;].count()) productData[&#39;UniqueBuys&#39;] = productData[&#39;ProductID&#39;].map(data.groupby(&#39;ProductID&#39;)[&#39;MEMBERID&#39;].nunique()) . productData.head() . ProductID Colour matherial product_group target_group features Category Cluster Buys UniqueBuys . 0 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 12 | 4 | . 3 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 4 | 2 | . 4 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 117 | 37 | . 18 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 280 | 140 | . 48 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 544 | 272 | . Data Exploration . import seaborn as sns . sns.pairplot(userData[[&#39;gender&#39;,&#39;age&#39;,&#39;Buys&#39;,&#39;UniqueBuys&#39;]], height=5,corner=True) . &lt;seaborn.axisgrid.PairGrid at 0x7f0300609950&gt; . ages are spread between 20 and 60. Moreover we see mid aged people buy more. Gender wise we see gender 2 to be more acitive in purchase activites. . below we can see the count distributions for categorical columns which we have. . import matplotlib.pyplot as plt for col in productData.columns[1:-2]: plt.rcParams[&quot;figure.figsize&quot;] = (10,10) productData[col].value_counts(ascending=True).plot(kind=&#39;barh&#39;) plt.title(col.upper()) plt.show() . productData.head() . ProductID Colour matherial product_group target_group features Category Cluster Buys UniqueBuys . 0 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 12 | 4 | . 3 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 4 | 2 | . 4 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 117 | 37 | . 18 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 280 | 140 | . 48 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 544 | 272 | . from below we can understand t shirts and pants woven were the favourite choices. . sns.catplot(x=&quot;Buys&quot;, y=&quot;Cluster&quot;, data=productData,orient=&#39;h&#39;, height=10) . &lt;seaborn.axisgrid.FacetGrid at 0x7f02f4129dd0&gt; . Clustering . Clustering user and produc groups. . from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np . def select_n_components(var_ratio, goal_var): total_variance = 0 n_components = 0 for explained_variance in var_ratio: total_variance += explained_variance n_components += 1 if total_variance &gt;= goal_var: break return n_components . onehotprods=pd.get_dummies(productData.drop([&#39;ProductID&#39;,&#39;Buys&#39;,&#39;UniqueBuys&#39;],axis=1), drop_first=True) X_sparse = csr_matrix(onehotprods) tsvd = TruncatedSVD(n_components=X_sparse.shape[1]-1) X_tsvd = tsvd.fit(X_sparse) comps=select_n_components( tsvd.explained_variance_ratio_ , 0.95) . from sklearn.preprocessing import normalize userDataNorm=normalize(userData[[&#39;gender&#39;,&#39;age&#39;,&#39;Buys&#39;,&#39;UniqueBuys&#39;]], norm=&#39;l2&#39;) prods=TruncatedSVD(n_components=comps).fit_transform(X_sparse) productDataNorm=normalize(prods, norm=&#39;l2&#39;) . from sklearn.cluster import KMeans from sklearn.metrics import davies_bouldin_score,silhouette_score . import numpy as np . def cluster(datascaled): scoresDBIndex=[] scoresElbow=[] scoresSilhoutte=[] for i in range(2,20): kmeans = KMeans(n_clusters=i, random_state=0,n_jobs=-1).fit(datascaled) datafull=np.column_stack((datascaled,kmeans.labels_)) scoresDBIndex.append(round(davies_bouldin_score(datafull[:,:-1],datafull[:,-1]),2)) scoresElbow.append(round(kmeans.inertia_,2)) scoresSilhoutte.append(round(silhouette_score(datafull[:,:-1],datafull[:,-1]),2)) return scoresDBIndex,scoresElbow,scoresSilhoutte . %%time resUser=cluster(userDataNorm) . CPU times: user 14min 48s, sys: 1min 39s, total: 16min 28s Wall time: 12min 17s . %%time resProd=cluster(productDataNorm) . CPU times: user 21.1 s, sys: 12.7 s, total: 33.7 s Wall time: 17.3 s . import matplotlib.pyplot as plt . for metric in resUser: plt.xticks(np.array(range(len(metric)))+2) plt.bar(x=np.array(range(len(metric)))+2,height=metric) plt.show() . for metric in resProd: plt.xticks(np.array(range(len(metric)))+2) plt.bar(x=np.array(range(len(metric)))+2,height=metric) plt.show() . kmeans = KMeans(n_clusters=4, random_state=0,n_jobs=-1).fit(userDataNorm) dataUser=np.column_stack((userDataNorm,kmeans.labels_)) . kmeans = KMeans(n_clusters=18, random_state=0,n_jobs=-1).fit(productDataNorm) dataProds=np.column_stack((productDataNorm,kmeans.labels_)) . import seaborn as sns . sns.countplot(dataUser[:,-1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd28dbcbe90&gt; . fig, ax = plt.subplots() fig.set_size_inches(8, 8) sns.countplot(dataProds[:,-1],ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd28e890b10&gt; . Popularity based recommender system . probs=getPopularProducts(20) . topProds={} def getPopularProducts(n): for i in range(4): users=dataUser[:,-1]==i uniqueProds=collabFiltering[&#39;MEMBERID&#39;].isin(userData[users][&#39;MEMBERID&#39;].values) topProds[i]=collabFiltering[uniqueProds].groupby(&#39;ProductID&#39;)[&#39;Count&#39;].sum().reset_index().sort_values(by=&#39;Count&#39;,ascending=False)[:n][&#39;ProductID&#39;].values probs=collabFiltering[uniqueProds].groupby(&#39;ProductID&#39;)[&#39;Count&#39;].sum().reset_index().sort_values(by=&#39;Count&#39;,ascending=False)[:n][&#39;Count&#39;].values probs=probs/probs.sum() return probs . def getPredsForUser(topProds,user,probs,topN): indx=userData[&#39;MEMBERID&#39;]==user cluster=dataUser[indx,-1] return np.random.choice(a=topProds[int(cluster)],size=topN,p=probs) . getPredsForUser(topProds,178765,probs,5) . array([&#39;020EO1B314&#39;, &#39;040EE1K418&#39;, &#39;030EO1B310&#39;, &#39;020EE1K308&#39;, &#39;020EE1K309&#39;], dtype=object) . getPredsForUser(topProds,771733,probs,5) . array([&#39;020EE1K311&#39;, &#39;990EE1B303&#39;, &#39;030EO1B308&#39;, &#39;990EE1B303&#39;, &#39;040EE1K418&#39;], dtype=object) . !pip install wandb -q . shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected The folder you are executing pip from can no longer be found. . userData.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 67872 entries, 0 to 786187 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 gender 67872 non-null float64 1 age 67872 non-null float64 2 MEMBERID 67872 non-null float64 dtypes: float64(3) memory usage: 2.1 MB . combinedData=collabFiltering.set_index(&#39;MEMBERID&#39;).join(userData.drop(columns=[&#39;Buys&#39;, &#39;UniqueBuys&#39;],axis=1).set_index(&#39;MEMBERID&#39;)).reset_index().set_index(&#39;ProductID&#39;).join(productData.drop(columns=[&#39;Buys&#39;, &#39;UniqueBuys&#39;],axis=1).set_index(&#39;ProductID&#39;)).reset_index() . xDeepFM . Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNNbased factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly . !pip install deepctr . Collecting deepctr Downloading https://files.pythonhosted.org/packages/b2/c5/c010195a2f1a7da5f373334584b44fd9fa8e7ca3ae7084c5ac98e3a5fb84/deepctr-0.8.4-py3-none-any.whl (113kB) |████████████████████████████████| 122kB 5.3MB/s Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from deepctr) (2.23.0) Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deepctr) (2.10.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;deepctr) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;deepctr) (2020.12.5) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;deepctr) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;deepctr) (1.24.3) Requirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0-&gt;deepctr) (1.19.5) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0-&gt;deepctr) (1.15.0) Installing collected packages: deepctr Successfully installed deepctr-0.8.4 . !pip install pytorch-tabnet . Collecting pytorch-tabnet Downloading https://files.pythonhosted.org/packages/94/e5/2a808d611a5d44e3c997c0d07362c04a56c70002208e00aec9eee3d923b5/pytorch_tabnet-3.1.1-py3-none-any.whl Requirement already satisfied: scipy&gt;1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1) Requirement already satisfied: torch&lt;2.0,&gt;=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.7.1+cu101) Requirement already satisfied: numpy&lt;2.0,&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.19.5) Requirement already satisfied: tqdm&lt;5.0,&gt;=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.41.1) Requirement already satisfied: scikit_learn&gt;0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (0.22.2.post1) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&lt;2.0,&gt;=1.2-&gt;pytorch-tabnet) (3.7.4.3) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn&gt;0.21-&gt;pytorch-tabnet) (1.0.1) Installing collected packages: pytorch-tabnet Successfully installed pytorch-tabnet-3.1.1 . collabFiltering.nunique() . MEMBERID 67872 ProductID 3302 Count 14 dtype: int64 . combinedData.nunique() . ProductID 3302 MEMBERID 67872 Count 14 gender 3 age 74 Colour 15 matherial 42 product_group 6 target_group 6 features 90 Category 17 Cluster 33 dtype: int64 . 178765,771733 . (178765, 771733) . user178765=userData[userData[&#39;MEMBERID&#39;]==178765].drop(columns=[&#39;Buys&#39;, &#39;UniqueBuys&#39;],axis=1).assign(foo=1).merge(productData.drop(columns=[&#39;Buys&#39;, &#39;UniqueBuys&#39;],axis=1).assign(foo=1)).drop(&#39;foo&#39;, 1) . preds178765=user178765.copy() . user771733=userData[userData[&#39;MEMBERID&#39;]==771733].drop(columns=[&#39;Buys&#39;, &#39;UniqueBuys&#39;],axis=1).assign(foo=1).merge(productData.drop(columns=[&#39;Buys&#39;, &#39;UniqueBuys&#39;],axis=1).assign(foo=1)).drop(&#39;foo&#39;, 1) . preds771733=user771733.copy() . combinedData.head() . ProductID MEMBERID Count gender age Colour matherial product_group target_group features Category Cluster . 0 010CA1O304 | 147522.0 | 3 | 2.0 | 43.0 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | . 1 010CA1O304 | 3755899.0 | 3 | 2.0 | 58.0 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | . 2 010CA1O304 | 6273937.0 | 3 | 2.0 | 40.0 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | . 3 010CA1O304 | 6326114.0 | 3 | 2.0 | 52.0 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | . 4 010CA1Q302 | 828374.0 | 2 | 2.0 | 33.0 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | . import pandas as pd from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder,MinMaxScaler from deepctr.models import xDeepFM from deepctr.feature_column import SparseFeat,get_feature_names,DenseFeat . lbegender = LabelEncoder() mnmxAge=MinMaxScaler(feature_range=(0, 1)) lbeMEMBERID = LabelEncoder() lbeProductID = LabelEncoder() lbeColour = LabelEncoder() lbematherial = LabelEncoder() lbeproduct_group = LabelEncoder() lbetarget_group = LabelEncoder() lbefeatures = LabelEncoder() lbeCategory = LabelEncoder() lbeCluster = LabelEncoder() . categorical_dims = {} . def feature_encoding(lbegender,mnmxAge,lbeMEMBERID,lbeProductID,lbeColour,lbematherial,lbeproduct_group,lbetarget_group,lbefeatures,lbeCategory,lbeCluster,fit,data,categorical_dims): if(fit==1): data[&#39;gender&#39;]=data[&#39;gender&#39;].astype(str) lbegender.fit(data[[&#39;gender&#39;]].values) data[&#39;gender&#39;] = lbegender.transform(data[[&#39;gender&#39;]].values) categorical_dims[&#39;gender&#39;] = len(lbegender.classes_) mnmxAge.fit(data[[&#39;age&#39;]].values) data[&#39;age&#39;] = mnmxAge.transform(data[[&#39;age&#39;]].values) data[&#39;MEMBERID&#39;]=data[&#39;MEMBERID&#39;].astype(str) lbeMEMBERID.fit(data[[&#39;MEMBERID&#39;]].values) data[&#39;MEMBERID&#39;] = lbeMEMBERID.transform(data[[&#39;MEMBERID&#39;]].values) categorical_dims[&#39;MEMBERID&#39;] = len(lbeMEMBERID.classes_) data[&#39;ProductID&#39;]=data[&#39;ProductID&#39;].astype(str) lbeProductID.fit(data[[&#39;ProductID&#39;]].values) data[&#39;ProductID&#39;] = lbeProductID.transform(data[[&#39;ProductID&#39;]].values) categorical_dims[&#39;ProductID&#39;] = len(lbeProductID.classes_) data[&#39;Colour&#39;]=data[&#39;Colour&#39;].astype(str) lbeColour.fit(data[[&#39;Colour&#39;]].values) data[&#39;Colour&#39;] = lbeColour.transform(data[[&#39;Colour&#39;]].values) categorical_dims[&#39;Colour&#39;] = len(lbeColour.classes_) data[&#39;matherial&#39;]=data[&#39;matherial&#39;].astype(str) lbematherial.fit(data[[&#39;matherial&#39;]].values) data[&#39;matherial&#39;] = lbematherial.transform(data[[&#39;matherial&#39;]].values) categorical_dims[&#39;matherial&#39;] = len(lbematherial.classes_) data[&#39;product_group&#39;]=data[&#39;product_group&#39;].astype(str) lbeproduct_group.fit(data[[&#39;product_group&#39;]].values) data[&#39;product_group&#39;] = lbeproduct_group.transform(data[[&#39;product_group&#39;]].values) categorical_dims[&#39;product_group&#39;] = len(lbeproduct_group.classes_) data[&#39;target_group&#39;]=data[&#39;target_group&#39;].astype(str) lbetarget_group.fit(data[[&#39;target_group&#39;]].values) data[&#39;target_group&#39;] = lbetarget_group.transform(data[[&#39;target_group&#39;]].values) categorical_dims[&#39;target_group&#39;] = len(lbetarget_group.classes_) data[&#39;features&#39;]=data[&#39;features&#39;].astype(str) lbefeatures.fit(data[[&#39;features&#39;]].values) data[&#39;features&#39;] = lbefeatures.transform(data[[&#39;features&#39;]].values) categorical_dims[&#39;features&#39;] = len(lbefeatures.classes_) data[&#39;Category&#39;]=data[&#39;Category&#39;].astype(str) lbeCategory.fit(data[[&#39;Category&#39;]].values) data[&#39;Category&#39;] = lbeCategory.transform(data[[&#39;Category&#39;]].values) categorical_dims[&#39;Category&#39;] = len(lbeCategory.classes_) data[&#39;Cluster&#39;]=data[&#39;Cluster&#39;].astype(str) lbeCluster.fit(data[[&#39;Cluster&#39;]].values) data[&#39;Cluster&#39;] = lbeCluster.transform(data[[&#39;Cluster&#39;]].values) categorical_dims[&#39;Cluster&#39;] = len(lbeCluster.classes_) else: data[&#39;gender&#39;]=data[&#39;gender&#39;].astype(str) data[&#39;gender&#39;] = lbegender.transform(data[[&#39;gender&#39;]].values) data[&#39;age&#39;]=data[&#39;age&#39;].astype(int) data[&#39;age&#39;] = mnmxAge.transform(data[[&#39;age&#39;]].values) data[&#39;MEMBERID&#39;]=data[&#39;MEMBERID&#39;].astype(str) data[&#39;MEMBERID&#39;] = lbeMEMBERID.transform(data[[&#39;MEMBERID&#39;]].values) data[&#39;ProductID&#39;]=data[&#39;ProductID&#39;].astype(str) data[&#39;ProductID&#39;] = lbeProductID.transform(data[[&#39;ProductID&#39;]].values) data[&#39;Colour&#39;]=data[&#39;Colour&#39;].astype(str) data[&#39;Colour&#39;] = lbeColour.transform(data[[&#39;Colour&#39;]].values) data[&#39;matherial&#39;]=data[&#39;matherial&#39;].astype(str) data[&#39;matherial&#39;] = lbematherial.transform(data[[&#39;matherial&#39;]].values) data[&#39;product_group&#39;]=data[&#39;product_group&#39;].astype(str) data[&#39;product_group&#39;] = lbeproduct_group.transform(data[[&#39;product_group&#39;]].values) data[&#39;target_group&#39;]=data[&#39;target_group&#39;].astype(str) data[&#39;target_group&#39;] = lbetarget_group.transform(data[[&#39;target_group&#39;]].values) data[&#39;features&#39;]=data[&#39;features&#39;].astype(str) data[&#39;features&#39;] = lbefeatures.transform(data[[&#39;features&#39;]].values) data[&#39;Category&#39;]=data[&#39;Category&#39;].astype(str) data[&#39;Category&#39;] = lbeCategory.transform(data[[&#39;Category&#39;]].values) data[&#39;Cluster&#39;]=data[&#39;Cluster&#39;].astype(str) data[&#39;Cluster&#39;] = lbeCluster.transform(data[[&#39;Cluster&#39;]].values) . data=combinedData.copy() . feature_encoding(lbegender,mnmxAge,lbeMEMBERID,lbeProductID,lbeColour,lbematherial,lbeproduct_group,lbetarget_group,lbefeatures,lbeCategory,lbeCluster,1,data,categorical_dims) . /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) . from keras.optimizers import Adam . sparse_features = [&quot;MEMBERID&quot;, &quot;ProductID&quot;,&quot;gender&quot;, &quot;Colour&quot;, &quot;matherial&quot;,&#39;product_group&#39;,&#39;target_group&#39;,&#39;features&#39;,&#39;Category&#39;,&#39;Cluster&#39;] dense_features = [&#39;age&#39;] target = [&#39;Count&#39;] fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique(),embedding_dim=16) for feat in sparse_features]+[DenseFeat(feat, 1,) for feat in dense_features] linear_feature_columns = fixlen_feature_columns dnn_feature_columns = fixlen_feature_columns feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns) train, test = train_test_split(data, test_size=0.05, random_state=28) train_model_input = {name:train[name].values for name in feature_names} test_model_input = {name:test[name].values for name in feature_names} model = xDeepFM(linear_feature_columns, dnn_feature_columns, task=&#39;regression&#39;) model.compile(Adam(learning_rate=1e-4), &quot;mse&quot;, metrics=[&#39;mse&#39;]) history = model.fit(train_model_input, train[target].values,batch_size=256, epochs=2, verbose=1, validation_split=0.1) pred_ans = model.predict(test_model_input, batch_size=256) print(&quot;test MSE&quot;, round(mean_squared_error(test[target].values, pred_ans), 4)) . Epoch 1/2 710/710 [==============================] - 36s 38ms/step - loss: 6.6033 - mse: 6.6033 - val_loss: 1.5627 - val_mse: 1.5626 Epoch 2/2 710/710 [==============================] - 26s 36ms/step - loss: 1.4131 - mse: 1.4130 - val_loss: 1.5554 - val_mse: 1.5552 test MSE 1.6104 . test_model_input . {&#39;Category&#39;: array([ 3, 3, 15, ..., 7, 3, 3]), &#39;Cluster&#39;: array([28, 28, 30, ..., 8, 28, 28]), &#39;Colour&#39;: array([ 0, 6, 6, ..., 5, 8, 13]), &#39;MEMBERID&#39;: array([55865, 48168, 66464, ..., 1085, 29943, 4402]), &#39;ProductID&#39;: array([ 398, 1642, 1374, ..., 833, 395, 1810]), &#39;age&#39;: array([0.6 , 0.46666667, 0.71111111, ..., 0.55555556, 0.4 , 0.37777778]), &#39;features&#39;: array([66, 1, 71, ..., 42, 66, 66]), &#39;gender&#39;: array([2, 2, 2, ..., 2, 2, 2]), &#39;matherial&#39;: array([ 4, 4, 23, ..., 14, 4, 4]), &#39;product_group&#39;: array([5, 5, 1, ..., 5, 5, 5]), &#39;target_group&#39;: array([5, 5, 5, ..., 5, 5, 5])} . feature_encoding(lbegender,mnmxAge,lbeMEMBERID,lbeProductID,lbeColour,lbematherial,lbeproduct_group,lbetarget_group,lbefeatures,lbeCategory,lbeCluster,0,user178765,categorical_dims) feature_encoding(lbegender,mnmxAge,lbeMEMBERID,lbeProductID,lbeColour,lbematherial,lbeproduct_group,lbetarget_group,lbefeatures,lbeCategory,lbeCluster,0,user771733,categorical_dims) . /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) . test_model_input178765 = {name:user178765[name].values for name in feature_names} results178765=model.predict(test_model_input178765) test_model_input771733 = {name:user771733[name].values for name in feature_names} results771733=model.predict(test_model_input771733) . preds178765[&#39;PredsxDeepFM&#39;]=results178765 . preds771733[&#39;PredsxDeepFM&#39;]=results771733 . preds178765.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM . 0 2.0 | 51.0 | 178765.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.237521 | . 1 2.0 | 51.0 | 178765.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.225422 | . 2 2.0 | 51.0 | 178765.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.104673 | . 3 2.0 | 51.0 | 178765.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.536121 | . 4 2.0 | 51.0 | 178765.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.562388 | . preds771733.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM . 0 2.0 | 42.0 | 771733.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.737335 | . 1 2.0 | 42.0 | 771733.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.690987 | . 2 2.0 | 42.0 | 771733.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.557454 | . 3 2.0 | 42.0 | 771733.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.694690 | . 4 2.0 | 42.0 | 771733.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.722045 | . TABNET . TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. . We pretrain the model then using it we train our regressor. . X_train = train.drop(&#39;Count&#39;,axis=1) y_train = train[&#39;Count&#39;].values.reshape(-1, 1) X_valid = test.drop(&#39;Count&#39;,axis=1) y_valid = test[&#39;Count&#39;].values.reshape(-1, 1) . features = [ col for col in X_train.columns ] cat_idxs = [ i for i, f in enumerate(features) if f in sparse_features] cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in sparse_features] . import torch . from pytorch_tabnet.pretraining import TabNetPretrainer from pytorch_tabnet.tab_model import TabNetRegressor . cat_dims . [3302, 67872, 3, 16, 43, 7, 7, 91, 18, 34] . cat_emb_dim = [32, 64,2, 8, 8, 2,2,8, 8, 8] . unsupervised_model = TabNetPretrainer( cat_idxs=cat_idxs, cat_dims=cat_dims, cat_emb_dim=cat_emb_dim, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-3), mask_type=&#39;entmax&#39; # &quot;sparsemax&quot; ) . Device used : cuda . unsupervised_model.fit( X_train=X_train.values, eval_set=[X_valid.values], max_epochs=10 , patience=2, batch_size=256, virtual_batch_size=128, num_workers=4, drop_last=False, pretraining_ratio=0.8 ) . epoch 0 | loss: 1.38828 | val_0_unsup_loss: 1.02841 | 0:00:51s epoch 1 | loss: 1.01323 | val_0_unsup_loss: 1.00654 | 0:01:42s epoch 2 | loss: 1.00342 | val_0_unsup_loss: 1.00144 | 0:02:34s epoch 3 | loss: 1.00083 | val_0_unsup_loss: 0.9965 | 0:03:26s epoch 4 | loss: 0.99586 | val_0_unsup_loss: 0.96427 | 0:04:18s epoch 5 | loss: 0.95759 | val_0_unsup_loss: 0.91266 | 0:05:10s epoch 6 | loss: 0.8998 | val_0_unsup_loss: 0.86415 | 0:06:01s epoch 7 | loss: 0.85387 | val_0_unsup_loss: 0.82604 | 0:06:53s epoch 8 | loss: 0.82051 | val_0_unsup_loss: 0.7972 | 0:07:45s epoch 9 | loss: 0.80024 | val_0_unsup_loss: 0.7797 | 0:08:37s Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_0_unsup_loss = 0.7797 Best weights from best epoch are automatically used! . batch_size=512 max_epochs=20 reg = TabNetRegressor(cat_dims=cat_dims, cat_emb_dim=cat_emb_dim, cat_idxs=cat_idxs, optimizer_fn=torch.optim.Adam, # Any optimizer works here optimizer_params=dict(lr=1e-3), scheduler_fn=torch.optim.lr_scheduler.OneCycleLR, scheduler_params={&quot;is_batch_level&quot;:True, &quot;max_lr&quot;:1e-3, &quot;steps_per_epoch&quot;:int(train.shape[0] / batch_size)+1, &quot;epochs&quot;:max_epochs }, mask_type=&#39;entmax&#39;) . Device used : cuda . reg.fit( X_train=X_train.values, y_train=y_train, eval_set=[(X_train.values, y_train), (X_valid.values, y_valid)], eval_name=[&#39;train&#39;, &#39;valid&#39;], eval_metric=[&#39;rmsle&#39;, &#39;mae&#39;, &#39;rmse&#39;, &#39;mse&#39;], max_epochs=max_epochs, patience=2, batch_size=batch_size, virtual_batch_size=128, num_workers=4, drop_last=False ) . epoch 0 | loss: 18.91781| train_rmsle: 1.73712 | train_mae: 3.59658 | train_rmse: 4.0807 | train_mse: 16.65214| valid_rmsle: 1.75556 | valid_mae: 3.62313 | valid_rmse: 4.11499 | valid_mse: 16.93316| 0:00:38s epoch 1 | loss: 13.38149| train_rmsle: 0.76863 | train_mae: 2.55413 | train_rmse: 3.1432 | train_mse: 9.87968 | valid_rmsle: 0.78853 | valid_mae: 2.58328 | valid_rmse: 3.18618 | valid_mse: 10.15175| 0:01:16s epoch 2 | loss: 6.40709 | train_rmsle: 0.15214 | train_mae: 1.45646 | train_rmse: 2.01241 | train_mse: 4.04979 | valid_rmsle: 0.16117 | valid_mae: 1.50294 | valid_rmse: 2.07335 | valid_mse: 4.29876 | 0:01:54s epoch 3 | loss: 3.42005 | train_rmsle: 0.10997 | train_mae: 1.33221 | train_rmse: 1.73908 | train_mse: 3.02439 | valid_rmsle: 0.11643 | valid_mae: 1.37679 | valid_rmse: 1.78973 | valid_mse: 3.20313 | 0:02:33s epoch 4 | loss: 2.96976 | train_rmsle: 0.10122 | train_mae: 1.29924 | train_rmse: 1.66859 | train_mse: 2.78418 | valid_rmsle: 0.10706 | valid_mae: 1.34399 | valid_rmse: 1.7164 | valid_mse: 2.94602 | 0:03:11s epoch 5 | loss: 2.76463 | train_rmsle: 0.09288 | train_mae: 1.23827 | train_rmse: 1.61293 | train_mse: 2.60154 | valid_rmsle: 0.09778 | valid_mae: 1.27246 | valid_rmse: 1.65709 | valid_mse: 2.74594 | 0:03:49s epoch 6 | loss: 2.59255 | train_rmsle: 0.08666 | train_mae: 1.19129 | train_rmse: 1.55841 | train_mse: 2.42864 | valid_rmsle: 0.09154 | valid_mae: 1.22796 | valid_rmse: 1.60419 | valid_mse: 2.57343 | 0:04:28s epoch 7 | loss: 2.42255 | train_rmsle: 0.08022 | train_mae: 1.13112 | train_rmse: 1.49973 | train_mse: 2.2492 | valid_rmsle: 0.08488 | valid_mae: 1.16719 | valid_rmse: 1.54575 | valid_mse: 2.38936 | 0:05:07s epoch 8 | loss: 2.2491 | train_rmsle: 0.07397 | train_mae: 1.06325 | train_rmse: 1.44302 | train_mse: 2.08232 | valid_rmsle: 0.07898 | valid_mae: 1.10247 | valid_rmse: 1.4936 | valid_mse: 2.23084 | 0:05:45s epoch 9 | loss: 2.10232 | train_rmsle: 0.06991 | train_mae: 1.02234 | train_rmse: 1.3956 | train_mse: 1.94769 | valid_rmsle: 0.07519 | valid_mae: 1.06527 | valid_rmse: 1.45225 | valid_mse: 2.10904 | 0:06:25s epoch 10 | loss: 1.98005 | train_rmsle: 0.06516 | train_mae: 0.96813 | train_rmse: 1.354 | train_mse: 1.83332 | valid_rmsle: 0.07111 | valid_mae: 1.01548 | valid_rmse: 1.41675 | valid_mse: 2.00718 | 0:07:03s epoch 11 | loss: 1.88307 | train_rmsle: 0.06161 | train_mae: 0.92746 | train_rmse: 1.32272 | train_mse: 1.74959 | valid_rmsle: 0.06829 | valid_mae: 0.98188 | valid_rmse: 1.39611 | valid_mse: 1.94911 | 0:07:41s epoch 12 | loss: 1.81564 | train_rmsle: 0.05951 | train_mae: 0.90368 | train_rmse: 1.29977 | train_mse: 1.6894 | valid_rmsle: 0.06692 | valid_mae: 0.96348 | valid_rmse: 1.38232 | valid_mse: 1.91082 | 0:08:20s epoch 13 | loss: 1.7623 | train_rmsle: 0.05815 | train_mae: 0.8886 | train_rmse: 1.28229 | train_mse: 1.64426 | valid_rmsle: 0.06626 | valid_mae: 0.95412 | valid_rmse: 1.37399 | valid_mse: 1.88784 | 0:08:59s epoch 14 | loss: 1.7204 | train_rmsle: 0.05675 | train_mae: 0.8714 | train_rmse: 1.2689 | train_mse: 1.61012 | valid_rmsle: 0.06547 | valid_mae: 0.94226 | valid_rmse: 1.36823 | valid_mse: 1.87206 | 0:09:38s epoch 15 | loss: 1.69646 | train_rmsle: 0.05622 | train_mae: 0.86579 | train_rmse: 1.25996 | train_mse: 1.58751 | valid_rmsle: 0.06524 | valid_mae: 0.93952 | valid_rmse: 1.36312 | valid_mse: 1.85811 | 0:10:16s epoch 16 | loss: 1.67365 | train_rmsle: 0.05552 | train_mae: 0.85691 | train_rmse: 1.25431 | train_mse: 1.57331 | valid_rmsle: 0.0649 | valid_mae: 0.9339 | valid_rmse: 1.36164 | valid_mse: 1.85406 | 0:10:55s epoch 17 | loss: 1.65818 | train_rmsle: 0.05516 | train_mae: 0.85292 | train_rmse: 1.25156 | train_mse: 1.5664 | valid_rmsle: 0.06477 | valid_mae: 0.93165 | valid_rmse: 1.36125 | valid_mse: 1.853 | 0:11:34s epoch 18 | loss: 1.65606 | train_rmsle: 0.05519 | train_mae: 0.85284 | train_rmse: 1.24985 | train_mse: 1.56212 | valid_rmsle: 0.06487 | valid_mae: 0.9324 | valid_rmse: 1.36101 | valid_mse: 1.85236 | 0:12:12s epoch 19 | loss: 1.6564 | train_rmsle: 0.05506 | train_mae: 0.85151 | train_rmse: 1.25005 | train_mse: 1.56262 | valid_rmsle: 0.06473 | valid_mae: 0.93091 | valid_rmse: 1.36118 | valid_mse: 1.85281 | 0:12:51s Stop training because you reached max_epochs = 20 with best_epoch = 18 and best_valid_mse = 1.85236 Best weights from best epoch are automatically used! . imp=dict(zip(features, reg.feature_importances_)) . imp={k: v for k, v in sorted(imp.items(), key=lambda item: item[1])} . plt.barh(list(imp.keys()), list(imp.values()), align=&#39;center&#39;) . &lt;BarContainer object of 11 artists&gt; . userTAB178765=user178765[[&#39;ProductID&#39;,&#39;MEMBERID&#39;,&#39;gender&#39;,&#39;age&#39;,&#39;Colour&#39;,&#39;matherial&#39;,&#39;product_group&#39;,&#39;target_group&#39;,&#39;features&#39;,&#39;Category&#39;,&#39;Cluster&#39;]] userTAB178765 . ProductID MEMBERID gender age Colour matherial product_group target_group features Category Cluster . 0 0 | 8968 | 2 | 0.566667 | 8 | 4 | 0 | 5 | 7 | 0 | 3 | . 1 1 | 8968 | 2 | 0.566667 | 2 | 24 | 0 | 5 | 64 | 1 | 20 | . 2 2 | 8968 | 2 | 0.566667 | 2 | 2 | 0 | 5 | 14 | 0 | 5 | . 3 3 | 8968 | 2 | 0.566667 | 5 | 4 | 5 | 5 | 34 | 4 | 16 | . 4 4 | 8968 | 2 | 0.566667 | 8 | 4 | 5 | 5 | 34 | 4 | 16 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 3172 3297 | 8968 | 2 | 0.566667 | 9 | 24 | 5 | 5 | 56 | 8 | 11 | . 3173 3298 | 8968 | 2 | 0.566667 | 8 | 34 | 5 | 5 | 66 | 3 | 28 | . 3174 3299 | 8968 | 2 | 0.566667 | 2 | 4 | 5 | 5 | 38 | 3 | 28 | . 3175 3300 | 8968 | 2 | 0.566667 | 5 | 24 | 5 | 5 | 56 | 3 | 15 | . 3176 3301 | 8968 | 2 | 0.566667 | 2 | 4 | 2 | 4 | 38 | 7 | 21 | . 3177 rows × 11 columns . import matplotlib.pyplot as plt . explain_matrix, masks = reg.explain(userTAB178765.values) fig, axs = plt.subplots(1, 3, figsize=(10,10)) for i in range(3): axs[i].imshow(masks[i][:10]) axs[i].set_title(f&quot;mask {i}&quot;) . preds178765[&#39;TABNET&#39;]=reg.predict(userTAB178765.values) . preds178765.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET . 0 2.0 | 51.0 | 178765.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.253060 | 3.220370 | . 1 2.0 | 51.0 | 178765.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.323148 | 3.112500 | . 2 2.0 | 51.0 | 178765.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.206944 | 2.699850 | . 3 2.0 | 51.0 | 178765.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.583255 | 2.939901 | . 4 2.0 | 51.0 | 178765.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.607694 | 2.143070 | . userTAB771733=user771733[[&#39;ProductID&#39;,&#39;MEMBERID&#39;,&#39;gender&#39;,&#39;age&#39;,&#39;Colour&#39;,&#39;matherial&#39;,&#39;product_group&#39;,&#39;target_group&#39;,&#39;features&#39;,&#39;Category&#39;,&#39;Cluster&#39;]] . explain_matrix, masks = reg.explain(userTAB771733.values) fig, axs = plt.subplots(1, 3, figsize=(10,10)) for i in range(3): axs[i].imshow(masks[i][:10]) axs[i].set_title(f&quot;mask {i}&quot;) . preds771733[&#39;TABNET&#39;]=reg.predict(userTAB771733.values) . preds771733.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET . 0 2.0 | 42.0 | 771733.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.737335 | 4.995235 | . 1 2.0 | 42.0 | 771733.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.690987 | 3.850037 | . 2 2.0 | 42.0 | 771733.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.557454 | 3.507605 | . 3 2.0 | 42.0 | 771733.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.694690 | 2.068795 | . 4 2.0 | 42.0 | 771733.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.722045 | 2.161431 | . XGBOOST . XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. We use xgboost on GPU here. . import xgboost as xgb . xg_train = xgb.DMatrix(X_train.values,y_train) xg_valid = xgb.DMatrix(X_valid.values,y_valid) . params = {&#39;tree_method&#39;: &#39;gpu_hist&#39;, &#39;max_depth&#39;: 10, &#39;alpha&#39;: 0.1, &#39;subsample&#39;: 0.6, &#39;scale_pos_weight&#39;: 1, &#39;learning_rate&#39;: 0.01, &#39;silent&#39;: 1, &#39;objective&#39;:&#39;reg:linear&#39;, &#39;eval_metric&#39;: &#39;rmse&#39;,&#39;n_gpus&#39;: 1} . regXGB = xgb.train(params, xg_train, 1000, evals=[(xg_train, &quot;train&quot;), (xg_valid, &quot;eval&quot;)],early_stopping_rounds=5, verbose_eval=True) . [0] train-rmse:3.68321 eval-rmse:3.70524 Multiple eval metrics have been passed: &#39;eval-rmse&#39; will be used for early stopping. Will train until eval-rmse hasn&#39;t improved in 5 rounds. [1] train-rmse:3.65164 eval-rmse:3.67372 [2] train-rmse:3.62054 eval-rmse:3.64268 [3] train-rmse:3.5898 eval-rmse:3.61194 [4] train-rmse:3.5593 eval-rmse:3.58151 [5] train-rmse:3.5292 eval-rmse:3.55144 [6] train-rmse:3.49945 eval-rmse:3.5218 [7] train-rmse:3.47003 eval-rmse:3.49245 [8] train-rmse:3.44088 eval-rmse:3.46335 [9] train-rmse:3.41215 eval-rmse:3.43473 [10] train-rmse:3.38379 eval-rmse:3.40643 [11] train-rmse:3.35568 eval-rmse:3.37841 [12] train-rmse:3.32792 eval-rmse:3.35072 [13] train-rmse:3.30051 eval-rmse:3.32336 [14] train-rmse:3.27342 eval-rmse:3.29633 [15] train-rmse:3.24658 eval-rmse:3.26951 [16] train-rmse:3.22009 eval-rmse:3.2431 [17] train-rmse:3.1938 eval-rmse:3.2169 [18] train-rmse:3.16796 eval-rmse:3.19112 [19] train-rmse:3.14241 eval-rmse:3.16558 [20] train-rmse:3.1171 eval-rmse:3.14034 [21] train-rmse:3.09209 eval-rmse:3.11538 [22] train-rmse:3.06739 eval-rmse:3.09079 [23] train-rmse:3.04291 eval-rmse:3.06639 [24] train-rmse:3.01878 eval-rmse:3.04232 [25] train-rmse:2.99501 eval-rmse:3.01862 [26] train-rmse:2.97145 eval-rmse:2.99514 [27] train-rmse:2.94813 eval-rmse:2.97186 [28] train-rmse:2.92514 eval-rmse:2.94896 [29] train-rmse:2.90235 eval-rmse:2.92619 [30] train-rmse:2.87994 eval-rmse:2.90385 [31] train-rmse:2.85769 eval-rmse:2.8817 [32] train-rmse:2.83574 eval-rmse:2.85977 [33] train-rmse:2.81406 eval-rmse:2.83821 [34] train-rmse:2.79271 eval-rmse:2.81692 [35] train-rmse:2.77162 eval-rmse:2.79592 [36] train-rmse:2.75066 eval-rmse:2.77505 [37] train-rmse:2.72997 eval-rmse:2.75444 [38] train-rmse:2.70965 eval-rmse:2.73414 [39] train-rmse:2.68947 eval-rmse:2.71408 [40] train-rmse:2.66947 eval-rmse:2.69418 [41] train-rmse:2.64986 eval-rmse:2.67461 [42] train-rmse:2.6304 eval-rmse:2.65517 [43] train-rmse:2.61112 eval-rmse:2.63597 [44] train-rmse:2.59219 eval-rmse:2.61711 [45] train-rmse:2.57347 eval-rmse:2.59843 [46] train-rmse:2.5549 eval-rmse:2.58005 [47] train-rmse:2.53655 eval-rmse:2.56178 [48] train-rmse:2.51846 eval-rmse:2.54374 [49] train-rmse:2.50056 eval-rmse:2.52595 [50] train-rmse:2.48277 eval-rmse:2.50821 [51] train-rmse:2.46537 eval-rmse:2.49088 [52] train-rmse:2.44829 eval-rmse:2.47388 [53] train-rmse:2.43131 eval-rmse:2.45701 [54] train-rmse:2.41452 eval-rmse:2.44029 [55] train-rmse:2.39795 eval-rmse:2.42381 [56] train-rmse:2.38171 eval-rmse:2.40763 [57] train-rmse:2.36551 eval-rmse:2.39147 [58] train-rmse:2.34953 eval-rmse:2.37558 [59] train-rmse:2.33373 eval-rmse:2.35982 [60] train-rmse:2.31816 eval-rmse:2.34436 [61] train-rmse:2.30285 eval-rmse:2.32916 [62] train-rmse:2.28779 eval-rmse:2.31418 [63] train-rmse:2.27284 eval-rmse:2.29933 [64] train-rmse:2.25816 eval-rmse:2.28468 [65] train-rmse:2.24366 eval-rmse:2.27025 [66] train-rmse:2.22926 eval-rmse:2.25597 [67] train-rmse:2.21507 eval-rmse:2.24187 [68] train-rmse:2.20109 eval-rmse:2.22796 [69] train-rmse:2.18732 eval-rmse:2.21426 [70] train-rmse:2.17366 eval-rmse:2.20064 [71] train-rmse:2.16024 eval-rmse:2.18728 [72] train-rmse:2.147 eval-rmse:2.17415 [73] train-rmse:2.13381 eval-rmse:2.16104 [74] train-rmse:2.12086 eval-rmse:2.14813 [75] train-rmse:2.10814 eval-rmse:2.13555 [76] train-rmse:2.09556 eval-rmse:2.12303 [77] train-rmse:2.08313 eval-rmse:2.11067 [78] train-rmse:2.0709 eval-rmse:2.09848 [79] train-rmse:2.05881 eval-rmse:2.08643 [80] train-rmse:2.04697 eval-rmse:2.07469 [81] train-rmse:2.03518 eval-rmse:2.06301 [82] train-rmse:2.02358 eval-rmse:2.0515 [83] train-rmse:2.01217 eval-rmse:2.04017 [84] train-rmse:2.00083 eval-rmse:2.02891 [85] train-rmse:1.98963 eval-rmse:2.01776 [86] train-rmse:1.97861 eval-rmse:2.00675 [87] train-rmse:1.96781 eval-rmse:1.99602 [88] train-rmse:1.95706 eval-rmse:1.98532 [89] train-rmse:1.94641 eval-rmse:1.97473 [90] train-rmse:1.93594 eval-rmse:1.96433 [91] train-rmse:1.92567 eval-rmse:1.9541 [92] train-rmse:1.9155 eval-rmse:1.94402 [93] train-rmse:1.90552 eval-rmse:1.93416 [94] train-rmse:1.89582 eval-rmse:1.9246 [95] train-rmse:1.88613 eval-rmse:1.91494 [96] train-rmse:1.87654 eval-rmse:1.90538 [97] train-rmse:1.86714 eval-rmse:1.89603 [98] train-rmse:1.85781 eval-rmse:1.88681 [99] train-rmse:1.84869 eval-rmse:1.87773 [100] train-rmse:1.83954 eval-rmse:1.86864 [101] train-rmse:1.83067 eval-rmse:1.8599 [102] train-rmse:1.82188 eval-rmse:1.85122 [103] train-rmse:1.81325 eval-rmse:1.84262 [104] train-rmse:1.80466 eval-rmse:1.8341 [105] train-rmse:1.79637 eval-rmse:1.8258 [106] train-rmse:1.78806 eval-rmse:1.81754 [107] train-rmse:1.77983 eval-rmse:1.80932 [108] train-rmse:1.7718 eval-rmse:1.80131 [109] train-rmse:1.7638 eval-rmse:1.79342 [110] train-rmse:1.75588 eval-rmse:1.78552 [111] train-rmse:1.74818 eval-rmse:1.77784 [112] train-rmse:1.74051 eval-rmse:1.77019 [113] train-rmse:1.73304 eval-rmse:1.76279 [114] train-rmse:1.7256 eval-rmse:1.75536 [115] train-rmse:1.71835 eval-rmse:1.74817 [116] train-rmse:1.71128 eval-rmse:1.74112 [117] train-rmse:1.70413 eval-rmse:1.73398 [118] train-rmse:1.69715 eval-rmse:1.72702 [119] train-rmse:1.69025 eval-rmse:1.72014 [120] train-rmse:1.68345 eval-rmse:1.71335 [121] train-rmse:1.67689 eval-rmse:1.70689 [122] train-rmse:1.67035 eval-rmse:1.70044 [123] train-rmse:1.66396 eval-rmse:1.69416 [124] train-rmse:1.65766 eval-rmse:1.688 [125] train-rmse:1.65143 eval-rmse:1.68183 [126] train-rmse:1.64522 eval-rmse:1.67576 [127] train-rmse:1.63916 eval-rmse:1.66974 [128] train-rmse:1.63313 eval-rmse:1.66379 [129] train-rmse:1.62717 eval-rmse:1.65788 [130] train-rmse:1.62133 eval-rmse:1.65216 [131] train-rmse:1.6156 eval-rmse:1.64649 [132] train-rmse:1.60994 eval-rmse:1.64093 [133] train-rmse:1.60451 eval-rmse:1.6356 [134] train-rmse:1.59905 eval-rmse:1.63018 [135] train-rmse:1.59365 eval-rmse:1.62479 [136] train-rmse:1.58837 eval-rmse:1.61957 [137] train-rmse:1.58314 eval-rmse:1.61437 [138] train-rmse:1.57802 eval-rmse:1.60932 [139] train-rmse:1.57297 eval-rmse:1.60439 [140] train-rmse:1.56798 eval-rmse:1.59946 [141] train-rmse:1.56312 eval-rmse:1.59465 [142] train-rmse:1.55837 eval-rmse:1.59002 [143] train-rmse:1.55362 eval-rmse:1.58532 [144] train-rmse:1.54893 eval-rmse:1.58069 [145] train-rmse:1.54427 eval-rmse:1.57612 [146] train-rmse:1.53975 eval-rmse:1.57168 [147] train-rmse:1.53537 eval-rmse:1.56744 [148] train-rmse:1.53091 eval-rmse:1.5631 [149] train-rmse:1.52663 eval-rmse:1.55887 [150] train-rmse:1.52249 eval-rmse:1.55481 [151] train-rmse:1.51836 eval-rmse:1.55076 [152] train-rmse:1.51424 eval-rmse:1.54677 [153] train-rmse:1.51016 eval-rmse:1.54279 [154] train-rmse:1.50617 eval-rmse:1.53885 [155] train-rmse:1.50222 eval-rmse:1.53497 [156] train-rmse:1.49822 eval-rmse:1.53101 [157] train-rmse:1.49439 eval-rmse:1.52726 [158] train-rmse:1.49062 eval-rmse:1.52353 [159] train-rmse:1.48695 eval-rmse:1.51988 [160] train-rmse:1.48333 eval-rmse:1.51631 [161] train-rmse:1.47972 eval-rmse:1.51279 [162] train-rmse:1.47631 eval-rmse:1.50945 [163] train-rmse:1.47286 eval-rmse:1.50604 [164] train-rmse:1.46945 eval-rmse:1.50269 [165] train-rmse:1.46613 eval-rmse:1.49946 [166] train-rmse:1.46283 eval-rmse:1.49625 [167] train-rmse:1.45967 eval-rmse:1.49322 [168] train-rmse:1.45659 eval-rmse:1.49023 [169] train-rmse:1.45335 eval-rmse:1.48707 [170] train-rmse:1.45029 eval-rmse:1.48411 [171] train-rmse:1.44732 eval-rmse:1.48121 [172] train-rmse:1.44436 eval-rmse:1.47835 [173] train-rmse:1.44141 eval-rmse:1.47551 [174] train-rmse:1.4386 eval-rmse:1.47279 [175] train-rmse:1.43573 eval-rmse:1.47004 [176] train-rmse:1.43295 eval-rmse:1.46741 [177] train-rmse:1.43015 eval-rmse:1.46467 [178] train-rmse:1.42749 eval-rmse:1.46214 [179] train-rmse:1.42475 eval-rmse:1.45946 [180] train-rmse:1.42207 eval-rmse:1.45685 [181] train-rmse:1.41949 eval-rmse:1.45434 [182] train-rmse:1.41693 eval-rmse:1.45187 [183] train-rmse:1.41453 eval-rmse:1.44961 [184] train-rmse:1.41208 eval-rmse:1.44721 [185] train-rmse:1.40949 eval-rmse:1.44472 [186] train-rmse:1.40717 eval-rmse:1.44247 [187] train-rmse:1.40481 eval-rmse:1.44018 [188] train-rmse:1.40239 eval-rmse:1.43784 [189] train-rmse:1.40013 eval-rmse:1.43563 [190] train-rmse:1.39796 eval-rmse:1.43355 [191] train-rmse:1.39582 eval-rmse:1.43152 [192] train-rmse:1.39363 eval-rmse:1.42941 [193] train-rmse:1.3915 eval-rmse:1.42738 [194] train-rmse:1.38936 eval-rmse:1.42536 [195] train-rmse:1.38731 eval-rmse:1.42337 [196] train-rmse:1.38516 eval-rmse:1.42129 [197] train-rmse:1.38314 eval-rmse:1.41935 [198] train-rmse:1.38118 eval-rmse:1.41751 [199] train-rmse:1.37923 eval-rmse:1.41564 [200] train-rmse:1.37732 eval-rmse:1.41387 [201] train-rmse:1.37546 eval-rmse:1.41211 [202] train-rmse:1.37352 eval-rmse:1.41028 [203] train-rmse:1.37176 eval-rmse:1.40861 [204] train-rmse:1.37 eval-rmse:1.40692 [205] train-rmse:1.36827 eval-rmse:1.40524 [206] train-rmse:1.36647 eval-rmse:1.4035 [207] train-rmse:1.36478 eval-rmse:1.40193 [208] train-rmse:1.36318 eval-rmse:1.40043 [209] train-rmse:1.36149 eval-rmse:1.39885 [210] train-rmse:1.35988 eval-rmse:1.39732 [211] train-rmse:1.35831 eval-rmse:1.39586 [212] train-rmse:1.35675 eval-rmse:1.39441 [213] train-rmse:1.35516 eval-rmse:1.39295 [214] train-rmse:1.35362 eval-rmse:1.3915 [215] train-rmse:1.35207 eval-rmse:1.39007 [216] train-rmse:1.35062 eval-rmse:1.38868 [217] train-rmse:1.34906 eval-rmse:1.38725 [218] train-rmse:1.34756 eval-rmse:1.38584 [219] train-rmse:1.34603 eval-rmse:1.38439 [220] train-rmse:1.34449 eval-rmse:1.38298 [221] train-rmse:1.3431 eval-rmse:1.38165 [222] train-rmse:1.34169 eval-rmse:1.38037 [223] train-rmse:1.34035 eval-rmse:1.37908 [224] train-rmse:1.33909 eval-rmse:1.37791 [225] train-rmse:1.33775 eval-rmse:1.37669 [226] train-rmse:1.33629 eval-rmse:1.37532 [227] train-rmse:1.33509 eval-rmse:1.3742 [228] train-rmse:1.33394 eval-rmse:1.37314 [229] train-rmse:1.33263 eval-rmse:1.37192 [230] train-rmse:1.33142 eval-rmse:1.37078 [231] train-rmse:1.33015 eval-rmse:1.36959 [232] train-rmse:1.32898 eval-rmse:1.36848 [233] train-rmse:1.32787 eval-rmse:1.36745 [234] train-rmse:1.32666 eval-rmse:1.36633 [235] train-rmse:1.32562 eval-rmse:1.36536 [236] train-rmse:1.3245 eval-rmse:1.36435 [237] train-rmse:1.32342 eval-rmse:1.36336 [238] train-rmse:1.32229 eval-rmse:1.36231 [239] train-rmse:1.32124 eval-rmse:1.36132 [240] train-rmse:1.32016 eval-rmse:1.36036 [241] train-rmse:1.31915 eval-rmse:1.35944 [242] train-rmse:1.31811 eval-rmse:1.35848 [243] train-rmse:1.31716 eval-rmse:1.3576 [244] train-rmse:1.31623 eval-rmse:1.35675 [245] train-rmse:1.31526 eval-rmse:1.35588 [246] train-rmse:1.31434 eval-rmse:1.35502 [247] train-rmse:1.31338 eval-rmse:1.35416 [248] train-rmse:1.31244 eval-rmse:1.35331 [249] train-rmse:1.31144 eval-rmse:1.35244 [250] train-rmse:1.31055 eval-rmse:1.35162 [251] train-rmse:1.30974 eval-rmse:1.35089 [252] train-rmse:1.3088 eval-rmse:1.35 [253] train-rmse:1.30789 eval-rmse:1.34919 [254] train-rmse:1.30714 eval-rmse:1.34851 [255] train-rmse:1.30632 eval-rmse:1.3478 [256] train-rmse:1.3055 eval-rmse:1.34704 [257] train-rmse:1.30455 eval-rmse:1.34621 [258] train-rmse:1.30367 eval-rmse:1.34542 [259] train-rmse:1.30282 eval-rmse:1.34471 [260] train-rmse:1.302 eval-rmse:1.34398 [261] train-rmse:1.30123 eval-rmse:1.34328 [262] train-rmse:1.30038 eval-rmse:1.34254 [263] train-rmse:1.29949 eval-rmse:1.34177 [264] train-rmse:1.2986 eval-rmse:1.34095 [265] train-rmse:1.29771 eval-rmse:1.34016 [266] train-rmse:1.29689 eval-rmse:1.33945 [267] train-rmse:1.29615 eval-rmse:1.33884 [268] train-rmse:1.29544 eval-rmse:1.33822 [269] train-rmse:1.29472 eval-rmse:1.33755 [270] train-rmse:1.29399 eval-rmse:1.33691 [271] train-rmse:1.29328 eval-rmse:1.33624 [272] train-rmse:1.29259 eval-rmse:1.33565 [273] train-rmse:1.292 eval-rmse:1.33511 [274] train-rmse:1.29127 eval-rmse:1.33452 [275] train-rmse:1.29054 eval-rmse:1.33386 [276] train-rmse:1.28998 eval-rmse:1.33337 [277] train-rmse:1.28927 eval-rmse:1.33272 [278] train-rmse:1.2886 eval-rmse:1.33211 [279] train-rmse:1.28802 eval-rmse:1.33164 [280] train-rmse:1.28739 eval-rmse:1.33115 [281] train-rmse:1.28679 eval-rmse:1.3306 [282] train-rmse:1.28627 eval-rmse:1.33014 [283] train-rmse:1.28568 eval-rmse:1.32965 [284] train-rmse:1.28502 eval-rmse:1.32905 [285] train-rmse:1.28445 eval-rmse:1.3286 [286] train-rmse:1.28385 eval-rmse:1.32805 [287] train-rmse:1.2833 eval-rmse:1.32757 [288] train-rmse:1.28265 eval-rmse:1.32695 [289] train-rmse:1.28217 eval-rmse:1.32664 [290] train-rmse:1.28159 eval-rmse:1.32613 [291] train-rmse:1.28111 eval-rmse:1.32573 [292] train-rmse:1.28053 eval-rmse:1.32528 [293] train-rmse:1.28004 eval-rmse:1.32482 [294] train-rmse:1.27954 eval-rmse:1.32443 [295] train-rmse:1.27896 eval-rmse:1.32387 [296] train-rmse:1.2784 eval-rmse:1.32338 [297] train-rmse:1.27785 eval-rmse:1.32297 [298] train-rmse:1.27742 eval-rmse:1.32267 [299] train-rmse:1.27693 eval-rmse:1.32228 [300] train-rmse:1.27643 eval-rmse:1.32189 [301] train-rmse:1.27583 eval-rmse:1.32136 [302] train-rmse:1.27542 eval-rmse:1.32099 [303] train-rmse:1.27495 eval-rmse:1.32059 [304] train-rmse:1.27443 eval-rmse:1.32022 [305] train-rmse:1.27399 eval-rmse:1.31984 [306] train-rmse:1.27342 eval-rmse:1.31933 [307] train-rmse:1.273 eval-rmse:1.31899 [308] train-rmse:1.27259 eval-rmse:1.31867 [309] train-rmse:1.27213 eval-rmse:1.31831 [310] train-rmse:1.27169 eval-rmse:1.31798 [311] train-rmse:1.27117 eval-rmse:1.31754 [312] train-rmse:1.27067 eval-rmse:1.31713 [313] train-rmse:1.27018 eval-rmse:1.31672 [314] train-rmse:1.26981 eval-rmse:1.31638 [315] train-rmse:1.2694 eval-rmse:1.31608 [316] train-rmse:1.26899 eval-rmse:1.3158 [317] train-rmse:1.26861 eval-rmse:1.31556 [318] train-rmse:1.26822 eval-rmse:1.31523 [319] train-rmse:1.26778 eval-rmse:1.31493 [320] train-rmse:1.26728 eval-rmse:1.31453 [321] train-rmse:1.26692 eval-rmse:1.31425 [322] train-rmse:1.26648 eval-rmse:1.31393 [323] train-rmse:1.26605 eval-rmse:1.31362 [324] train-rmse:1.26568 eval-rmse:1.31334 [325] train-rmse:1.26533 eval-rmse:1.3131 [326] train-rmse:1.26492 eval-rmse:1.3128 [327] train-rmse:1.26442 eval-rmse:1.31237 [328] train-rmse:1.26407 eval-rmse:1.31206 [329] train-rmse:1.26374 eval-rmse:1.31182 [330] train-rmse:1.26326 eval-rmse:1.31143 [331] train-rmse:1.26292 eval-rmse:1.31121 [332] train-rmse:1.26256 eval-rmse:1.31093 [333] train-rmse:1.2622 eval-rmse:1.31065 [334] train-rmse:1.26185 eval-rmse:1.31041 [335] train-rmse:1.2615 eval-rmse:1.31016 [336] train-rmse:1.26099 eval-rmse:1.30972 [337] train-rmse:1.26066 eval-rmse:1.3095 [338] train-rmse:1.26025 eval-rmse:1.30914 [339] train-rmse:1.25999 eval-rmse:1.30895 [340] train-rmse:1.25967 eval-rmse:1.30872 [341] train-rmse:1.25934 eval-rmse:1.30845 [342] train-rmse:1.25909 eval-rmse:1.30834 [343] train-rmse:1.25873 eval-rmse:1.30805 [344] train-rmse:1.25838 eval-rmse:1.30776 [345] train-rmse:1.25803 eval-rmse:1.30749 [346] train-rmse:1.25775 eval-rmse:1.30724 [347] train-rmse:1.25748 eval-rmse:1.30706 [348] train-rmse:1.25716 eval-rmse:1.30688 [349] train-rmse:1.25687 eval-rmse:1.30669 [350] train-rmse:1.25658 eval-rmse:1.30646 [351] train-rmse:1.25632 eval-rmse:1.30624 [352] train-rmse:1.25602 eval-rmse:1.30603 [353] train-rmse:1.25572 eval-rmse:1.30578 [354] train-rmse:1.25541 eval-rmse:1.30553 [355] train-rmse:1.25513 eval-rmse:1.30535 [356] train-rmse:1.25486 eval-rmse:1.30517 [357] train-rmse:1.25459 eval-rmse:1.30502 [358] train-rmse:1.25436 eval-rmse:1.30486 [359] train-rmse:1.25401 eval-rmse:1.30459 [360] train-rmse:1.25372 eval-rmse:1.30442 [361] train-rmse:1.25342 eval-rmse:1.30418 [362] train-rmse:1.25319 eval-rmse:1.30401 [363] train-rmse:1.25286 eval-rmse:1.30379 [364] train-rmse:1.25258 eval-rmse:1.30356 [365] train-rmse:1.25233 eval-rmse:1.30342 [366] train-rmse:1.25213 eval-rmse:1.30327 [367] train-rmse:1.25182 eval-rmse:1.30307 [368] train-rmse:1.25147 eval-rmse:1.30278 [369] train-rmse:1.2512 eval-rmse:1.30257 [370] train-rmse:1.25087 eval-rmse:1.30235 [371] train-rmse:1.25061 eval-rmse:1.3022 [372] train-rmse:1.25026 eval-rmse:1.30192 [373] train-rmse:1.24998 eval-rmse:1.3017 [374] train-rmse:1.24979 eval-rmse:1.30161 [375] train-rmse:1.24956 eval-rmse:1.3015 [376] train-rmse:1.24923 eval-rmse:1.30136 [377] train-rmse:1.24897 eval-rmse:1.30117 [378] train-rmse:1.24864 eval-rmse:1.30102 [379] train-rmse:1.24843 eval-rmse:1.3009 [380] train-rmse:1.24822 eval-rmse:1.30074 [381] train-rmse:1.24799 eval-rmse:1.30066 [382] train-rmse:1.24774 eval-rmse:1.30047 [383] train-rmse:1.24746 eval-rmse:1.30033 [384] train-rmse:1.24718 eval-rmse:1.30013 [385] train-rmse:1.24691 eval-rmse:1.29996 [386] train-rmse:1.24669 eval-rmse:1.29988 [387] train-rmse:1.24642 eval-rmse:1.29974 [388] train-rmse:1.2462 eval-rmse:1.29954 [389] train-rmse:1.246 eval-rmse:1.29947 [390] train-rmse:1.24582 eval-rmse:1.29939 [391] train-rmse:1.24561 eval-rmse:1.29925 [392] train-rmse:1.24546 eval-rmse:1.2992 [393] train-rmse:1.24518 eval-rmse:1.29898 [394] train-rmse:1.24499 eval-rmse:1.29888 [395] train-rmse:1.24473 eval-rmse:1.29868 [396] train-rmse:1.24452 eval-rmse:1.29855 [397] train-rmse:1.2443 eval-rmse:1.29842 [398] train-rmse:1.24406 eval-rmse:1.29824 [399] train-rmse:1.24389 eval-rmse:1.29817 [400] train-rmse:1.24369 eval-rmse:1.29811 [401] train-rmse:1.24353 eval-rmse:1.29803 [402] train-rmse:1.24331 eval-rmse:1.29789 [403] train-rmse:1.24311 eval-rmse:1.29779 [404] train-rmse:1.2429 eval-rmse:1.29767 [405] train-rmse:1.24265 eval-rmse:1.29756 [406] train-rmse:1.24242 eval-rmse:1.29739 [407] train-rmse:1.24221 eval-rmse:1.29734 [408] train-rmse:1.24201 eval-rmse:1.29719 [409] train-rmse:1.24189 eval-rmse:1.29716 [410] train-rmse:1.2417 eval-rmse:1.29708 [411] train-rmse:1.24142 eval-rmse:1.29695 [412] train-rmse:1.24118 eval-rmse:1.29683 [413] train-rmse:1.24107 eval-rmse:1.29676 [414] train-rmse:1.24092 eval-rmse:1.29667 [415] train-rmse:1.24074 eval-rmse:1.29656 [416] train-rmse:1.24041 eval-rmse:1.29632 [417] train-rmse:1.24014 eval-rmse:1.29614 [418] train-rmse:1.23993 eval-rmse:1.29605 [419] train-rmse:1.23972 eval-rmse:1.29593 [420] train-rmse:1.23955 eval-rmse:1.29579 [421] train-rmse:1.23941 eval-rmse:1.29574 [422] train-rmse:1.2393 eval-rmse:1.29565 [423] train-rmse:1.23903 eval-rmse:1.29554 [424] train-rmse:1.23886 eval-rmse:1.2955 [425] train-rmse:1.23862 eval-rmse:1.29541 [426] train-rmse:1.23835 eval-rmse:1.29525 [427] train-rmse:1.23823 eval-rmse:1.29516 [428] train-rmse:1.23798 eval-rmse:1.29506 [429] train-rmse:1.23788 eval-rmse:1.29498 [430] train-rmse:1.23773 eval-rmse:1.29486 [431] train-rmse:1.23759 eval-rmse:1.29481 [432] train-rmse:1.23747 eval-rmse:1.29476 [433] train-rmse:1.2373 eval-rmse:1.29469 [434] train-rmse:1.2372 eval-rmse:1.29465 [435] train-rmse:1.23693 eval-rmse:1.2945 [436] train-rmse:1.23676 eval-rmse:1.29439 [437] train-rmse:1.23659 eval-rmse:1.29434 [438] train-rmse:1.23643 eval-rmse:1.29427 [439] train-rmse:1.23619 eval-rmse:1.29416 [440] train-rmse:1.23599 eval-rmse:1.29398 [441] train-rmse:1.23575 eval-rmse:1.29385 [442] train-rmse:1.2356 eval-rmse:1.2938 [443] train-rmse:1.23549 eval-rmse:1.29377 [444] train-rmse:1.23535 eval-rmse:1.29375 [445] train-rmse:1.2352 eval-rmse:1.29372 [446] train-rmse:1.23504 eval-rmse:1.2936 [447] train-rmse:1.23489 eval-rmse:1.29349 [448] train-rmse:1.23479 eval-rmse:1.29346 [449] train-rmse:1.23465 eval-rmse:1.29341 [450] train-rmse:1.2345 eval-rmse:1.29336 [451] train-rmse:1.23429 eval-rmse:1.29324 [452] train-rmse:1.23415 eval-rmse:1.29322 [453] train-rmse:1.23402 eval-rmse:1.29315 [454] train-rmse:1.23388 eval-rmse:1.29307 [455] train-rmse:1.23369 eval-rmse:1.29303 [456] train-rmse:1.23351 eval-rmse:1.29292 [457] train-rmse:1.23335 eval-rmse:1.29287 [458] train-rmse:1.23321 eval-rmse:1.29281 [459] train-rmse:1.23306 eval-rmse:1.29276 [460] train-rmse:1.2329 eval-rmse:1.29266 [461] train-rmse:1.23278 eval-rmse:1.29261 [462] train-rmse:1.23262 eval-rmse:1.29257 [463] train-rmse:1.23244 eval-rmse:1.29244 [464] train-rmse:1.23231 eval-rmse:1.29237 [465] train-rmse:1.23209 eval-rmse:1.29222 [466] train-rmse:1.23189 eval-rmse:1.29218 [467] train-rmse:1.23179 eval-rmse:1.29212 [468] train-rmse:1.23157 eval-rmse:1.29202 [469] train-rmse:1.23139 eval-rmse:1.29193 [470] train-rmse:1.23128 eval-rmse:1.29189 [471] train-rmse:1.23116 eval-rmse:1.29184 [472] train-rmse:1.23103 eval-rmse:1.29181 [473] train-rmse:1.23086 eval-rmse:1.29175 [474] train-rmse:1.23076 eval-rmse:1.2917 [475] train-rmse:1.23059 eval-rmse:1.2916 [476] train-rmse:1.23048 eval-rmse:1.29152 [477] train-rmse:1.23038 eval-rmse:1.29151 [478] train-rmse:1.23012 eval-rmse:1.29134 [479] train-rmse:1.2299 eval-rmse:1.29125 [480] train-rmse:1.22971 eval-rmse:1.29122 [481] train-rmse:1.22965 eval-rmse:1.2912 [482] train-rmse:1.22954 eval-rmse:1.29112 [483] train-rmse:1.2294 eval-rmse:1.29106 [484] train-rmse:1.22925 eval-rmse:1.29099 [485] train-rmse:1.22903 eval-rmse:1.29088 [486] train-rmse:1.22885 eval-rmse:1.2908 [487] train-rmse:1.22873 eval-rmse:1.29075 [488] train-rmse:1.22863 eval-rmse:1.29073 [489] train-rmse:1.2285 eval-rmse:1.29067 [490] train-rmse:1.22829 eval-rmse:1.29055 [491] train-rmse:1.22815 eval-rmse:1.29048 [492] train-rmse:1.22809 eval-rmse:1.29047 [493] train-rmse:1.22792 eval-rmse:1.29039 [494] train-rmse:1.22782 eval-rmse:1.29034 [495] train-rmse:1.22768 eval-rmse:1.29027 [496] train-rmse:1.22748 eval-rmse:1.2902 [497] train-rmse:1.22736 eval-rmse:1.29012 [498] train-rmse:1.22722 eval-rmse:1.29005 [499] train-rmse:1.22696 eval-rmse:1.28986 [500] train-rmse:1.22682 eval-rmse:1.28988 [501] train-rmse:1.22652 eval-rmse:1.28965 [502] train-rmse:1.22632 eval-rmse:1.28956 [503] train-rmse:1.22615 eval-rmse:1.28946 [504] train-rmse:1.22602 eval-rmse:1.2894 [505] train-rmse:1.2259 eval-rmse:1.28933 [506] train-rmse:1.22573 eval-rmse:1.28924 [507] train-rmse:1.22561 eval-rmse:1.28924 [508] train-rmse:1.22545 eval-rmse:1.28913 [509] train-rmse:1.22535 eval-rmse:1.28914 [510] train-rmse:1.22516 eval-rmse:1.28904 [511] train-rmse:1.22501 eval-rmse:1.28893 [512] train-rmse:1.22489 eval-rmse:1.28888 [513] train-rmse:1.22482 eval-rmse:1.28886 [514] train-rmse:1.22467 eval-rmse:1.28877 [515] train-rmse:1.22445 eval-rmse:1.28861 [516] train-rmse:1.22434 eval-rmse:1.28857 [517] train-rmse:1.22412 eval-rmse:1.28844 [518] train-rmse:1.22396 eval-rmse:1.28839 [519] train-rmse:1.22383 eval-rmse:1.28834 [520] train-rmse:1.22367 eval-rmse:1.28828 [521] train-rmse:1.22351 eval-rmse:1.28823 [522] train-rmse:1.22334 eval-rmse:1.28816 [523] train-rmse:1.22321 eval-rmse:1.28811 [524] train-rmse:1.22309 eval-rmse:1.28808 [525] train-rmse:1.22299 eval-rmse:1.28804 [526] train-rmse:1.22283 eval-rmse:1.28798 [527] train-rmse:1.22269 eval-rmse:1.28787 [528] train-rmse:1.22252 eval-rmse:1.28781 [529] train-rmse:1.22235 eval-rmse:1.28776 [530] train-rmse:1.22227 eval-rmse:1.28777 [531] train-rmse:1.22217 eval-rmse:1.28775 [532] train-rmse:1.22203 eval-rmse:1.28768 [533] train-rmse:1.22192 eval-rmse:1.28767 [534] train-rmse:1.22177 eval-rmse:1.28763 [535] train-rmse:1.22169 eval-rmse:1.28759 [536] train-rmse:1.22151 eval-rmse:1.28744 [537] train-rmse:1.22139 eval-rmse:1.28738 [538] train-rmse:1.22121 eval-rmse:1.28729 [539] train-rmse:1.22116 eval-rmse:1.28728 [540] train-rmse:1.22104 eval-rmse:1.28729 [541] train-rmse:1.221 eval-rmse:1.28729 [542] train-rmse:1.2209 eval-rmse:1.28727 [543] train-rmse:1.22076 eval-rmse:1.28724 [544] train-rmse:1.22059 eval-rmse:1.28721 [545] train-rmse:1.22052 eval-rmse:1.28719 [546] train-rmse:1.22039 eval-rmse:1.28718 [547] train-rmse:1.22022 eval-rmse:1.28721 [548] train-rmse:1.22005 eval-rmse:1.28716 [549] train-rmse:1.21994 eval-rmse:1.28715 [550] train-rmse:1.21985 eval-rmse:1.28711 [551] train-rmse:1.21975 eval-rmse:1.28707 [552] train-rmse:1.21967 eval-rmse:1.28705 [553] train-rmse:1.21951 eval-rmse:1.28697 [554] train-rmse:1.21934 eval-rmse:1.28689 [555] train-rmse:1.2192 eval-rmse:1.28681 [556] train-rmse:1.21905 eval-rmse:1.28672 [557] train-rmse:1.21892 eval-rmse:1.28666 [558] train-rmse:1.21882 eval-rmse:1.28664 [559] train-rmse:1.21864 eval-rmse:1.28651 [560] train-rmse:1.21854 eval-rmse:1.28655 [561] train-rmse:1.21843 eval-rmse:1.28651 [562] train-rmse:1.21831 eval-rmse:1.28644 [563] train-rmse:1.21825 eval-rmse:1.28644 [564] train-rmse:1.21803 eval-rmse:1.2863 [565] train-rmse:1.21793 eval-rmse:1.2863 [566] train-rmse:1.21785 eval-rmse:1.28627 [567] train-rmse:1.21772 eval-rmse:1.28626 [568] train-rmse:1.21749 eval-rmse:1.28614 [569] train-rmse:1.21739 eval-rmse:1.28613 [570] train-rmse:1.21728 eval-rmse:1.28608 [571] train-rmse:1.21717 eval-rmse:1.28605 [572] train-rmse:1.21702 eval-rmse:1.28598 [573] train-rmse:1.21693 eval-rmse:1.28595 [574] train-rmse:1.21683 eval-rmse:1.28591 [575] train-rmse:1.21673 eval-rmse:1.28588 [576] train-rmse:1.21665 eval-rmse:1.28586 [577] train-rmse:1.2165 eval-rmse:1.2858 [578] train-rmse:1.21636 eval-rmse:1.28574 [579] train-rmse:1.21614 eval-rmse:1.28559 [580] train-rmse:1.21606 eval-rmse:1.28557 [581] train-rmse:1.21595 eval-rmse:1.28551 [582] train-rmse:1.21574 eval-rmse:1.28538 [583] train-rmse:1.21553 eval-rmse:1.28523 [584] train-rmse:1.21541 eval-rmse:1.28522 [585] train-rmse:1.21528 eval-rmse:1.28519 [586] train-rmse:1.21509 eval-rmse:1.28518 [587] train-rmse:1.21491 eval-rmse:1.28508 [588] train-rmse:1.2148 eval-rmse:1.28503 [589] train-rmse:1.21473 eval-rmse:1.28503 [590] train-rmse:1.21452 eval-rmse:1.28489 [591] train-rmse:1.21435 eval-rmse:1.2848 [592] train-rmse:1.2142 eval-rmse:1.28474 [593] train-rmse:1.21411 eval-rmse:1.28472 [594] train-rmse:1.2139 eval-rmse:1.2846 [595] train-rmse:1.21378 eval-rmse:1.28456 [596] train-rmse:1.21371 eval-rmse:1.28453 [597] train-rmse:1.21355 eval-rmse:1.28447 [598] train-rmse:1.21347 eval-rmse:1.28446 [599] train-rmse:1.21338 eval-rmse:1.28444 [600] train-rmse:1.21319 eval-rmse:1.28442 [601] train-rmse:1.21305 eval-rmse:1.28438 [602] train-rmse:1.21292 eval-rmse:1.28436 [603] train-rmse:1.21278 eval-rmse:1.28433 [604] train-rmse:1.21265 eval-rmse:1.28426 [605] train-rmse:1.21256 eval-rmse:1.28425 [606] train-rmse:1.21245 eval-rmse:1.28426 [607] train-rmse:1.21232 eval-rmse:1.28422 [608] train-rmse:1.21226 eval-rmse:1.28422 [609] train-rmse:1.21213 eval-rmse:1.2842 [610] train-rmse:1.21205 eval-rmse:1.2842 [611] train-rmse:1.2119 eval-rmse:1.28413 [612] train-rmse:1.21175 eval-rmse:1.28409 [613] train-rmse:1.21169 eval-rmse:1.28409 [614] train-rmse:1.21155 eval-rmse:1.28402 [615] train-rmse:1.21146 eval-rmse:1.28399 [616] train-rmse:1.21132 eval-rmse:1.28394 [617] train-rmse:1.21118 eval-rmse:1.28394 [618] train-rmse:1.21103 eval-rmse:1.28388 [619] train-rmse:1.21083 eval-rmse:1.28388 [620] train-rmse:1.21074 eval-rmse:1.28385 [621] train-rmse:1.2106 eval-rmse:1.28378 [622] train-rmse:1.21045 eval-rmse:1.28372 [623] train-rmse:1.21036 eval-rmse:1.28373 [624] train-rmse:1.21018 eval-rmse:1.28368 [625] train-rmse:1.21009 eval-rmse:1.28365 [626] train-rmse:1.20999 eval-rmse:1.28363 [627] train-rmse:1.20983 eval-rmse:1.28356 [628] train-rmse:1.20972 eval-rmse:1.28352 [629] train-rmse:1.20961 eval-rmse:1.28349 [630] train-rmse:1.20949 eval-rmse:1.28348 [631] train-rmse:1.20939 eval-rmse:1.28347 [632] train-rmse:1.20928 eval-rmse:1.28344 [633] train-rmse:1.20915 eval-rmse:1.28338 [634] train-rmse:1.20904 eval-rmse:1.28338 [635] train-rmse:1.20889 eval-rmse:1.28331 [636] train-rmse:1.20873 eval-rmse:1.28332 [637] train-rmse:1.20863 eval-rmse:1.28329 [638] train-rmse:1.20853 eval-rmse:1.28323 [639] train-rmse:1.20837 eval-rmse:1.28314 [640] train-rmse:1.20825 eval-rmse:1.28307 [641] train-rmse:1.20812 eval-rmse:1.28303 [642] train-rmse:1.20802 eval-rmse:1.28304 [643] train-rmse:1.20788 eval-rmse:1.283 [644] train-rmse:1.20772 eval-rmse:1.28297 [645] train-rmse:1.20761 eval-rmse:1.28296 [646] train-rmse:1.20741 eval-rmse:1.28287 [647] train-rmse:1.20729 eval-rmse:1.28289 [648] train-rmse:1.20711 eval-rmse:1.28281 [649] train-rmse:1.20695 eval-rmse:1.28274 [650] train-rmse:1.20684 eval-rmse:1.28269 [651] train-rmse:1.20676 eval-rmse:1.28266 [652] train-rmse:1.20671 eval-rmse:1.28266 [653] train-rmse:1.20663 eval-rmse:1.28265 [654] train-rmse:1.20653 eval-rmse:1.28263 [655] train-rmse:1.20639 eval-rmse:1.28258 [656] train-rmse:1.20629 eval-rmse:1.28254 [657] train-rmse:1.2062 eval-rmse:1.2825 [658] train-rmse:1.20609 eval-rmse:1.28245 [659] train-rmse:1.206 eval-rmse:1.28245 [660] train-rmse:1.2059 eval-rmse:1.28245 [661] train-rmse:1.20575 eval-rmse:1.28238 [662] train-rmse:1.20562 eval-rmse:1.28232 [663] train-rmse:1.20551 eval-rmse:1.28229 [664] train-rmse:1.20535 eval-rmse:1.28223 [665] train-rmse:1.20521 eval-rmse:1.28217 [666] train-rmse:1.20509 eval-rmse:1.28214 [667] train-rmse:1.20501 eval-rmse:1.28217 [668] train-rmse:1.20493 eval-rmse:1.28216 [669] train-rmse:1.20484 eval-rmse:1.28215 [670] train-rmse:1.20476 eval-rmse:1.28213 [671] train-rmse:1.20464 eval-rmse:1.28211 [672] train-rmse:1.20451 eval-rmse:1.28209 [673] train-rmse:1.20443 eval-rmse:1.28207 [674] train-rmse:1.20426 eval-rmse:1.28201 [675] train-rmse:1.20416 eval-rmse:1.282 [676] train-rmse:1.20403 eval-rmse:1.28198 [677] train-rmse:1.20388 eval-rmse:1.28195 [678] train-rmse:1.20377 eval-rmse:1.28191 [679] train-rmse:1.20361 eval-rmse:1.28184 [680] train-rmse:1.20352 eval-rmse:1.28182 [681] train-rmse:1.20338 eval-rmse:1.28176 [682] train-rmse:1.20327 eval-rmse:1.28169 [683] train-rmse:1.20311 eval-rmse:1.28168 [684] train-rmse:1.20297 eval-rmse:1.28163 [685] train-rmse:1.20292 eval-rmse:1.28162 [686] train-rmse:1.20283 eval-rmse:1.28163 [687] train-rmse:1.20272 eval-rmse:1.28161 [688] train-rmse:1.20255 eval-rmse:1.28154 [689] train-rmse:1.20242 eval-rmse:1.2815 [690] train-rmse:1.2022 eval-rmse:1.28135 [691] train-rmse:1.20208 eval-rmse:1.28132 [692] train-rmse:1.20197 eval-rmse:1.28129 [693] train-rmse:1.20186 eval-rmse:1.28124 [694] train-rmse:1.20173 eval-rmse:1.28122 [695] train-rmse:1.20161 eval-rmse:1.2812 [696] train-rmse:1.20144 eval-rmse:1.28111 [697] train-rmse:1.2014 eval-rmse:1.28111 [698] train-rmse:1.20127 eval-rmse:1.28111 [699] train-rmse:1.20119 eval-rmse:1.28106 [700] train-rmse:1.20108 eval-rmse:1.28103 [701] train-rmse:1.20097 eval-rmse:1.28096 [702] train-rmse:1.20092 eval-rmse:1.28096 [703] train-rmse:1.20082 eval-rmse:1.28098 [704] train-rmse:1.20077 eval-rmse:1.28093 [705] train-rmse:1.20073 eval-rmse:1.28093 [706] train-rmse:1.2006 eval-rmse:1.28094 [707] train-rmse:1.20045 eval-rmse:1.2809 [708] train-rmse:1.20033 eval-rmse:1.28084 [709] train-rmse:1.20024 eval-rmse:1.28084 [710] train-rmse:1.20001 eval-rmse:1.28073 [711] train-rmse:1.19985 eval-rmse:1.28072 [712] train-rmse:1.19974 eval-rmse:1.28074 [713] train-rmse:1.19961 eval-rmse:1.28071 [714] train-rmse:1.19949 eval-rmse:1.28067 [715] train-rmse:1.19931 eval-rmse:1.28068 [716] train-rmse:1.19924 eval-rmse:1.28066 [717] train-rmse:1.19911 eval-rmse:1.28066 [718] train-rmse:1.199 eval-rmse:1.28065 [719] train-rmse:1.19889 eval-rmse:1.28062 [720] train-rmse:1.19875 eval-rmse:1.28058 [721] train-rmse:1.1986 eval-rmse:1.28054 [722] train-rmse:1.19852 eval-rmse:1.28048 [723] train-rmse:1.19834 eval-rmse:1.28044 [724] train-rmse:1.19823 eval-rmse:1.28043 [725] train-rmse:1.19811 eval-rmse:1.28039 [726] train-rmse:1.19802 eval-rmse:1.28036 [727] train-rmse:1.19791 eval-rmse:1.28037 [728] train-rmse:1.19775 eval-rmse:1.28036 [729] train-rmse:1.19762 eval-rmse:1.28035 [730] train-rmse:1.19748 eval-rmse:1.28029 [731] train-rmse:1.19739 eval-rmse:1.2803 [732] train-rmse:1.19724 eval-rmse:1.28025 [733] train-rmse:1.19716 eval-rmse:1.28023 [734] train-rmse:1.19707 eval-rmse:1.2802 [735] train-rmse:1.19697 eval-rmse:1.28019 [736] train-rmse:1.19685 eval-rmse:1.28018 [737] train-rmse:1.19674 eval-rmse:1.28018 [738] train-rmse:1.19667 eval-rmse:1.28018 [739] train-rmse:1.19658 eval-rmse:1.28015 [740] train-rmse:1.19646 eval-rmse:1.28015 [741] train-rmse:1.19636 eval-rmse:1.28011 [742] train-rmse:1.1963 eval-rmse:1.2801 [743] train-rmse:1.19616 eval-rmse:1.28004 [744] train-rmse:1.19606 eval-rmse:1.28003 [745] train-rmse:1.19591 eval-rmse:1.28003 [746] train-rmse:1.19581 eval-rmse:1.28002 [747] train-rmse:1.19571 eval-rmse:1.27998 [748] train-rmse:1.19556 eval-rmse:1.27997 [749] train-rmse:1.19539 eval-rmse:1.27988 [750] train-rmse:1.19533 eval-rmse:1.27988 [751] train-rmse:1.19524 eval-rmse:1.27985 [752] train-rmse:1.19518 eval-rmse:1.27981 [753] train-rmse:1.19514 eval-rmse:1.27981 [754] train-rmse:1.19507 eval-rmse:1.27982 [755] train-rmse:1.19498 eval-rmse:1.27984 [756] train-rmse:1.19488 eval-rmse:1.27987 [757] train-rmse:1.19474 eval-rmse:1.27979 [758] train-rmse:1.19458 eval-rmse:1.27975 [759] train-rmse:1.19443 eval-rmse:1.27975 [760] train-rmse:1.19428 eval-rmse:1.27973 [761] train-rmse:1.19413 eval-rmse:1.2797 [762] train-rmse:1.19401 eval-rmse:1.27967 [763] train-rmse:1.1939 eval-rmse:1.27963 [764] train-rmse:1.19382 eval-rmse:1.27959 [765] train-rmse:1.19371 eval-rmse:1.27957 [766] train-rmse:1.19356 eval-rmse:1.27954 [767] train-rmse:1.19349 eval-rmse:1.27955 [768] train-rmse:1.19335 eval-rmse:1.27954 [769] train-rmse:1.19324 eval-rmse:1.27954 [770] train-rmse:1.19312 eval-rmse:1.27959 [771] train-rmse:1.19305 eval-rmse:1.27956 [772] train-rmse:1.19291 eval-rmse:1.27953 [773] train-rmse:1.19286 eval-rmse:1.27954 [774] train-rmse:1.19276 eval-rmse:1.27952 [775] train-rmse:1.19261 eval-rmse:1.27949 [776] train-rmse:1.19252 eval-rmse:1.27947 [777] train-rmse:1.19245 eval-rmse:1.27946 [778] train-rmse:1.19232 eval-rmse:1.2795 [779] train-rmse:1.1922 eval-rmse:1.27947 [780] train-rmse:1.1921 eval-rmse:1.27946 [781] train-rmse:1.19202 eval-rmse:1.27944 [782] train-rmse:1.1919 eval-rmse:1.27937 [783] train-rmse:1.19176 eval-rmse:1.27937 [784] train-rmse:1.19161 eval-rmse:1.2793 [785] train-rmse:1.19153 eval-rmse:1.2793 [786] train-rmse:1.19143 eval-rmse:1.27929 [787] train-rmse:1.1913 eval-rmse:1.27927 [788] train-rmse:1.19115 eval-rmse:1.27927 [789] train-rmse:1.19104 eval-rmse:1.27926 [790] train-rmse:1.19097 eval-rmse:1.27926 [791] train-rmse:1.19087 eval-rmse:1.27928 [792] train-rmse:1.19075 eval-rmse:1.2792 [793] train-rmse:1.19064 eval-rmse:1.27918 [794] train-rmse:1.19054 eval-rmse:1.2792 [795] train-rmse:1.19046 eval-rmse:1.27918 [796] train-rmse:1.19027 eval-rmse:1.27912 [797] train-rmse:1.19016 eval-rmse:1.27911 [798] train-rmse:1.19001 eval-rmse:1.27909 [799] train-rmse:1.1899 eval-rmse:1.27904 [800] train-rmse:1.18979 eval-rmse:1.279 [801] train-rmse:1.18967 eval-rmse:1.27891 [802] train-rmse:1.18953 eval-rmse:1.27887 [803] train-rmse:1.18944 eval-rmse:1.2789 [804] train-rmse:1.18928 eval-rmse:1.27889 [805] train-rmse:1.18918 eval-rmse:1.27885 [806] train-rmse:1.18905 eval-rmse:1.27878 [807] train-rmse:1.18894 eval-rmse:1.27879 [808] train-rmse:1.18884 eval-rmse:1.27873 [809] train-rmse:1.18874 eval-rmse:1.2787 [810] train-rmse:1.18861 eval-rmse:1.27867 [811] train-rmse:1.1885 eval-rmse:1.27863 [812] train-rmse:1.18842 eval-rmse:1.27862 [813] train-rmse:1.1883 eval-rmse:1.27858 [814] train-rmse:1.18819 eval-rmse:1.27851 [815] train-rmse:1.18814 eval-rmse:1.2785 [816] train-rmse:1.18799 eval-rmse:1.27845 [817] train-rmse:1.18787 eval-rmse:1.27843 [818] train-rmse:1.18779 eval-rmse:1.27844 [819] train-rmse:1.18768 eval-rmse:1.2784 [820] train-rmse:1.18756 eval-rmse:1.27841 [821] train-rmse:1.18743 eval-rmse:1.27842 [822] train-rmse:1.18729 eval-rmse:1.27837 [823] train-rmse:1.18719 eval-rmse:1.27836 [824] train-rmse:1.18708 eval-rmse:1.27837 [825] train-rmse:1.18698 eval-rmse:1.27836 [826] train-rmse:1.18682 eval-rmse:1.2784 [827] train-rmse:1.18671 eval-rmse:1.27841 [828] train-rmse:1.18659 eval-rmse:1.27844 Stopping. Best iteration: [823] train-rmse:1.18719 eval-rmse:1.27836 . imps=regXGB.get_score(importance_type=&quot;gain&quot;) . mapper = {&#39;f{0}&#39;.format(i): v for i, v in enumerate(X_train.columns)} mapped = {mapper[k]: v for k, v in imps.items()} mapped . {&#39;Category&#39;: 86.01106266338357, &#39;Cluster&#39;: 62.58328920975717, &#39;Colour&#39;: 62.59635477396971, &#39;MEMBERID&#39;: 6.438786473627468, &#39;ProductID&#39;: 28.5300310862712, &#39;age&#39;: 6.448272046050587, &#39;features&#39;: 28.406104707330893, &#39;gender&#39;: 5.317592519525576, &#39;matherial&#39;: 42.912575481483564, &#39;product_group&#39;: 29.446970483139104, &#39;target_group&#39;: 26.31013688098456} . mapped={k: v for k, v in sorted(mapped.items(), key=lambda item: item[1])} . plt.barh(list(mapped.keys()), list(mapped.values()), align=&#39;center&#39;) . &lt;BarContainer object of 11 artists&gt; . preds178765[&#39;XGBOOST&#39;]=regXGB.predict(xgb.DMatrix(userTAB178765.values)) . preds178765.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET XGBOOST . 0 2.0 | 51.0 | 178765.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.237521 | 3.222044 | 3.357754 | . 1 2.0 | 51.0 | 178765.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.225422 | 4.481304 | 2.631370 | . 2 2.0 | 51.0 | 178765.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.104673 | 3.699023 | 3.218219 | . 3 2.0 | 51.0 | 178765.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.536121 | 2.612721 | 2.006189 | . 4 2.0 | 51.0 | 178765.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.562388 | 2.135189 | 1.997912 | . preds771733[&#39;XGBOOST&#39;]=regXGB.predict(xgb.DMatrix(userTAB771733.values)) . preds771733.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET XGBOOST . 0 2.0 | 42.0 | 771733.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.737335 | 4.995235 | 3.339263 | . 1 2.0 | 42.0 | 771733.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.690987 | 3.850037 | 2.423147 | . 2 2.0 | 42.0 | 771733.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.557454 | 3.507605 | 3.074020 | . 3 2.0 | 42.0 | 771733.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.694690 | 2.068795 | 2.013976 | . 4 2.0 | 42.0 | 771733.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.722045 | 2.161431 | 2.018070 | . !pip install surprise . Collecting surprise Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl Collecting scikit-surprise Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB) |████████████████████████████████| 11.8MB 262kB/s Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise-&gt;surprise) (1.0.1) Requirement already satisfied: numpy&gt;=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise-&gt;surprise) (1.19.5) Requirement already satisfied: scipy&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise-&gt;surprise) (1.4.1) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise-&gt;surprise) (1.15.0) Building wheels for collected packages: scikit-surprise Building wheel for scikit-surprise (setup.py) ... done Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1617534 sha256=7e11dd7f3a669259b0c1ef1b661db9bf98970f8a70b79bad1f00a675ee00137f Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0 Successfully built scikit-surprise Installing collected packages: scikit-surprise, surprise Successfully installed scikit-surprise-1.1.1 surprise-0.1 . SVD . A latent factor model involving parameter estimation by stochastic gradient descent. . from surprise import SVD,KNNWithMeans from surprise import Dataset,Reader from surprise.model_selection import cross_validate . reader = Reader(rating_scale=(0, 15)) data = Dataset.load_from_df(collabFiltering[[&#39;MEMBERID&#39;, &#39;ProductID&#39;, &#39;Count&#39;]], reader).build_full_trainset() . dataCV = Dataset.load_from_df(collabFiltering[[&#39;MEMBERID&#39;, &#39;ProductID&#39;, &#39;Count&#39;]], reader) . %%time algo = SVD(n_factors=50) algo.fit(data) . CPU times: user 8.1 s, sys: 9.67 ms, total: 8.11 s Wall time: 8.11 s . cross_validate(algo, dataCV, measures=[&#39;RMSE&#39;, &#39;MAE&#39;], cv=3, verbose=True) . Evaluating RMSE, MAE of algorithm SVD on 3 split(s). Fold 1 Fold 2 Fold 3 Mean Std RMSE (testset) 1.2679 1.2663 1.2623 1.2655 0.0024 MAE (testset) 0.8322 0.8293 0.8265 0.8293 0.0023 Fit time 4.97 4.97 5.01 4.99 0.02 Test time 0.47 0.47 0.48 0.47 0.01 . {&#39;fit_time&#39;: (4.97152304649353, 4.9744813442230225, 5.013360500335693), &#39;test_mae&#39;: array([0.83217444, 0.82932375, 0.82649174]), &#39;test_rmse&#39;: array([1.26786387, 1.26630617, 1.26225603]), &#39;test_time&#39;: (0.4653141498565674, 0.4667832851409912, 0.47748875617980957)} . est771733=[] . x=data.to_inner_uid(771733.0) uid=data.to_raw_uid(x) for prod in preds771733[&#39;ProductID&#39;].values: x=data.to_inner_iid(prod) iid=data.to_raw_iid(x) iid = str(prod) pred = algo.predict(uid, iid, verbose=False) est771733.append(pred.est) . preds771733[&#39;SVD&#39;]=est771733 . preds771733.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET XGBOOST SVD . 0 2.0 | 42.0 | 771733.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.591115 | 4.669821 | 3.406367 | 3.432931 | . 1 2.0 | 42.0 | 771733.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.679366 | 2.420033 | 2.445273 | 3.162018 | . 2 2.0 | 42.0 | 771733.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.559642 | 2.765619 | 3.083060 | 2.822812 | . 3 2.0 | 42.0 | 771733.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.665021 | 2.798069 | 2.005585 | 1.503755 | . 4 2.0 | 42.0 | 771733.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.688125 | 2.021320 | 2.014450 | 1.695521 | . est178765=[] . x=data.to_inner_uid(178765.0) uid=data.to_raw_uid(x) for prod in preds178765[&#39;ProductID&#39;].values: x=data.to_inner_iid(prod) iid=data.to_raw_iid(x) iid = str(prod) pred = algo.predict(uid, iid, verbose=False) est178765.append(pred.est) . preds178765[&#39;SVD&#39;]=est178765 . FASTAI . Deep learing based method using sigmoid in the output. . ! [ -e /content ] &amp;&amp; pip install -Uqq fastai . |████████████████████████████████| 194kB 5.3MB/s |████████████████████████████████| 61kB 7.4MB/s . from fastai.tabular.all import * from fastai.collab import * . dls = CollabDataLoaders.from_df(collabFiltering, bs=512,valid_pct=0.05) dls.show_batch() . MEMBERID ProductID Count . 0 603212 | 020EE1K309 | 4 | . 1 399045 | 040EE1C314 | 2 | . 2 6.65067e+06 | 069CC2K029 | 3 | . 3 790358 | 050EE1D302 | 2 | . 4 2.05658e+06 | 030EE1E306 | 4 | . 5 2.07586e+06 | 030EE2B303 | 3 | . 6 2.54369e+06 | 040EE1F319 | 3 | . 7 8.44458e+06 | 040EE1C307 | 2 | . 8 1.59278e+06 | 040EE2K302 | 3 | . 9 819114 | 020EE1G315 | 3 | . learn = collab_learner(dls, n_factors=64, y_range=(0, 15),use_nn=True,loss_func=MSELossFlat()) . learn.lr_find() . SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.004365158267319202) . learn.fit_one_cycle(2, 5e-3, wd=0.1, cbs=[ReduceLROnPlateau(monitor=&#39;valid_loss&#39;, min_delta=0.1, patience=2,factor=0.5,min_lr=1e-6)]) . epoch train_loss valid_loss time . 0 | 1.706153 | 1.658415 | 00:14 | . 1 | 1.044258 | 1.655310 | 00:14 | . dl771733=learn.dls.test_dl(preds771733[[&#39;MEMBERID&#39;,&#39;ProductID&#39;]]) . res771733=learn.get_preds(dl=dl771733) . res771733 . (tensor([[1.8110], [1.6873], [2.2668], ..., [2.0824], [3.7344], [2.0430]]), None) . preds771733[&#39;FASTAI&#39;]=res771733[0].squeeze().numpy() . preds771733.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET XGBOOST SVD FASTAI . 0 2.0 | 42.0 | 771733.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.591115 | 4.669821 | 3.406367 | 3.432931 | 1.811049 | . 1 2.0 | 42.0 | 771733.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.679366 | 2.420033 | 2.445273 | 3.162018 | 1.687348 | . 2 2.0 | 42.0 | 771733.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.559642 | 2.765619 | 3.083060 | 2.822812 | 2.266758 | . 3 2.0 | 42.0 | 771733.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.665021 | 2.798069 | 2.005585 | 1.503755 | 1.543742 | . 4 2.0 | 42.0 | 771733.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.688125 | 2.021320 | 2.014450 | 1.695521 | 1.496837 | . dl178765=learn.dls.test_dl(preds178765[[&#39;MEMBERID&#39;,&#39;ProductID&#39;]]) . res178765=learn.get_preds(dl=dl178765) . preds178765[&#39;FASTAI&#39;]=res178765[0].squeeze().numpy() . preds178765.head() . gender age MEMBERID ProductID Colour matherial product_group target_group features Category Cluster PredsxDeepFM TABNET XGBOOST SVD FASTAI . 0 2.0 | 51.0 | 178765.0 | 010CA1O304 | OTHER | Cotton | Accessories | Women | backpack | Accessories hard | Bags Non-Leather | 2.253060 | 3.220370 | 3.414702 | 2.528516 | 2.809034 | . 1 2.0 | 51.0 | 178765.0 | 010CA1Q302 | BLUE | Polyester | Accessories | Women | shawls | Accessories soft | Shawls/Scarves | 2.323148 | 3.112500 | 2.658072 | 2.479034 | 2.186305 | . 2 2.0 | 51.0 | 178765.0 | 010CA1S301 | BLUE | Buffalo Split Leather | Accessories | Women | belts cm | Accessories hard | Belts | 2.206944 | 2.699850 | 3.202499 | 2.243059 | 3.152376 | . 3 2.0 | 51.0 | 178765.0 | 010CC1B302 | GREY | Cotton | Women | Women | length service | Denim | Pants Denim | 1.583255 | 2.939901 | 1.989041 | 0.944824 | 1.777302 | . 4 2.0 | 51.0 | 178765.0 | 010CC1B305 | OTHER | Cotton | Women | Women | length service | Denim | Pants Denim | 1.607694 | 2.143070 | 1.998453 | 0.977214 | 1.883812 | . ENSEMBLE . We combine all model and make prediction in probabilitic sense. . results771733=preds771733[[&#39;MEMBERID&#39;,&#39;ProductID&#39;,&#39;PredsxDeepFM&#39;,&#39;TABNET&#39;,&#39;XGBOOST&#39;,&#39;SVD&#39;,&#39;FASTAI&#39;]] . results178765=preds178765[[&#39;MEMBERID&#39;,&#39;ProductID&#39;,&#39;PredsxDeepFM&#39;,&#39;TABNET&#39;,&#39;XGBOOST&#39;,&#39;SVD&#39;,&#39;FASTAI&#39;]] . ens178765=results178765[&#39;PredsxDeepFM&#39;]*0.3+0.2*results178765[&#39;TABNET&#39;]+0.2*results178765[&#39;XGBOOST&#39;]+0.2*results178765[&#39;FASTAI&#39;]+0.1*results178765[&#39;SVD&#39;] . mixup178765=preds178765[[&#39;MEMBERID&#39;,&#39;ProductID&#39;]].copy() . mixup178765[&#39;ENS&#39;]=ens178765 . prods178765=mixup178765.sort_values(by=&#39;ENS&#39;,ascending=False)[&#39;ProductID&#39;][:20].values . weights178765=mixup178765.sort_values(by=&#39;ENS&#39;,ascending=False)[&#39;ENS&#39;][:20].values/(mixup178765.sort_values(by=&#39;ENS&#39;,ascending=False)[&#39;ENS&#39;][:20].sum()) . only sampling from top 20. . np.random.choice(a=prods178765,p=weights178765,size=5) . array([&#39;990EF1A342&#39;, &#39;990EF1A338&#39;, &#39;990EF1A342&#39;, &#39;128EO1E003&#39;, &#39;990EF1A343&#39;], dtype=object) . ens178765=ens178765/ens178765.sum() . MEMBER = 178765 . Samling from all. . np.random.choice(a=results178765[&#39;ProductID&#39;].values,p=ens178765,size=10) . array([&#39;069EO1E021&#39;, &#39;020EO2F304&#39;, &#39;088CA1Q005&#39;, &#39;060EE1E334&#39;, &#39;069EF1A022&#39;, &#39;040EF7A319&#39;, &#39;050EE1F351&#39;, &#39;050EE2K354&#39;, &#39;058EE1K077&#39;, &#39;079EI1B014&#39;], dtype=object) . mixup771733=preds771733[[&#39;MEMBERID&#39;,&#39;ProductID&#39;]].copy() . ens771733=results771733[&#39;PredsxDeepFM&#39;]*0.3+0.2*results771733[&#39;TABNET&#39;]+0.2*results771733[&#39;XGBOOST&#39;]+0.2*results771733[&#39;FASTAI&#39;]+0.1*results771733[&#39;SVD&#39;] . mixup771733[&#39;ENS&#39;]=ens771733 . Only sampling from top 20. . MEMBER = 771733 . prods771733=mixup771733.sort_values(by=&#39;ENS&#39;,ascending=False)[&#39;ProductID&#39;][:20].values weights771733=mixup771733.sort_values(by=&#39;ENS&#39;,ascending=False)[&#39;ENS&#39;][:20].values/(mixup771733.sort_values(by=&#39;ENS&#39;,ascending=False)[&#39;ENS&#39;][:20].sum()) np.random.choice(a=prods771733,p=weights771733,size=5) . array([&#39;990EF1A343&#39;, &#39;999EO1B805&#39;, &#39;990EF1A342&#39;, &#39;999EO1B805&#39;, &#39;999EF1A836&#39;], dtype=object) . ens771733=ens771733/ens771733.sum() . sampling from all . np.random.choice(a=results771733[&#39;ProductID&#39;].values,p=ens771733,size=10) . array([&#39;069EA1Q004&#39;, &#39;998EF1A824&#39;, &#39;050EE2K308&#39;, &#39;020EF8A304&#39;, &#39;040EE2K313&#39;, &#39;069EE1B023&#39;, &#39;049EF1A092&#39;, &#39;030EO1G304&#39;, &#39;119EG1T007&#39;, &#39;099EF1T063&#39;], dtype=object) .",
            "url": "https://ashish244co.github.io/blog/recommendation%20systems/jupyter/2021/04/04/recommend-system-fashion.html",
            "relUrl": "/recommendation%20systems/jupyter/2021/04/04/recommend-system-fashion.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Sentiment classification",
            "content": "Data download . !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz !tar -xzf aclImdb_v1.tar.gz !ls . --2020-12-04 12:11:02-- https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10 Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 84125825 (80M) [application/x-gzip] Saving to: ‘aclImdb_v1.tar.gz’ aclImdb_v1.tar.gz 100%[===================&gt;] 80.23M 20.3MB/s in 7.1s 2020-12-04 12:11:09 (11.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825] aclImdb aclImdb_v1.tar.gz sample_data . Alternative with tf.datasets . I imported data in my own way since i thought it would be a good idea to do semi supervised traiing also using the unsupervised text. . def read_file(path): with open(path, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file: lines = file.readlines() text = &quot; &quot;.join(lines) return text . import glob import os . def getData(): path_posttrain = os.path.join(&#39;aclImdb/train/&#39;, &quot;pos&quot;, &quot;*.txt&quot;) path_negtrain = os.path.join(&#39;aclImdb/train/&#39;, &quot;neg&quot;, &quot;*.txt&quot;) postrain = glob.glob(path_posttrain) negtrain = glob.glob(path_negtrain) data_pos_train = [read_file(path) for path in postrain] data_neg_train = [read_file(path) for path in negtrain] path_unsup = os.path.join(&#39;aclImdb/train/&#39;, &quot;unsup&quot;, &quot;*.txt&quot;) unsup = glob.glob(path_unsup) unsup_train = [read_file(path) for path in unsup] path_postest = os.path.join(&#39;aclImdb/test/&#39;, &quot;pos&quot;, &quot;*.txt&quot;) path_negtest = os.path.join(&#39;aclImdb/test/&#39;, &quot;neg&quot;, &quot;*.txt&quot;) postest = glob.glob(path_postest) negtest = glob.glob(path_negtest) data_pos_test = [read_file(path) for path in postest] data_neg_test = [read_file(path) for path in negtest] return data_pos_train+data_neg_train,[1.0] * len(data_pos_train) + [0.0] * len(data_neg_train),data_pos_test+data_neg_test,[1.0] * len(data_pos_test) + [0.0] * len(data_neg_test),unsup_train . trainX,trainY,testX,testY,unsup=getData() . trainX[0],trainY[0] . (&#34;A couple(Janet and Richard) go camping out in the woods near a giant swamp. After camping and enjoying nature, the couple takes shelter in what they think is an abandoned farm house. Soon, a pair of escaped convicts show up and, after much delaying of the inevitable, they proceed to rape Janet and lock Richard in a birdcage.&lt;br /&gt;&lt;br /&gt;This LAST HOUSE ON THE LEFT-like film has to be one of the most underrated horror films ever made. It&#39;s one of the more sick and twisted early 70s shockers. Moreover, I found this to be quite enchanting and beautiful in it&#39;s perverse tone. I love CAGED TERROR. The music definitely helps lend a sense of personality to the film as well as a lot of beauty. I found the film to be quite creepy.&lt;br /&gt;&lt;br /&gt;The flaws mainly have to do with the pacing of the film, which is to say that the film is rather slow and meandering. While I didn&#39;t mind the pacing due to the beauty and suspense of the film in question, I do think that it will both most people. The acting isn&#39;t too good nor is the dialogue, at least in the early scenes. This film takes a little more patience than usual, and it&#39;s really not for everyone.&lt;br /&gt;&lt;br /&gt;In short, this was a good film. Not the greatest horror film I&#39;ve ever seen, but it is certainly a lot of fun. It&#39;s not exactly the easiest film to find. It&#39;s possible to find it in the USED section of a lot of stores if you look hard enough. It&#39;s not for everyone, but if you&#39;re a fan of trash cinema then it&#39;s definitely worth checking out.&#34;, 1.0) . testX[0],testY[0] . (&#34;This Metro film is episodic, but nearly a constant series of chases, mainly trying to escape police, whether real or imagined, as Buster is mistaken for an escaped criminal. It is consistently inventive and entertaining. Its greatest value is in its documenting what Hollywood looked like in the early twenties, since 95% of it is shot outside among the streets and building exteriors of the time. One gem moment and one gem sequence are present here.&lt;br /&gt;&lt;br /&gt;The great moment is when a train at a great distance quickly approaches the camera and finally stops just short of it - with Buster glumly sitting on the cowcatcher and thus moving from a long shot to a close-up within seconds.&lt;br /&gt;&lt;br /&gt;The great sequence is with the phone booth next to the elevator - one constantly being mistaken for the other with races from floor to floor - one of the great Keaton gags.&lt;br /&gt;&lt;br /&gt;Kino&#39;s print is sharp and clear - almost pristine. There is a violin/piano score accompaniment. This is one to seek out and enjoy.&#34;, 1.0) . import pandas as pd . unsupDf=pd.DataFrame(unsup,columns=[&#39;Text&#39;]) . trainDf=pd.DataFrame(trainX,columns=[&#39;Text&#39;]) . testDf=pd.DataFrame(testX,columns=[&#39;Text&#39;]) . trainDf[&#39;Label&#39;]=trainY . testDf[&#39;Label&#39;]=testY . trainDf=trainDf.sample(frac=1).reset_index(drop=True) . testDf=testDf.sample(frac=1).reset_index(drop=True) . trainDf.to_csv(&#39;train.csv&#39;) testDf.to_csv(&#39;test.csv&#39;) . trainPlusUnsupDf=pd.concat([unsupDf[&#39;Text&#39;],trainDf[&#39;Text&#39;]]) . trainPlusUnsupDf=pd.DataFrame(trainPlusUnsupDf,columns=[&#39;Text&#39;]) . I use spacy here for text preprocessing for bag of words based model which i use as a tokenizer in the input of tfidf vectorizer. So spacy here toenizes and cleans as well. . %%capture !pip install spacy !pip install en_core_web_sm !pip install optuna . import spacy . Diabled tagger and other part of pipelines since they take it takes a lot of time to run or else. . nlp = spacy.load(&quot;en_core_web_sm&quot;, disable=[&#39;parser&#39;, &#39;ner&#39;,&#39;tagger&#39;]) . nlp.Defaults.stop_words.update([&#39;from&#39;, &#39;subject&#39;, &#39;re&#39;, &#39;edu&#39;, &#39;use&#39;, &#39;not&#39;, &#39;would&#39;, &#39;say&#39;, &#39;could&#39;, &#39;_&#39;, &#39;be&#39;, &#39;know&#39;, &#39;good&#39;, &#39;go&#39;, &#39;get&#39;, &#39;do&#39;, &#39;done&#39;, &#39;try&#39;, &#39;many&#39;, &#39;some&#39;, &#39;nice&#39;, &#39;thank&#39;, &#39;think&#39;, &#39;see&#39;, &#39;rather&#39;, &#39;easy&#39;, &#39;easily&#39;, &#39;lot&#39;, &#39;lack&#39;, &#39;make&#39;, &#39;want&#39;, &#39;seem&#39;, &#39;run&#39;, &#39;need&#39;, &#39;even&#39;, &#39;right&#39;, &#39;line&#39;, &#39;even&#39;, &#39;also&#39;, &#39;may&#39;, &#39;take&#39;, &#39;come&#39;]) . import string . also using regex for basic text preprocessing for deep learning and fastext models. which i just clearing html texts ensuring only alphanumeric characters are there and replacing number with num tag. . from bs4 import BeautifulSoup . Cleaning . import re def alpha_num(text): return re.sub(r&#39;[^A-Za-z0-9 ]&#39;, &#39;&#39;, text) #a good idea to replace all the numbers with a special token def replace_num(text): return re.sub(r&#39;[0-9]&#39;, &#39;__NUM__&#39;, text) . trainPlusUnsupDf[&#39;Text&#39;] = trainPlusUnsupDf[&#39;Text&#39;].str.lower() trainPlusUnsupDf[&#39;Text&#39;]= trainPlusUnsupDf[&#39;Text&#39;].apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) trainPlusUnsupDf[&#39;Text&#39;] = trainPlusUnsupDf[&#39;Text&#39;].apply(alpha_num) trainPlusUnsupDf[&#39;Text&#39;] = trainPlusUnsupDf[&#39;Text&#39;].apply(replace_num) . def spacyTokenize(text): doc = nlp(text) text_words = [token for token in doc] return text_words . def tokenizerBOW(text): doc = nlp(text) text_words = [token for token in doc if token.is_alpha and not token.is_stop and not token.is_digit and token.text not in string.punctuation] text_words = [ token.lemma_.lower().strip() if token.lemma_ != &quot;-PRON-&quot; else token.lower_ for token in text_words ] return text_words . The tokenizer also automatically lemmatise and removes stop words and puncuations and other basic checks using spacy. . %%time trainPlusUnsupDf[&#39;TextTOK&#39;]=trainPlusUnsupDf[&#39;Text&#39;].apply(spacyTokenize) trainPlusUnsupDf[&#39;length&#39;]=trainPlusUnsupDf[&#39;TextTOK&#39;].apply(lambda x:len(x)) . CPU times: user 1min 12s, sys: 681 ms, total: 1min 13s Wall time: 1min 13s . trainPlusUnsupDf[&#39;length&#39;].describe() . count 75000.000000 mean 237.455867 std 175.447057 min 9.000000 25% 129.000000 50% 178.000000 75% 288.000000 max 2503.000000 Name: length, dtype: float64 . import seaborn as sns . sns.distplot(trainPlusUnsupDf[&#39;length&#39;],kde=False) . /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0f383a8c18&gt; . here we can see the distribution of length of input text mostly here i guess maxlen of 300 will be a good idea which kind of cover approx till 75 percentile of all the texts. . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.preprocessing import StandardScaler . from scipy import sparse . def pipelineBOW(data): tfidfVectorizer = TfidfVectorizer(tokenizer = tokenizerBOW,ngram_range=(1,3),sublinear_tf=1,strip_accents=&#39;unicode&#39;,max_features = 40000) data=data.apply(lambda x:x.lower()) data=data.apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) tfidfVectorizer.fit(data.values) print(&#39;TFIDF Done&#39;) return tfidfVectorizer . def processData(data,tfidfVectorizer): data=data.apply(lambda x:x.lower()) data=tfidfVectorizer.transform(data.values) return data . Modelling . from sklearn import model_selection . from sklearn.model_selection import cross_val_score . import numpy as np . trainPlusUnsupDf.head() . Text TextTOK length . 0 oil industrialist leonard dawson had a __NUM__... | [oil, industrialist, leonard, dawson, had, a, ... | 213 | . 1 having recently revisited my old van damme col... | [having, recently, revisited, my, old, van, da... | 940 | . 2 i thought this might be funny going in and tay... | [i, thought, this, might, be, funny, going, in... | 210 | . 3 one of the many movies that mistakes profanity... | [one, of, the, many, movies, that, mistakes, p... | 132 | . 4 god will forgive them etc this is the best lin... | [god, will, forgive, them, etc, this, is, the,... | 245 | . %%time tfidfVectorizer=pipelineBOW(trainPlusUnsupDf[&#39;Text&#39;]) . /usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter &#39;token_pattern&#39; will not be used since &#39;tokenizer&#39; is not None&#39; warnings.warn(&#34;The parameter &#39;token_pattern&#39; will not be used&#34; . TFIDF Done CPU times: user 3min 11s, sys: 366 ms, total: 3min 11s Wall time: 3min 11s . %%time train=processData(trainDf[&#39;Text&#39;],tfidfVectorizer) . CPU times: user 45.7 s, sys: 0 ns, total: 45.7 s Wall time: 45.7 s . %%time test=processData(testDf[&#39;Text&#39;],tfidfVectorizer) . CPU times: user 43.9 s, sys: 0 ns, total: 43.9 s Wall time: 43.9 s . from sklearn.metrics import classification_report from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import LinearSVC . import optuna . Instead of handpickin parameter by myselfi automated the whole process using opuna where he choice of models are linear support vector machines, random forest and logistic regression. . Optuna by default uses TPE sampler. . According to the documentation: . This sampler is based on independent sampling. See also BaseSampler for more details of ‘independent sampling’. . On each trial, for each parameter, TPE fits one Gaussian Mixture Model (GMM) l(x) to the set of parameter values associated with the best objective values, and another GMM g(x) to the remaining parameter values. It chooses the parameter value x that maximizes the ratio l(x)/g(x). . %%time def objectiveSklearn(trial): classifier_name = trial.suggest_categorical(&#39;classifier&#39;, [&#39;LogReg&#39;,&#39;SVC&#39;,&#39;RandomForest&#39;]) if classifier_name == &#39;LogReg&#39;: loss=trial.suggest_categorical(&quot;loss&quot;, [&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;]) classifier_obj = LogisticRegression(class_weight=&#39;balanced&#39;,n_jobs=-1) if (classifier_name == &#39;SVC&#39;): svc_c = trial.suggest_float(&quot;c&quot;, 1e-10, 1e10, log=True) classifier_obj = LinearSVC(C=svc_c) else: rf_max_depth = int(trial.suggest_int(&#39;rf_max_depth&#39;, 3, 15)) n_est=int(trial.suggest_categorical(&quot;n_estimator&quot;, [100,200,300,400,500])) classifier_obj = RandomForestClassifier(max_depth=rf_max_depth, n_estimators=n_est,n_jobs=-1) accuracy=cross_val_score(classifier_obj, train, trainDf[&#39;Label&#39;].values, cv=3,n_jobs=-1) return np.mean(accuracy) studySK = optuna.create_study(direction=&#39;maximize&#39;) studySK.optimize(objectiveSklearn, n_trials=30) . [I 2020-12-04 13:28:48,002] A new study created in memory with name: no-name-1b77f598-6c55-4079-b4cf-c6288f590dbe [I 2020-12-04 13:29:14,304] Trial 0 finished with value: 0.8346396922831557 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;l2&#39;, &#39;rf_max_depth&#39;: 12, &#39;n_estimator&#39;: 400}. Best is trial 0 with value: 0.8346396922831557. [I 2020-12-04 13:29:22,459] Trial 1 finished with value: 0.8660798555772325 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 1112.8121932926088}. Best is trial 1 with value: 0.8660798555772325. [I 2020-12-04 13:29:24,040] Trial 2 finished with value: 0.870639869991248 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 6.091001448466234}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:29:37,624] Trial 3 finished with value: 0.8310396010752837 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;l1&#39;, &#39;rf_max_depth&#39;: 6, &#39;n_estimator&#39;: 400}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:29:43,109] Trial 4 finished with value: 0.8146395882233177 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 4, &#39;n_estimator&#39;: 200}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:11,168] Trial 5 finished with value: 0.8359996170905152 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 11, &#39;n_estimator&#39;: 500}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:12,967] Trial 6 finished with value: 0.8660398539771684 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 34653.80363906542}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:14,739] Trial 7 finished with value: 0.8660398539771684 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 6089723.822115924}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:28,199] Trial 8 finished with value: 0.827719765858069 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 6, &#39;n_estimator&#39;: 400}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:41,732] Trial 9 finished with value: 0.8342797162810438 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;elasticnet&#39;, &#39;rf_max_depth&#39;: 13, &#39;n_estimator&#39;: 200}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:42,019] Trial 10 finished with value: 0.821519594645077 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 3.640261501310243e-07}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:42,450] Trial 11 finished with value: 0.8631596859746721 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.019657026783969394}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:44,891] Trial 12 finished with value: 0.8689198827852321 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 11.204722563404262}. Best is trial 2 with value: 0.870639869991248. [I 2020-12-04 13:30:45,325] Trial 13 finished with value: 0.8796797996229905 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.08395483786035728}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:45,637] Trial 14 finished with value: 0.8316796266763077 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.00016477035976404718}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:45,876] Trial 15 finished with value: 0.820759588242901 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 1.6781273218095836e-10}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:47,688] Trial 16 finished with value: 0.8660398539771684 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 3470240422.013036}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:48,115] Trial 17 finished with value: 0.8726396620059665 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.041590138298862295}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:48,454] Trial 18 finished with value: 0.8322796026791875 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 3.794433800671928e-05}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:48,881] Trial 19 finished with value: 0.8632396747753758 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.02024212363284256}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:57,113] Trial 20 finished with value: 0.8250796650536528 and parameters: {&#39;classifier&#39;: &#39;RandomForest&#39;, &#39;rf_max_depth&#39;: 15, &#39;n_estimator&#39;: 100}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:58,166] Trial 21 finished with value: 0.8745198428047513 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 2.8141429666154796}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:30:58,588] Trial 22 finished with value: 0.8779197484194065 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.06403159117739417}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:06,631] Trial 23 finished with value: 0.8662398619774884 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 322.3759012574662}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:06,967] Trial 24 finished with value: 0.8316396010772037 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 6.562613218751375e-05}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:19,668] Trial 25 finished with value: 0.8295197706636367 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;l2&#39;, &#39;rf_max_depth&#39;: 8, &#39;n_estimator&#39;: 300}. Best is trial 13 with value: 0.8796797996229905. [I 2020-12-04 13:31:20,207] Trial 26 finished with value: 0.8853198188402698 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.46782324148727283}. Best is trial 26 with value: 0.8853198188402698. [I 2020-12-04 13:31:20,442] Trial 27 finished with value: 0.820759593042709 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 5.226694743474181e-08}. Best is trial 26 with value: 0.8853198188402698. [I 2020-12-04 13:31:20,802] Trial 28 finished with value: 0.8414796443069626 and parameters: {&#39;classifier&#39;: &#39;SVC&#39;, &#39;c&#39;: 0.003193234097835833}. Best is trial 26 with value: 0.8853198188402698. [I 2020-12-04 13:31:32,070] Trial 29 finished with value: 0.8235597290462292 and parameters: {&#39;classifier&#39;: &#39;LogReg&#39;, &#39;loss&#39;: &#39;elasticnet&#39;, &#39;rf_max_depth&#39;: 3, &#39;n_estimator&#39;: 500}. Best is trial 26 with value: 0.8853198188402698. . CPU times: user 1.49 s, sys: 136 ms, total: 1.62 s Wall time: 2min 44s . studySK.best_params . {&#39;c&#39;: 0.46782324148727283, &#39;classifier&#39;: &#39;SVC&#39;} . svc=LinearSVC(C=studySK.best_params[&#39;c&#39;]) svc.fit(train, trainDf[&#39;Label&#39;].values) . LinearSVC(C=0.46782324148727283, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001, verbose=0) . print(classification_report(svc.predict(test),testDf[&#39;Label&#39;])) . precision recall f1-score support 0.0 0.88 0.87 0.87 12646 1.0 0.87 0.88 0.87 12354 accuracy 0.87 25000 macro avg 0.87 0.87 0.87 25000 weighted avg 0.87 0.87 0.87 25000 . The BOW models performs well infact one could use the unsupervised data also for TFIDF which gives us good result. Another imporvement could have been using truncated SVD in the pipeline to make it even more easier for models to fit. Since fitting models was not time taking much we could do an exhaustive search for hyperparameters and find good one for these models. . FAST TEXT . Fast text expect inout data to be of a certain form hence we need to preprocess our data accordingly. It is a method for cpu utilisation and seems to be quite robust and compact . !pip install fasttext . Collecting fasttext Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB) |████████████████████████████████| 71kB 3.8MB/s Requirement already satisfied: pybind11&gt;=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1) Requirement already satisfied: setuptools&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5) Building wheels for collected packages: fasttext Building wheel for fasttext (setup.py) ... done Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3038678 sha256=402134e5718634d7967fe276865bdb4c7c97360bb7bb1ebc28bc25e836f1e89c Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592 Successfully built fasttext Installing collected packages: fasttext Successfully installed fasttext-0.9.2 . We clean minimally for deep learning models. . trainDf[&#39;Text&#39;]= trainDf[&#39;Text&#39;].apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) trainDf[&#39;Text&#39;] = trainDf[&#39;Text&#39;].str.lower() trainDf[&#39;Text&#39;] = trainDf[&#39;Text&#39;].apply(alpha_num) trainDf[&#39;Text&#39;] = trainDf[&#39;Text&#39;].apply(replace_num) . testDf[&#39;Text&#39;]= testDf[&#39;Text&#39;].apply(lambda x:BeautifulSoup(x,&#39;html.parser&#39;).get_text()) testDf[&#39;Text&#39;] = testDf[&#39;Text&#39;].str.lower() testDf[&#39;Text&#39;] = testDf[&#39;Text&#39;].apply(alpha_num) testDf[&#39;Text&#39;] = testDf[&#39;Text&#39;].apply(replace_num) . def makeFastextData(data,dataset): with open(&#39;fasttext_input_imdb_&#39;+dataset +&#39;.txt&#39;, &#39;w&#39;) as f: for text,label in zip(data[&#39;Text&#39;],data[&#39;Label&#39;]): f.writelines(f&#39;__label__{label} {text} n&#39;) . makeFastextData(trainDf,&#39;train&#39;) . makeFastextData(testDf,&#39;test&#39;) . !wc fasttext_input_imdb_train.txt . 25000 5744161 32330565 fasttext_input_imdb_train.txt . !wc fasttext_input_imdb_test.txt . 25000 5615443 31564214 fasttext_input_imdb_test.txt . Some extra preprocessing . !cat fasttext_input_imdb_train.txt | sed -e &quot;s/ ([. !?,&#39;/()] )/ 1 /g&quot; | tr &quot;[:upper:]&quot; &quot;[:lower:]&quot; &gt; fasttext_input_imdb_train_proc.txt . !cat fasttext_input_imdb_test.txt | sed -e &quot;s/ ([. !?,&#39;/()] )/ 1 /g&quot; | tr &quot;[:upper:]&quot; &quot;[:lower:]&quot; &gt; fasttext_input_imdb_test_proc.txt . !head -n 24000 fasttext_input_imdb_train_proc.txt &gt; imdb_train.bin . !tail -n 1000 fasttext_input_imdb_train_proc.txt &gt; imdb_valid.bin . !head -n 25000 fasttext_input_imdb_test_proc.txt &gt; imdb_test.bin . import fasttext . Here we use autotune funcionality of fasttext to find a perfect model for us by doing changes based on validation data. limited the duration to 100 for fast execution and also limit the size of the output model. For faster convergence we use hierarchical softmax as the loss. . %%time model = fasttext.train_supervised(input=&#39;imdb_train.bin&#39;,autotuneValidationFile=&#39;imdb_valid.bin&#39;,autotuneModelSize=&quot;2M&quot;, autotuneDuration=100, loss=&#39;hs&#39;) . CPU times: user 2min 22s, sys: 31.1 ms, total: 2min 22s Wall time: 2min 22s . _,precision,recall=model.test(&#39;imdb_test.bin&#39;) . print(&#39;Precision:&#39;,precision,&#39;recall:&#39;,recall) . Precision: 0.87892 recall: 0.87892 . saved the model here in compact quantized form so that it can be use later. . model.quantize(retrain=False) model.save_model(&quot;model_filename.ftz&quot;) . Fast text is a very good easy to use and very well engineered solution which give great results too. i tried to search for a way to learn representation present in text and use it as a start for classification but could not find that bridge although one can learn the representation easily by using function learn_unsupervised . Sequence RNN . we use keras tokenizer trained on the unsupervised+training test and then transform test data using it as well as train. i experiment with 2 layer lstm structure for which we select hyperparameters using optuna again. used a 2 layer lstm structure for which we find optimal hyperparameter due to resource content and time needed by models to run we dont choose many trials. . import tensorflow . from tensorflow.keras.preprocessing.text import Tokenizer . t = Tokenizer() t.fit_on_texts(trainPlusUnsupDf[&#39;Text&#39;].values) . trainText = t.texts_to_sequences(trainDf[&#39;Text&#39;].values) . testText = t.texts_to_sequences(testDf[&#39;Text&#39;].values) . trainTextPadded=tensorflow.keras.preprocessing.sequence.pad_sequences(trainText, maxlen=300, dtype=&#39;int32&#39;, padding=&#39;post&#39;, truncating=&#39;post&#39;) . testTextPadded=tensorflow.keras.preprocessing.sequence.pad_sequences(testText, maxlen=300, dtype=&#39;int32&#39;, padding=&#39;post&#39;, truncating=&#39;post&#39;) . from sklearn.model_selection import train_test_split . from tensorflow.keras.layers import LSTM,Embedding,GRU,Dense,Input,Dropout,Bidirectional from tensorflow.keras import Model from tensorflow.keras.optimizers import RMSprop,Adam . X_train, X_test, y_train, y_test = train_test_split(trainTextPadded, trainDf[&#39;Label&#39;], test_size=0.2, random_state=42) . %%time def objectiveLSTM(trial): inpt = Input(shape = (300,)) embedding = Embedding(len(t.word_index) + 1,output_dim=trial.suggest_categorical(&quot;embedding&quot;, [50,100]), input_length=300, mask_zero=True)(inpt) LSTM1=Bidirectional(LSTM(units=trial.suggest_categorical(&quot;units1&quot;, [64,128,256]),return_sequences=True))(embedding) dropout=Dropout(rate=trial.suggest_loguniform(&#39;dropout&#39;, 1e-1, 5e-1))(LSTM1) LSTM2=Bidirectional(LSTM(units=trial.suggest_categorical(&quot;units2&quot;, [32,64,128]),return_sequences=False))(dropout) output=Dense(1, activation=&#39;sigmoid&#39;)(LSTM2) model=Model(inpt,output) lr = trial.suggest_loguniform(&#39;lr&#39;, 1e-5, 1e-2) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=RMSprop(lr=lr), metrics=[&#39;accuracy&#39;]) model.fit(X_train,y_train,validation_split=0.1,shuffle=True,batch_size=512,epochs=10,verbose=True) accuracy=model.evaluate(X_test,y_test) return accuracy[0] studyDL = optuna.create_study(direction=&#39;minimize&#39;) studyDL.optimize(objectiveLSTM, n_trials=5) . [I 2020-12-04 15:56:48,108] A new study created in memory with name: no-name-63df6f6a-b44a-48ca-9ae5-105f9e61f3a7 . Epoch 1/10 36/36 [==============================] - 14s 377ms/step - loss: 0.6842 - accuracy: 0.6080 - val_loss: 0.6049 - val_accuracy: 0.6760 Epoch 2/10 36/36 [==============================] - 9s 259ms/step - loss: 0.5117 - accuracy: 0.7650 - val_loss: 0.4550 - val_accuracy: 0.7850 Epoch 3/10 36/36 [==============================] - 9s 259ms/step - loss: 0.3579 - accuracy: 0.8429 - val_loss: 0.5821 - val_accuracy: 0.7550 Epoch 4/10 36/36 [==============================] - 9s 259ms/step - loss: 0.2589 - accuracy: 0.8945 - val_loss: 0.5419 - val_accuracy: 0.7770 Epoch 5/10 36/36 [==============================] - 9s 258ms/step - loss: 0.1753 - accuracy: 0.9327 - val_loss: 0.6102 - val_accuracy: 0.7345 Epoch 6/10 36/36 [==============================] - 9s 259ms/step - loss: 0.1415 - accuracy: 0.9468 - val_loss: 0.5000 - val_accuracy: 0.8165 Epoch 7/10 36/36 [==============================] - 9s 259ms/step - loss: 0.0930 - accuracy: 0.9638 - val_loss: 0.9224 - val_accuracy: 0.6855 Epoch 8/10 36/36 [==============================] - 9s 260ms/step - loss: 0.1016 - accuracy: 0.9628 - val_loss: 0.6156 - val_accuracy: 0.7845 Epoch 9/10 36/36 [==============================] - 9s 258ms/step - loss: 0.0511 - accuracy: 0.9852 - val_loss: 0.5967 - val_accuracy: 0.7960 Epoch 10/10 36/36 [==============================] - 9s 259ms/step - loss: 0.0460 - accuracy: 0.9824 - val_loss: 0.7120 - val_accuracy: 0.8050 157/157 [==============================] - 3s 22ms/step - loss: 0.6811 - accuracy: 0.8076 . [I 2020-12-04 15:58:59,365] Trial 0 finished with value: 0.6811148524284363 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.22504036672028457, &#39;units2&#39;: 32, &#39;lr&#39;: 0.008674885296861211}. Best is trial 0 with value: 0.6811148524284363. . Epoch 1/10 36/36 [==============================] - 14s 385ms/step - loss: 0.6923 - accuracy: 0.5211 - val_loss: 0.6897 - val_accuracy: 0.5410 Epoch 2/10 36/36 [==============================] - 10s 274ms/step - loss: 0.6286 - accuracy: 0.6923 - val_loss: 0.4985 - val_accuracy: 0.7650 Epoch 3/10 36/36 [==============================] - 10s 274ms/step - loss: 0.4178 - accuracy: 0.8182 - val_loss: 0.4259 - val_accuracy: 0.8110 Epoch 4/10 36/36 [==============================] - 10s 273ms/step - loss: 0.3308 - accuracy: 0.8665 - val_loss: 0.3624 - val_accuracy: 0.8455 Epoch 5/10 36/36 [==============================] - 10s 273ms/step - loss: 0.2694 - accuracy: 0.8977 - val_loss: 0.3391 - val_accuracy: 0.8675 Epoch 6/10 36/36 [==============================] - 10s 272ms/step - loss: 0.2275 - accuracy: 0.9151 - val_loss: 0.3360 - val_accuracy: 0.8775 Epoch 7/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1878 - accuracy: 0.9328 - val_loss: 0.4527 - val_accuracy: 0.8445 Epoch 8/10 36/36 [==============================] - 10s 274ms/step - loss: 0.1630 - accuracy: 0.9429 - val_loss: 0.3379 - val_accuracy: 0.8790 Epoch 9/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1361 - accuracy: 0.9536 - val_loss: 0.3432 - val_accuracy: 0.8775 Epoch 10/10 36/36 [==============================] - 10s 275ms/step - loss: 0.1099 - accuracy: 0.9637 - val_loss: 0.3591 - val_accuracy: 0.8590 157/157 [==============================] - 4s 23ms/step - loss: 0.3803 - accuracy: 0.8502 . [I 2020-12-04 16:01:04,098] Trial 1 finished with value: 0.38028624653816223 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.3706454194859867, &#39;units2&#39;: 64, &#39;lr&#39;: 0.00016189924268387887}. Best is trial 1 with value: 0.38028624653816223. . Epoch 1/10 36/36 [==============================] - 13s 374ms/step - loss: 0.6136 - accuracy: 0.6732 - val_loss: 0.4087 - val_accuracy: 0.8200 Epoch 2/10 36/36 [==============================] - 10s 274ms/step - loss: 0.3540 - accuracy: 0.8510 - val_loss: 0.4048 - val_accuracy: 0.8260 Epoch 3/10 36/36 [==============================] - 10s 274ms/step - loss: 0.2343 - accuracy: 0.9084 - val_loss: 0.3642 - val_accuracy: 0.8550 Epoch 4/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1784 - accuracy: 0.9336 - val_loss: 0.3657 - val_accuracy: 0.8660 Epoch 5/10 36/36 [==============================] - 10s 273ms/step - loss: 0.1308 - accuracy: 0.9510 - val_loss: 0.4816 - val_accuracy: 0.8545 Epoch 6/10 36/36 [==============================] - 10s 272ms/step - loss: 0.0933 - accuracy: 0.9664 - val_loss: 0.4857 - val_accuracy: 0.7950 Epoch 7/10 36/36 [==============================] - 10s 273ms/step - loss: 0.0546 - accuracy: 0.9820 - val_loss: 0.5534 - val_accuracy: 0.8635 Epoch 8/10 36/36 [==============================] - 10s 274ms/step - loss: 0.0463 - accuracy: 0.9835 - val_loss: 0.5407 - val_accuracy: 0.8580 Epoch 9/10 36/36 [==============================] - 10s 273ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 0.5886 - val_accuracy: 0.8555 Epoch 10/10 36/36 [==============================] - 10s 274ms/step - loss: 0.0321 - accuracy: 0.9884 - val_loss: 0.5884 - val_accuracy: 0.8580 157/157 [==============================] - 4s 22ms/step - loss: 0.5836 - accuracy: 0.8582 . [I 2020-12-04 16:03:08,472] Trial 2 finished with value: 0.5836058855056763 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.1275607257989057, &#39;units2&#39;: 64, &#39;lr&#39;: 0.0020301932674781486}. Best is trial 1 with value: 0.38028624653816223. . Epoch 1/10 36/36 [==============================] - 19s 514ms/step - loss: 0.6922 - accuracy: 0.5332 - val_loss: 0.6908 - val_accuracy: 0.5860 Epoch 2/10 36/36 [==============================] - 15s 415ms/step - loss: 0.6754 - accuracy: 0.6570 - val_loss: 0.5943 - val_accuracy: 0.7025 Epoch 3/10 36/36 [==============================] - 15s 414ms/step - loss: 0.5075 - accuracy: 0.7626 - val_loss: 0.4557 - val_accuracy: 0.7970 Epoch 4/10 36/36 [==============================] - 15s 415ms/step - loss: 0.3980 - accuracy: 0.8305 - val_loss: 0.4485 - val_accuracy: 0.8050 Epoch 5/10 36/36 [==============================] - 15s 412ms/step - loss: 0.3251 - accuracy: 0.8691 - val_loss: 0.4432 - val_accuracy: 0.8120 Epoch 6/10 36/36 [==============================] - 15s 417ms/step - loss: 0.2710 - accuracy: 0.8966 - val_loss: 0.3585 - val_accuracy: 0.8470 Epoch 7/10 36/36 [==============================] - 15s 415ms/step - loss: 0.2351 - accuracy: 0.9135 - val_loss: 0.3566 - val_accuracy: 0.8560 Epoch 8/10 36/36 [==============================] - 15s 414ms/step - loss: 0.1995 - accuracy: 0.9301 - val_loss: 0.4250 - val_accuracy: 0.8350 Epoch 9/10 36/36 [==============================] - 15s 415ms/step - loss: 0.1723 - accuracy: 0.9423 - val_loss: 0.3400 - val_accuracy: 0.8590 Epoch 10/10 36/36 [==============================] - 15s 415ms/step - loss: 0.1463 - accuracy: 0.9514 - val_loss: 0.4038 - val_accuracy: 0.8635 157/157 [==============================] - 4s 24ms/step - loss: 0.4163 - accuracy: 0.8548 . [I 2020-12-04 16:06:05,257] Trial 3 finished with value: 0.4163428843021393 and parameters: {&#39;embedding&#39;: 100, &#39;units1&#39;: 128, &#39;dropout&#39;: 0.2709290810082003, &#39;units2&#39;: 64, &#39;lr&#39;: 9.041356047814194e-05}. Best is trial 1 with value: 0.38028624653816223. . Epoch 1/10 36/36 [==============================] - 16s 451ms/step - loss: 0.6749 - accuracy: 0.6149 - val_loss: 0.5397 - val_accuracy: 0.7425 Epoch 2/10 36/36 [==============================] - 12s 338ms/step - loss: 0.4680 - accuracy: 0.7923 - val_loss: 0.7116 - val_accuracy: 0.6825 Epoch 3/10 36/36 [==============================] - 12s 338ms/step - loss: 0.4096 - accuracy: 0.8179 - val_loss: 0.4056 - val_accuracy: 0.8115 Epoch 4/10 36/36 [==============================] - 12s 338ms/step - loss: 0.2572 - accuracy: 0.8961 - val_loss: 0.4227 - val_accuracy: 0.8235 Epoch 5/10 36/36 [==============================] - 12s 338ms/step - loss: 0.1988 - accuracy: 0.9237 - val_loss: 0.7829 - val_accuracy: 0.7080 Epoch 6/10 36/36 [==============================] - 12s 338ms/step - loss: 0.1731 - accuracy: 0.9328 - val_loss: 0.4729 - val_accuracy: 0.8015 Epoch 7/10 36/36 [==============================] - 12s 337ms/step - loss: 0.1406 - accuracy: 0.9492 - val_loss: 0.8127 - val_accuracy: 0.7070 Epoch 8/10 36/36 [==============================] - 12s 336ms/step - loss: 0.1125 - accuracy: 0.9569 - val_loss: 0.5266 - val_accuracy: 0.7965 Epoch 9/10 36/36 [==============================] - 12s 338ms/step - loss: 0.0567 - accuracy: 0.9808 - val_loss: 0.5345 - val_accuracy: 0.8400 Epoch 10/10 36/36 [==============================] - 12s 337ms/step - loss: 0.0791 - accuracy: 0.9735 - val_loss: 1.2125 - val_accuracy: 0.7055 157/157 [==============================] - 4s 23ms/step - loss: 1.2458 - accuracy: 0.7060 . [I 2020-12-04 16:08:33,779] Trial 4 finished with value: 1.2457996606826782 and parameters: {&#39;embedding&#39;: 50, &#39;units1&#39;: 128, &#39;dropout&#39;: 0.3363530997614701, &#39;units2&#39;: 32, &#39;lr&#39;: 0.003724474987378308}. Best is trial 1 with value: 0.38028624653816223. . CPU times: user 7min 47s, sys: 28.5 s, total: 8min 16s Wall time: 11min 45s . studyDL.best_params . {&#39;dropout&#39;: 0.3706454194859867, &#39;embedding&#39;: 50, &#39;lr&#39;: 0.00016189924268387887, &#39;units1&#39;: 64, &#39;units2&#39;: 64} . studyDL.best_trial . FrozenTrial(number=1, value=0.38028624653816223, datetime_start=datetime.datetime(2020, 12, 4, 15, 58, 59, 366599), datetime_complete=datetime.datetime(2020, 12, 4, 16, 1, 4, 97783), params={&#39;embedding&#39;: 50, &#39;units1&#39;: 64, &#39;dropout&#39;: 0.3706454194859867, &#39;units2&#39;: 64, &#39;lr&#39;: 0.00016189924268387887}, distributions={&#39;embedding&#39;: CategoricalDistribution(choices=(50, 100)), &#39;units1&#39;: CategoricalDistribution(choices=(64, 128, 256)), &#39;dropout&#39;: LogUniformDistribution(high=0.5, low=0.1), &#39;units2&#39;: CategoricalDistribution(choices=(32, 64, 128)), &#39;lr&#39;: LogUniformDistribution(high=0.01, low=1e-05)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=1, state=TrialState.COMPLETE) . So for using pretrained embedding we used here glove and fasttext embeddign which initiliases the weights of our embedding layer and we set it trainable to false . !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip . --2020-12-04 14:32:47-- http://nlp.stanford.edu/data/glove.twitter.27B.zip Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140 Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected. HTTP request sent, awaiting response... 302 Found Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following] --2020-12-04 14:32:48-- https://nlp.stanford.edu/data/glove.twitter.27B.zip Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following] --2020-12-04 14:32:48-- http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22 Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 1520408563 (1.4G) [application/zip] Saving to: ‘glove.twitter.27B.zip’ glove.twitter.27B.z 100%[===================&gt;] 1.42G 2.18MB/s in 11m 43s 2020-12-04 14:44:31 (2.06 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563] . !unzip glove.twitter.27B.zip . Archive: glove.twitter.27B.zip inflating: glove.twitter.27B.25d.txt inflating: glove.twitter.27B.50d.txt replace glove.twitter.27B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y inflating: glove.twitter.27B.100d.txt inflating: glove.twitter.27B.200d.txt . !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip . --2020-12-04 14:48:01-- https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ... Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 681808098 (650M) [application/zip] Saving to: ‘wiki-news-300d-1M.vec.zip’ wiki-news-300d-1M.v 100%[===================&gt;] 650.22M 12.5MB/s in 53s 2020-12-04 14:48:56 (12.2 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098] . !unzip wiki-news-300d-1M.vec.zip . Archive: wiki-news-300d-1M.vec.zip inflating: wiki-news-300d-1M.vec . import numpy as np . def embeddingWeights(embeddingType,word_index): embeddings_index = {} if(embeddingType==&#39;glove&#39;): f = open(&#39;glove.twitter.27B.50d.txt&#39;) EMBEDDING_DIM=50 else: f = open(&#39;wiki-news-300d-1M.vec&#39;) EMBEDDING_DIM=300 for line in f: values = line.split() word = values[0] coefs = np.asarray(values[1:], dtype=&#39;float32&#39;) embeddings_index[word] = coefs f.close() embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM)) for word, i in word_index.items(): embedding_vector = embeddings_index.get(word) if embedding_vector is not None: embedding_matrix[i] = embedding_vector return embedding_matrix,EMBEDDING_DIM . %%time glove,emd=embeddingWeights(&#39;glove&#39;,t.word_index) . CPU times: user 17.1 s, sys: 958 ms, total: 18 s Wall time: 18 s . %%time fasttext,embd=embeddingWeights(&#39;fasttext&#39;,t.word_index) . CPU times: user 1min 5s, sys: 3.01 s, total: 1min 8s Wall time: 1min 8s . def makeLSTM(embedding,dropout,lr,units1,units2,usePretrained,embedding_matrix): inpt = Input(shape = (300,)) if(not usePretrained): embedding = Embedding(len(t.word_index) + 1,output_dim=embedding, input_length=300, mask_zero=True)(inpt) else: embedding = Embedding(len(t.word_index) + 1,output_dim=embedding, input_length=300, mask_zero=True,weights=[embedding_matrix],trainable=False)(inpt) LSTM1=Bidirectional(LSTM(units=units1,return_sequences=True))(embedding) drop=Dropout(rate=dropout)(LSTM1) LSTM2=Bidirectional(LSTM(units=units2,return_sequences=False))(drop) output=Dense(1, activation=&#39;sigmoid&#39;)(LSTM2) model=Model(inpt,output) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=RMSprop(lr=lr), metrics=[&#39;accuracy&#39;]) model.fit(trainTextPadded,trainDf[&#39;Label&#39;].values,validation_split=0.1,shuffle=True,batch_size=512,epochs=10,verbose=True) return model . Now we use the params we found using optuna for lstm based models. . {&#39;dropout&#39;: 0.3706454194859867, &#39;embedding&#39;: 50, &#39;lr&#39;: 0.00016189924268387887, &#39;units1&#39;: 64, &#39;units2&#39;: 64} . modelLSTM=makeLSTM(50,0.37,0.00016,64,64,False,None) . Epoch 1/10 44/44 [==============================] - 16s 370ms/step - loss: 0.6898 - accuracy: 0.5742 - val_loss: 0.6753 - val_accuracy: 0.6944 Epoch 2/10 44/44 [==============================] - 12s 283ms/step - loss: 0.5134 - accuracy: 0.7696 - val_loss: 0.4666 - val_accuracy: 0.7824 Epoch 3/10 44/44 [==============================] - 12s 283ms/step - loss: 0.3553 - accuracy: 0.8519 - val_loss: 0.4220 - val_accuracy: 0.8084 Epoch 4/10 44/44 [==============================] - 12s 283ms/step - loss: 0.2882 - accuracy: 0.8859 - val_loss: 0.3599 - val_accuracy: 0.8428 Epoch 5/10 44/44 [==============================] - 12s 282ms/step - loss: 0.2342 - accuracy: 0.9120 - val_loss: 0.3435 - val_accuracy: 0.8568 Epoch 6/10 44/44 [==============================] - 12s 284ms/step - loss: 0.1997 - accuracy: 0.9261 - val_loss: 0.3219 - val_accuracy: 0.8772 Epoch 7/10 44/44 [==============================] - 12s 284ms/step - loss: 0.1648 - accuracy: 0.9422 - val_loss: 0.3309 - val_accuracy: 0.8776 Epoch 8/10 44/44 [==============================] - 12s 283ms/step - loss: 0.1391 - accuracy: 0.9528 - val_loss: 0.3400 - val_accuracy: 0.8784 Epoch 9/10 44/44 [==============================] - 12s 283ms/step - loss: 0.1175 - accuracy: 0.9609 - val_loss: 0.4453 - val_accuracy: 0.8628 Epoch 10/10 44/44 [==============================] - 12s 283ms/step - loss: 0.0993 - accuracy: 0.9683 - val_loss: 0.3774 - val_accuracy: 0.8764 . modelLSTM.evaluate(testTextPadded,testDf[&#39;Label&#39;]) . 782/782 [==============================] - 18s 22ms/step - loss: 0.4298 - accuracy: 0.8522 . [0.42976242303848267, 0.8522400259971619] . modelLSTMGlove=makeLSTM(50,0.37,0.00016,64,64,True,glove) . Epoch 1/10 44/44 [==============================] - 14s 317ms/step - loss: 0.6733 - accuracy: 0.5908 - val_loss: 0.6399 - val_accuracy: 0.6396 Epoch 2/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5996 - accuracy: 0.6828 - val_loss: 0.5484 - val_accuracy: 0.7224 Epoch 3/10 44/44 [==============================] - 10s 235ms/step - loss: 0.5605 - accuracy: 0.7183 - val_loss: 0.5308 - val_accuracy: 0.7412 Epoch 4/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5487 - accuracy: 0.7259 - val_loss: 0.5198 - val_accuracy: 0.7500 Epoch 5/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5344 - accuracy: 0.7363 - val_loss: 0.5922 - val_accuracy: 0.6804 Epoch 6/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5264 - accuracy: 0.7410 - val_loss: 0.5494 - val_accuracy: 0.7336 Epoch 7/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5217 - accuracy: 0.7460 - val_loss: 0.5292 - val_accuracy: 0.7356 Epoch 8/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5115 - accuracy: 0.7504 - val_loss: 0.5728 - val_accuracy: 0.7120 Epoch 9/10 44/44 [==============================] - 10s 235ms/step - loss: 0.5113 - accuracy: 0.7527 - val_loss: 0.4952 - val_accuracy: 0.7668 Epoch 10/10 44/44 [==============================] - 10s 236ms/step - loss: 0.5005 - accuracy: 0.7584 - val_loss: 0.4992 - val_accuracy: 0.7672 . modelLSTMGlove.evaluate(testTextPadded,testDf[&#39;Label&#39;]) . 782/782 [==============================] - 18s 23ms/step - loss: 0.5002 - accuracy: 0.7660 . [0.5001837015151978, 0.766040027141571] . Keeping all the parameters same as that of simple lstm(flexible embedding layers)vs glove embedding layer we see the drop in performance which is quite explainable by the fact that text used train glove is different from the text here (even tho its text from twitter). Moreover can also see that the model takes more time to fit (train accuracy) but this could improve with more epoch or using high dimensional embedding of glove. could also have more complicated achitechtures and more intensive hyperparameter search but for now due to resource contraint we limit ourselves. . Since the embedding size offered by fasttext is bigger it might be good idea to use more units of lstm. . modelLSTMfasttext=makeLSTM(300,0.37,0.00016,128,64,True,fasttext) . Epoch 1/10 44/44 [==============================] - 23s 517ms/step - loss: 0.6563 - accuracy: 0.6223 - val_loss: 0.7108 - val_accuracy: 0.5560 Epoch 2/10 44/44 [==============================] - 19s 439ms/step - loss: 0.5511 - accuracy: 0.7198 - val_loss: 0.4735 - val_accuracy: 0.7880 Epoch 3/10 44/44 [==============================] - 19s 438ms/step - loss: 0.4919 - accuracy: 0.7630 - val_loss: 0.4836 - val_accuracy: 0.7688 Epoch 4/10 44/44 [==============================] - 19s 437ms/step - loss: 0.4570 - accuracy: 0.7896 - val_loss: 0.5255 - val_accuracy: 0.7464 Epoch 5/10 44/44 [==============================] - 19s 438ms/step - loss: 0.4453 - accuracy: 0.7941 - val_loss: 0.5087 - val_accuracy: 0.7656 Epoch 6/10 44/44 [==============================] - 19s 437ms/step - loss: 0.4302 - accuracy: 0.8054 - val_loss: 0.4884 - val_accuracy: 0.7596 Epoch 7/10 44/44 [==============================] - 19s 436ms/step - loss: 0.4207 - accuracy: 0.8088 - val_loss: 0.4005 - val_accuracy: 0.8200 Epoch 8/10 44/44 [==============================] - 19s 438ms/step - loss: 0.4111 - accuracy: 0.8140 - val_loss: 0.3988 - val_accuracy: 0.8300 Epoch 9/10 44/44 [==============================] - 19s 438ms/step - loss: 0.3991 - accuracy: 0.8220 - val_loss: 0.3714 - val_accuracy: 0.8384 Epoch 10/10 44/44 [==============================] - 19s 437ms/step - loss: 0.3999 - accuracy: 0.8192 - val_loss: 0.3683 - val_accuracy: 0.8460 . modelLSTMfasttext.evaluate(testTextPadded,testDf[&#39;Label&#39;]) . 782/782 [==============================] - 21s 27ms/step - loss: 0.3705 - accuracy: 0.8396 . [0.37046363949775696, 0.8396000266075134] . Here we get reasonable accuracy but we had to use a slightly bigger model. But since its a bigger size vectors it can accomodate somehow. But we can see still wiki news contains corpus which is different from casual commenting on imdb by people. . contrasting performance of deep learning models with bag of words (fits faster to data aswell) model its interesting to notice that maybe indivisual words might play more role in determing the sentiment as compared to the full sequential structure. but one cannot say that with full certainity since didnt train our models for long enough neither explored more trials of hyperparameters. . BERT BASED . So we try 2 variants of bert based models first by using distil bert from hugging face an then using ktrain to train the bigger bert. . !pip install transformers . Collecting transformers Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB) |████████████████████████████████| 1.4MB 5.6MB/s Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 19.9MB/s Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4) Collecting tokenizers==0.9.4 Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB) |████████████████████████████████| 2.9MB 28.2MB/s Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (1.15.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers) (0.17.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers) (2.4.7) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (2020.11.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers) (3.0.4) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a1ec9e5910bcec29fdd40a087b5167a0a7839887e10711dce208e8319dfc82e2 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sacremoses Installing collected packages: sacremoses, tokenizers, transformers Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0 . from transformers import DistilBertTokenizerFast, TFDistilBertModel . We almost used unprocessed data due to the nature of bert tokenizer and also we set do lower case to true which convert data to lower case since we use distilbert base uncased . tokenizer = DistilBertTokenizerFast.from_pretrained(&#39;distilbert-base-uncased&#39;, do_lower_case=True) . . from tqdm.notebook import tqdm . bert = TFDistilBertModel.from_pretrained(&#39;distilbert-base-uncased&#39;) . . Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: [&#39;vocab_projector&#39;, &#39;activation_13&#39;, &#39;vocab_transform&#39;, &#39;vocab_layer_norm&#39;] - This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training. . bert.summary() . Model: &#34;tf_distil_bert_model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= distilbert (TFDistilBertMain multiple 66362880 ================================================================= Total params: 66,362,880 Trainable params: 66,362,880 Non-trainable params: 0 _________________________________________________________________ . For tokenisation we use fast encoder batch for distil bert which works very fast and automatically adds special tokens and does padding and truncation according to the max length. . import numpy as np . def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512): all_ids = [] for i in tqdm(range(0, len(texts), chunk_size)): text_chunk = texts[i:i+chunk_size].tolist() encs = tokenizer.batch_encode_plus(text_chunk,add_special_tokens=True,max_length=maxlen,padding=True,truncation=True) all_ids.extend(np.asarray(encs[&#39;input_ids&#39;])) return all_ids . x_train = fast_encode(trainDf[&#39;Text&#39;].astype(str), tokenizer, maxlen=300) x_train=np.stack(x_train, axis=0) . . x_test = fast_encode(testDf[&#39;Text&#39;].astype(str), tokenizer, maxlen=300) x_test=np.stack(x_test, axis=0) . . y_train = trainDf[&#39;Label&#39;].values y_test = testDf[&#39;Label&#39;].values . from transformers import AdamWeightDecay,get_cosine_schedule_with_warmup . def modelDBERT(): input = Input(shape=(300,),dtype=&#39;int32&#39;) bert.trainable=True bertOp = bert(input)[0] cls_token = bertOp[:, 0, :] dropout=Dropout(0.1)(cls_token) dense1=Dense(128,activation=&#39;relu&#39;)(dropout) out = Dense(1, activation=&#39;sigmoid&#39;)(dense1) model = Model(inputs=input, outputs=out) model.compile(Adam(lr=3e-5), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) return model . Here we just add a bit of dropout to somehow avoid overfiting and a dense layer to fine tune. . modelbert=modelDBERT() . modelbert.summary() . Model: &#34;functional_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 300)] 0 _________________________________________________________________ tf_distil_bert_model (TFDist multiple 66362880 _________________________________________________________________ tf_op_layer_strided_slice_2 [(None, 768)] 0 _________________________________________________________________ dropout_21 (Dropout) (None, 768) 0 _________________________________________________________________ dense_4 (Dense) (None, 128) 98432 _________________________________________________________________ dense_5 (Dense) (None, 1) 129 ================================================================= Total params: 66,461,441 Trainable params: 66,461,441 Non-trainable params: 0 _________________________________________________________________ . modelbert.fit(x_train,y_train,validation_split=0.1,epochs=1,batch_size=16,verbose=1,shuffle=True) . WARNING:tensorflow:Model was constructed with shape (None, 300) for input Tensor(&#34;input_3:0&#34;, shape=(None, 300), dtype=int32), but it was called on an input with incompatible shape (None, 400). WARNING:tensorflow:Model was constructed with shape (None, 300) for input Tensor(&#34;input_3:0&#34;, shape=(None, 300), dtype=int32), but it was called on an input with incompatible shape (None, 400). 1407/1407 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.8800WARNING:tensorflow:Model was constructed with shape (None, 300) for input Tensor(&#34;input_3:0&#34;, shape=(None, 300), dtype=int32), but it was called on an input with incompatible shape (None, 400). 1407/1407 [==============================] - 587s 418ms/step - loss: 0.2798 - accuracy: 0.8800 - val_loss: 0.1992 - val_accuracy: 0.9228 . &lt;tensorflow.python.keras.callbacks.History at 0x7f1180c0efd0&gt; . modelbert.evaluate(x_test,y_test) . 782/782 [==============================] - 196s 250ms/step - loss: 0.1960 - accuracy: 0.9227 . [0.19601084291934967, 0.9226800203323364] . Accuracy wise we get the best results for bert. But one thing which i noticed was that pure fine tuning (freezing the bert layer) and tuning extra added layers doesnt work very well. what works is we update the whole model with a small learning rate so that the weights of bert are not disturbed much .",
            "url": "https://ashish244co.github.io/blog/nlp/deep%20learning/machine%20learning/jupyter/2020/12/30/sentiment-classification-IMDB.html",
            "relUrl": "/nlp/deep%20learning/machine%20learning/jupyter/2020/12/30/sentiment-classification-IMDB.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Electricity Price Prediction",
            "content": "Import Data . !wget &quot;https://frankfurt-school-dataset.s3.eu-central-1.amazonaws.com/Electricity_data_hourly_products.csv&quot; . --2020-11-08 17:28:57-- https://frankfurt-school-dataset.s3.eu-central-1.amazonaws.com/Electricity_data_hourly_products.csv Resolving frankfurt-school-dataset.s3.eu-central-1.amazonaws.com (frankfurt-school-dataset.s3.eu-central-1.amazonaws.com)... 52.219.74.144 Connecting to frankfurt-school-dataset.s3.eu-central-1.amazonaws.com (frankfurt-school-dataset.s3.eu-central-1.amazonaws.com)|52.219.74.144|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 350029849 (334M) [text/csv] Saving to: ‘Electricity_data_hourly_products.csv’ Electricity_data_ho 100%[===================&gt;] 333.81M 29.4MB/s in 12s 2020-11-08 17:29:10 (27.6 MB/s) - ‘Electricity_data_hourly_products.csv’ saved [350029849/350029849] . !ls -lh . total 334M -rw-r--r-- 1 root root 334M Sep 30 08:49 Electricity_data_hourly_products.csv drwxr-xr-x 1 root root 4.0K Oct 28 16:30 sample_data . import pandas as pd df = pd.read_csv(&#39;Electricity_data_hourly_products.csv&#39;, index_col=0) # Display the first 5 rows of the data; for a description of the content, see the text below this cell display(df.head()) # Display basic dataframe info df.info() . contractId qty px . Datetime . 2020-03-01 11:59:13.229 11629792 | 0.5 | -0.99 | . 2020-03-01 11:59:13.243 11629792 | 0.5 | -0.99 | . 2020-03-01 11:59:13.243 11629792 | 0.1 | -1.00 | . 2020-03-01 11:59:46.669 11629792 | 3.0 | -0.99 | . 2020-03-01 11:59:55.065 11629792 | 3.0 | -0.99 | . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 8219996 entries, 2020-03-01 11:59:13.229 to 2020-08-24 20:42:39.432 Data columns (total 3 columns): # Column Dtype -- 0 contractId int64 1 qty float64 2 px float64 dtypes: float64(2), int64(1) memory usage: 250.9+ MB . !pip install seglearn . Collecting seglearn Downloading https://files.pythonhosted.org/packages/ca/ed/434d43124650e14c5ae1f79618faa5dfd80ebc59eb7850185f7a9f5fbb2d/seglearn-1.2.1-py3-none-any.whl (11.3MB) |████████████████████████████████| 11.3MB 2.5MB/s Requirement already satisfied: scikit-learn&gt;=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seglearn) (0.22.2.post1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from seglearn) (1.18.5) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from seglearn) (1.4.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn&gt;=0.21.3-&gt;seglearn) (0.17.0) Installing collected packages: seglearn Successfully installed seglearn-1.2.1 . import seaborn as sns ! pip install statsmodels from statsmodels.graphics.tsaplots import plot_acf, plot_pacf . Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (0.10.2) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.5.1) Requirement already satisfied: scipy&gt;=0.18 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.4.1) Requirement already satisfied: numpy&gt;=1.11 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.18.5) Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.1.4) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy&gt;=0.4.0-&gt;statsmodels) (1.15.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19-&gt;statsmodels) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19-&gt;statsmodels) (2018.9) . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . Visualizations . Partial - Autocorrelation Plots . We take contractID 11762694 as an example for visualization purposes . df1=df[df[&#39;contractId&#39;] == 11762694] . Price Plot . import matplotlib.pyplot as plt import numpy as np plot_pacf(df1[&#39;px&#39;].values,lags=40,zero=False) plt.xlabel(&#39;Measurement points back in time-relatively&#39;) plt.figure(figsize=(10,10), linewidth=1) plt.show() . &lt;Figure size 720x720 with 0 Axes&gt; . We plot partial autocorrelations to summarize the relationship between px values at a given time in the time series with its obersvations at prior time steps. It appears that the first 7 lags are likely correlated and not simply a statistical fluke. However, the px values do not suggest a strong partial autocorrelation, with approx. 0.2 of the first lag being the maximum value. . The plot for the quantity traded . plot_pacf(df1[&#39;qty&#39;].values,lags=40,zero=False) plt.xlabel(&#39;Measurement points back in time-relatively&#39;) plt.figure(figsize=(10,10), linewidth=1) plt.show() . &lt;Figure size 720x720 with 0 Axes&gt; . No strong correlations can be seen from the lags of the quantity qx variable. . df2=df[df[&#39;contractId&#39;] == 11877196] . import matplotlib.pyplot as plt import numpy as np plot_pacf(df2[&#39;px&#39;].values,lags=40,zero=False) plt.xlabel(&#39;Measurement points back in time-relatively&#39;) plt.figure(figsize=(10,10), linewidth=1) plt.show() . &lt;Figure size 720x720 with 0 Axes&gt; . For contractID 11877196 higer correlations of the first lagged variables are observable, with its first lag having an autocorrelation of above 0.5 . The Plot for quantity traded . plot_pacf(df2[&#39;qty&#39;].values,lags=40,zero=False) plt.xlabel(&#39;Measurement points back in time-relatively&#39;) plt.figure(figsize=(10,10), linewidth=1) plt.show() . &lt;Figure size 720x720 with 0 Axes&gt; . Here, we see that for the quantity variable of the given contractID, the autocorrlations are statistically relevant. However, the first lag&#39;s autocorrelation is still below 0.1 and subsequent autocorrelations are even less. . Generally, in this multi-dimensional time series dataset we see that autocorrelations differ for each contractID. . Data Preprocessing . from seglearn.transform import FeatureRep, SegmentXYForecast, last from sklearn.model_selection import train_test_split import numpy as np from functools import partial def bin_ohlcv(df, contractId, binning_size=&#39;H&#39;): df_cid = df[df.contractId == contractId] # resample for a binsize and the ohlc the result; and volume too. data = df_cid[[&#39;px&#39;]].resample(binning_size).ohlc().px data[&#39;volsum&#39;] = df_cid[[&#39;qty&#39;]].resample(binning_size).sum() data[&#39;contractId&#39;] = contractId return data def fillna_close(df): for index, row in df.iterrows(): if not np.isnan(row[&#39;close&#39;]): close = row[&#39;close&#39;] else: df.loc[index,&#39;open&#39;] = close df.loc[index,&#39;high&#39;] = close df.loc[index,&#39;low&#39;] = close df.loc[index,&#39;close&#39;] = close return df . window_size = 5 . df . contractId qty px . Datetime . 2020-03-01 11:59:13.229 11629792 | 0.5 | -0.99 | . 2020-03-01 11:59:13.243 11629792 | 0.5 | -0.99 | . 2020-03-01 11:59:13.243 11629792 | 0.1 | -1.00 | . 2020-03-01 11:59:46.669 11629792 | 3.0 | -0.99 | . 2020-03-01 11:59:55.065 11629792 | 3.0 | -0.99 | . ... ... | ... | ... | . 2020-08-25 07:47:18.102 11877196 | 1.6 | 38.53 | . 2020-08-24 19:56:11.583 11879762 | 4.5 | 38.95 | . 2020-08-24 20:42:39.432 11879762 | 10.5 | 38.95 | . 2020-08-24 19:56:11.583 11879763 | 11.0 | 34.95 | . 2020-08-24 20:42:39.432 11879813 | 52.5 | 37.98 | . 8219996 rows × 3 columns . The modeling process can be improved by augmenting the information fed into the algorithm. We encode the dataframe by months, days, hours and day of the week to show tha we are dealing with a cyclical time series. This may reveal hidden patterns in the dataset that cannot be revealed with the regular dataframe. . def encode_cyclicals(df_x): #&quot;month&quot;,&quot;day&quot;,&quot;hour&quot;, &quot;cdbw&quot;, &quot;dayofweek&quot; df_x[&#39;month_sin&#39;] = np.sin(2*np.pi*df_x.month/12) df_x[&#39;month_cos&#39;] = np.cos(2*np.pi*df_x.month/12) df_x.drop(&#39;month&#39;, axis=1, inplace=True) df_x[&#39;day_sin&#39;] = np.sin(2*np.pi*df_x.day/31) df_x[&#39;day_cos&#39;] = np.cos(2*np.pi*df_x.day/31) df_x.drop(&#39;day&#39;, axis=1, inplace=True) df_x[&#39;dayofweek_sin&#39;] = np.sin(2*np.pi*df_x.dayofweek/7) df_x[&#39;dayofweek_cos&#39;] = np.cos(2*np.pi*df_x.dayofweek/7) df_x.drop(&#39;dayofweek&#39;, axis=1, inplace=True) df_x[&#39;hour_sin&#39;] = np.sin(2*np.pi*df_x.hour/24) df_x[&#39;hour_cos&#39;] = np.cos(2*np.pi*df_x.hour/24) df_x.drop(&#39;hour&#39;, axis=1, inplace=True) return df_x . def preproc(df, window_size, forecast_distance=1): df.index=pd.to_datetime(df.index) contractIds = df.contractId.unique() segmenter = SegmentXYForecast(width=window_size, step=1, y_func=last, forecast=forecast_distance) columns = [&#39;contractId&#39;,&#39;y&#39;,]+[ x for x in range(window_size*5)] train_df = pd.DataFrame(columns=columns) train_rolled_df = [] label_rolled_df=[] error_cids = pd.DataFrame() for cid in (contractIds): X_train = fillna_close(bin_ohlcv(df, cid)) X_train[&#39;month&#39;]=X_train.index.month X_train[&#39;day&#39;]=X_train.index.day X_train[&#39;dayofweek&#39;]=X_train.index.dayofweek X_train[&#39;hour&#39;]=X_train.index.hour X_train=encode_cyclicals(X_train) if X_train.shape[0] &lt; window_size+forecast_distance: error_cids[str(cid)] = X_train.shape[0] else: y_train = X_train[&#39;close&#39;] X_train = X_train.drop([&#39;contractId&#39;,&#39;close&#39;], axis=1) X_train_rolled, y_train_rolled, _= segmenter.fit_transform([X_train.values],[y_train.values.flatten()]) train_rolled_df.append(X_train_rolled) label_rolled_df.append(y_train_rolled) count = 0 for i in X_train_rolled: data = pd.Series(i.flatten()) train_df = train_df.append(data, ignore_index=True) train_df.iloc[-1,0] = str(cid) train_df.iloc[-1,1] = y_train_rolled[count] count = count+1 return train_rolled_df,label_rolled_df,train_df, error_cids . df . contractId qty px . Datetime . 2020-03-01 11:59:13.229 11629792 | 0.5 | -0.99 | . 2020-03-01 11:59:13.243 11629792 | 0.5 | -0.99 | . 2020-03-01 11:59:13.243 11629792 | 0.1 | -1.00 | . 2020-03-01 11:59:46.669 11629792 | 3.0 | -0.99 | . 2020-03-01 11:59:55.065 11629792 | 3.0 | -0.99 | . ... ... | ... | ... | . 2020-08-25 07:47:18.102 11877196 | 1.6 | 38.53 | . 2020-08-24 19:56:11.583 11879762 | 4.5 | 38.95 | . 2020-08-24 20:42:39.432 11879762 | 10.5 | 38.95 | . 2020-08-24 19:56:11.583 11879763 | 11.0 | 34.95 | . 2020-08-24 20:42:39.432 11879813 | 52.5 | 37.98 | . 8219996 rows × 3 columns . train_rolled_df,label_rolled_df,train_df, error_cids = preproc(df,window_size) . label_rolled_df[0] . array([26.91, 23.7 , 23.69, 21. , 23.41]) . np.concatenate(label_rolled_df,axis=0) # Preprocessing for LSTM . array([26.91, 23.7 , 23.69, ..., 36.9 , 35. , 38.53]) . np.vstack(train_rolled_df).shape . (44548, 5, 12) . train_rolled_df=np.vstack(train_rolled_df) label_rolled_df=np.concatenate(label_rolled_df,axis=0) . train_df.head() . contractId y 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 . 0 11630300 | 26.91 | 29.99 | 30.00 | 29.99 | 5.2 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | 2.588190e-01 | -0.965926 | 30.02 | 30.80 | 28.82 | 99.9 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | 1.224647e-16 | -1.000000 | 20.51 | 28.82 | 20.51 | 279.5 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.258819 | -0.965926 | 25.99 | 31.40 | 25.90 | 469.8 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.500000 | -8.660254e-01 | 30.40 | 31.19 | 27.20 | 451.4 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.707107 | -7.071068e-01 | . 1 11630300 | 23.70 | 30.02 | 30.80 | 28.82 | 99.9 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | 1.224647e-16 | -1.000000 | 20.51 | 28.82 | 20.51 | 279.5 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -2.588190e-01 | -0.965926 | 25.99 | 31.40 | 25.90 | 469.8 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.500000 | -0.866025 | 30.40 | 31.19 | 27.20 | 451.4 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.707107 | -7.071068e-01 | 30.09 | 30.99 | 26.54 | 381.6 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.866025 | -5.000000e-01 | . 2 11630300 | 23.69 | 20.51 | 28.82 | 20.51 | 279.5 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -2.588190e-01 | -0.965926 | 25.99 | 31.40 | 25.90 | 469.8 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -5.000000e-01 | -0.866025 | 30.40 | 31.19 | 27.20 | 451.4 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.707107 | -0.707107 | 30.09 | 30.99 | 26.54 | 381.6 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.866025 | -5.000000e-01 | 26.60 | 28.40 | 18.57 | 1245.9 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.965926 | -2.588190e-01 | . 3 11630300 | 21.00 | 25.99 | 31.40 | 25.90 | 469.8 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -5.000000e-01 | -0.866025 | 30.40 | 31.19 | 27.20 | 451.4 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -7.071068e-01 | -0.707107 | 30.09 | 30.99 | 26.54 | 381.6 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.866025 | -0.500000 | 26.60 | 28.40 | 18.57 | 1245.9 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.965926 | -2.588190e-01 | 23.70 | 26.71 | 21.01 | 947.0 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -1.000000 | -1.836970e-16 | . 4 11630300 | 23.41 | 30.40 | 31.19 | 27.20 | 451.4 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -7.071068e-01 | -0.707107 | 30.09 | 30.99 | 26.54 | 381.6 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -8.660254e-01 | -0.500000 | 26.60 | 28.40 | 18.57 | 1245.9 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.965926 | -0.258819 | 23.70 | 26.71 | 21.01 | 947.0 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -1.000000 | -1.836970e-16 | 23.00 | 25.60 | 19.90 | 1213.6 | 1.0 | 6.123234e-17 | 0.201299 | 0.97953 | -0.781831 | 0.62349 | -0.965926 | 2.588190e-01 | . Data Split . split_size = .1 # Creating train and test splits X_train, X_test, y_train, y_test = train_test_split(train_df.drop([&#39;contractId&#39;,&#39;y&#39;],axis=1),label_rolled_df, test_size=split_size, shuffle=False) # Creating by splitting from remaning data of train X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=split_size, shuffle=False) mean=np.mean(X_train, axis=0) std=np.std(X_train, axis=0) X_train=(X_train-mean)/std X_valid=(X_valid-mean)/std X_test=(X_test-mean)/std . Here we process data in two types one flattened for classic supervised machine learning algos and other for deep learning algorithms. . We start with linear regression then gradually try more complex algorithms like random forest and xgboost and record their scores. . Classic ML Algorithms . from sklearn.ensemble import RandomForestRegressor from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score, precision_score, confusion_matrix, recall_score, accuracy_score from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import MinMaxScaler %precision 4 . &#39;%.4f&#39; . Dummy Predictor . from sklearn.dummy import DummyRegressor dummy_model = DummyRegressor(strategy=&#39;mean&#39;) dummy_model.fit(X_train, y_train) . DummyRegressor(constant=None, quantile=None, strategy=&#39;mean&#39;) . from sklearn.metrics import mean_squared_error,mean_absolute_error from math import sqrt def evaluate_model(model, X_valid, y_valid_true): predictions = model.predict(X_valid) rms = sqrt(mean_squared_error(y_valid_true, predictions)) mae = mean_absolute_error(y_valid_true, predictions) return rms,mae . dummyrmse,dummymae = evaluate_model(dummy_model, X_test, y_test) . Linear Regression . from sklearn.linear_model import LinearRegression linreg=LinearRegression() linreg.fit(X_train, y_train) linregrmse,linregmae = evaluate_model(linreg, X_test, y_test) . from sklearn.ensemble import RandomForestRegressor N_ESTIMATORS = 50 RANDOM_STATE = 452543634 . RF_base_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, max_depth=4, random_state=RANDOM_STATE,n_jobs=-1) RF_base_model.fit(X_train, y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=4, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1, oob_score=False, random_state=452543634, verbose=0, warm_start=False) . rfrmse,rfmae = evaluate_model(RF_base_model, X_test, y_test) . XGBoost . import xgboost as xgb # If in trouble, use !pip install xgboost # XGBoost needs it&#39;s custom data format to run quickly dmatrix_train = xgb.DMatrix(data=X_train,label=y_train) dmatrix_valid = xgb.DMatrix(data=X_valid,label=y_valid) dmatrix_test = xgb.DMatrix(data=X_test,label=y_test) . params = {&#39;objective&#39;: &#39;reg:linear&#39;, &#39;eval_metric&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 50} evallist = [(dmatrix_valid, &#39;eval&#39;), (dmatrix_train, &#39;train&#39;)] num_round = 40 #Can easily overfit, experiment with it! xg_reg = xgb.train(params, dmatrix_train, num_round, [(dmatrix_valid, &#39;test&#39;), (dmatrix_train, &#39;train&#39;)]) . [17:38:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [0] test-rmse:26.1554 train-rmse:21.2551 [1] test-rmse:18.8739 train-rmse:15.5145 [2] test-rmse:13.7881 train-rmse:11.6365 [3] test-rmse:10.4652 train-rmse:9.09538 [4] test-rmse:8.32127 train-rmse:7.49158 [5] test-rmse:7.06419 train-rmse:6.51137 [6] test-rmse:6.3734 train-rmse:5.92332 [7] test-rmse:6.0122 train-rmse:5.59236 [8] test-rmse:5.812 train-rmse:5.37985 [9] test-rmse:5.70591 train-rmse:5.2242 [10] test-rmse:5.67404 train-rmse:5.12481 [11] test-rmse:5.63926 train-rmse:5.04321 [12] test-rmse:5.63908 train-rmse:4.9943 [13] test-rmse:5.63276 train-rmse:4.95529 [14] test-rmse:5.64107 train-rmse:4.89816 [15] test-rmse:5.64481 train-rmse:4.8419 [16] test-rmse:5.64753 train-rmse:4.82373 [17] test-rmse:5.65081 train-rmse:4.7665 [18] test-rmse:5.65059 train-rmse:4.74386 [19] test-rmse:5.65369 train-rmse:4.72363 [20] test-rmse:5.65731 train-rmse:4.6661 [21] test-rmse:5.65727 train-rmse:4.66012 [22] test-rmse:5.67063 train-rmse:4.64217 [23] test-rmse:5.67136 train-rmse:4.62821 [24] test-rmse:5.67135 train-rmse:4.5893 [25] test-rmse:5.67206 train-rmse:4.57615 [26] test-rmse:5.67512 train-rmse:4.55667 [27] test-rmse:5.6736 train-rmse:4.53339 [28] test-rmse:5.67358 train-rmse:4.51604 [29] test-rmse:5.68321 train-rmse:4.49139 [30] test-rmse:5.68168 train-rmse:4.47806 [31] test-rmse:5.68406 train-rmse:4.44631 [32] test-rmse:5.68343 train-rmse:4.44191 [33] test-rmse:5.67943 train-rmse:4.42157 [34] test-rmse:5.6802 train-rmse:4.38615 [35] test-rmse:5.68671 train-rmse:4.36027 [36] test-rmse:5.68674 train-rmse:4.33193 [37] test-rmse:5.68674 train-rmse:4.32344 [38] test-rmse:5.68641 train-rmse:4.31968 [39] test-rmse:5.69402 train-rmse:4.31066 . xgboostrmse=np.sqrt(1/len(X_test)*np.sum((xg_reg.predict(dmatrix_test)-y_test)**2,axis=0)) xgboostmae=np.sqrt(1/len(X_test)*np.sum(abs(xg_reg.predict(dmatrix_test)-y_test),axis=0)) . split_size = .1 # Creating train and test splits X_train, X_test, y_train, y_test = train_test_split(train_rolled_df,label_rolled_df, test_size=split_size, shuffle=False) # Creating by splitting from remaning data of train X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=split_size, shuffle=False) mean=np.mean(X_train, axis=0) std=np.std(X_train, axis=0) X_train=(X_train-mean)/std X_valid=(X_valid-mean)/std X_test=(X_test-mean)/std . X_train.shape . (36083, 5, 12) . X_valid.shape . (4010, 5, 12) . X_test.shape . (4455, 5, 12) . Deep Learning . For deep learning models we try to be creative and use 1D CNN which is also well suited for time series problems. Then we try a GRU since its much simpler than LSTM which lesser parameters. We add attention to it to make it work better. The models somewhat underfit but still gives us reasonable score with very few parameters. . X_train.shape . (36083, 5, 12) . Sincee Huber loss is a loss function is less sensitive to outliers in data we use it here. Moreover we employ learning rate scheduling as it was used in training resnet. . !pip install attention . Collecting attention Downloading https://files.pythonhosted.org/packages/2d/27/98d7350db36a3537e24c9ec488d893b71092037f5c74e8984d01e9c1d316/attention-3.0-py3-none-any.whl Requirement already satisfied: tensorflow&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from attention) (2.3.0) Requirement already satisfied: numpy&gt;=1.18.1 in /usr/local/lib/python3.6/dist-packages (from attention) (1.18.5) Requirement already satisfied: keras&gt;=2.3.1 in /usr/local/lib/python3.6/dist-packages (from attention) (2.4.3) Requirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.1.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (3.3.0) Requirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.15.0) Requirement already satisfied: google-pasta&gt;=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (0.2.0) Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (0.10.0) Requirement already satisfied: tensorboard&lt;3,&gt;=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (2.3.0) Requirement already satisfied: grpcio&gt;=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.33.2) Requirement already satisfied: tensorflow-estimator&lt;2.4.0,&gt;=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (2.3.0) Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.4.1) Requirement already satisfied: h5py&lt;2.11.0,&gt;=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (2.10.0) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (0.35.1) Requirement already satisfied: protobuf&gt;=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (3.12.4) Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.12.1) Requirement already satisfied: keras-preprocessing&lt;1.2,&gt;=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.1.2) Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (1.6.3) Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow&gt;=2.1-&gt;attention) (0.3.3) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras&gt;=2.3.1-&gt;attention) (3.13) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (1.17.2) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (1.0.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (50.3.2) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (2.23.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (0.4.2) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (1.7.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (3.3.3) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (4.6) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (4.1.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (2020.6.20) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (1.24.3) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (1.3.0) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (2.0.0) Requirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3&#34;-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (0.4.8) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (3.1.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;markdown&gt;=2.6.8-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow&gt;=2.1-&gt;attention) (3.4.0) Installing collected packages: attention Successfully installed attention-3.0 . from tensorflow.keras.layers import Dense, Dropout, LSTM,GRU,Conv1D,MaxPooling1D,Flatten,GlobalMaxPooling1D from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense, Dropout, LSTM,TimeDistributed,Bidirectional from tensorflow.keras import backend as be from tensorflow.keras.optimizers import Adam from attention import Attention from tensorflow.keras.losses import Huber import matplotlib.pyplot as plt . y_train.shape . (36083,) . X_train.shape . (36083, 5, 12) . reduceLR=ReduceLROnPlateau( monitor=&quot;val_mae&quot;, factor=0.1, patience=5, verbose=2, mode=&quot;auto&quot;, min_delta=0.0001, cooldown=0, min_lr=0.0000000001 ) . X_train.shape . (36083, 5, 12) . model = Sequential() model.add(Conv1D(filters=64, kernel_size=3, activation=&#39;relu&#39;, padding=&quot;same&quot; ,input_shape=(window_size, X_train.shape[2]))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dropout(0.3)) model.add(Dense(1,&#39;linear&#39;)) model.summary() model.compile(loss=Huber(), optimizer=Adam(lr=3e-4),metrics=[&quot;mae&quot;]) history = model.fit(X_train, y_train, epochs=40, batch_size=128,shuffle=True, validation_data=(X_valid,y_valid) ,callbacks =[reduceLR]) . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv1d (Conv1D) (None, 5, 64) 2368 _________________________________________________________________ max_pooling1d (MaxPooling1D) (None, 2, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 128) 0 _________________________________________________________________ dense (Dense) (None, 128) 16512 _________________________________________________________________ dropout (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 129 ================================================================= Total params: 19,009 Trainable params: 19,009 Non-trainable params: 0 _________________________________________________________________ Epoch 1/40 282/282 [==============================] - 1s 5ms/step - loss: 11.4584 - mae: 11.9420 - val_loss: 4.0743 - val_mae: 4.5548 Epoch 2/40 282/282 [==============================] - 1s 4ms/step - loss: 4.4267 - mae: 4.8948 - val_loss: 3.4966 - val_mae: 3.9718 Epoch 3/40 282/282 [==============================] - 1s 4ms/step - loss: 3.9455 - mae: 4.4116 - val_loss: 3.4199 - val_mae: 3.8955 Epoch 4/40 282/282 [==============================] - 1s 4ms/step - loss: 3.8068 - mae: 4.2725 - val_loss: 2.9257 - val_mae: 3.3878 Epoch 5/40 282/282 [==============================] - 1s 4ms/step - loss: 3.6967 - mae: 4.1607 - val_loss: 2.8525 - val_mae: 3.3141 Epoch 6/40 282/282 [==============================] - 1s 4ms/step - loss: 3.6216 - mae: 4.0854 - val_loss: 3.0077 - val_mae: 3.4717 Epoch 7/40 282/282 [==============================] - 1s 4ms/step - loss: 3.5718 - mae: 4.0332 - val_loss: 2.6927 - val_mae: 3.1515 Epoch 8/40 282/282 [==============================] - 1s 4ms/step - loss: 3.5441 - mae: 4.0059 - val_loss: 2.5548 - val_mae: 3.0072 Epoch 9/40 282/282 [==============================] - 1s 4ms/step - loss: 3.5239 - mae: 3.9851 - val_loss: 2.6775 - val_mae: 3.1365 Epoch 10/40 282/282 [==============================] - 1s 4ms/step - loss: 3.5037 - mae: 3.9662 - val_loss: 2.7814 - val_mae: 3.2430 Epoch 11/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4775 - mae: 3.9396 - val_loss: 2.5983 - val_mae: 3.0553 Epoch 12/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4960 - mae: 3.9581 - val_loss: 2.6799 - val_mae: 3.1401 Epoch 13/40 278/282 [============================&gt;.] - ETA: 0s - loss: 3.4522 - mae: 3.9139 Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.000000142492354e-05. 282/282 [==============================] - 1s 4ms/step - loss: 3.4518 - mae: 3.9137 - val_loss: 2.5819 - val_mae: 3.0358 Epoch 14/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4490 - mae: 3.9102 - val_loss: 2.5647 - val_mae: 3.0195 Epoch 15/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4371 - mae: 3.8977 - val_loss: 2.5075 - val_mae: 2.9589 Epoch 16/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4262 - mae: 3.8865 - val_loss: 2.5936 - val_mae: 3.0497 Epoch 17/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4150 - mae: 3.8762 - val_loss: 2.5451 - val_mae: 2.9985 Epoch 18/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4087 - mae: 3.8697 - val_loss: 2.5169 - val_mae: 2.9689 Epoch 19/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4303 - mae: 3.8915 - val_loss: 2.5686 - val_mae: 3.0235 Epoch 20/40 270/282 [===========================&gt;..] - ETA: 0s - loss: 3.4036 - mae: 3.8647 Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.000000106112566e-06. 282/282 [==============================] - 1s 4ms/step - loss: 3.4089 - mae: 3.8700 - val_loss: 2.5982 - val_mae: 3.0550 Epoch 21/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4224 - mae: 3.8826 - val_loss: 2.5837 - val_mae: 3.0398 Epoch 22/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4179 - mae: 3.8792 - val_loss: 2.5681 - val_mae: 3.0231 Epoch 23/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4117 - mae: 3.8724 - val_loss: 2.5602 - val_mae: 3.0146 Epoch 24/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4037 - mae: 3.8640 - val_loss: 2.5634 - val_mae: 3.0181 Epoch 25/40 282/282 [==============================] - ETA: 0s - loss: 3.4120 - mae: 3.8719 Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.000000106112566e-07. 282/282 [==============================] - 1s 4ms/step - loss: 3.4120 - mae: 3.8719 - val_loss: 2.5730 - val_mae: 3.0282 Epoch 26/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4213 - mae: 3.8827 - val_loss: 2.5709 - val_mae: 3.0260 Epoch 27/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4212 - mae: 3.8813 - val_loss: 2.5707 - val_mae: 3.0258 Epoch 28/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4076 - mae: 3.8678 - val_loss: 2.5708 - val_mae: 3.0259 Epoch 29/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4053 - mae: 3.8667 - val_loss: 2.5709 - val_mae: 3.0260 Epoch 30/40 276/282 [============================&gt;.] - ETA: 0s - loss: 3.4108 - mae: 3.8713 Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.000000106112566e-08. 282/282 [==============================] - 1s 4ms/step - loss: 3.4201 - mae: 3.8806 - val_loss: 2.5712 - val_mae: 3.0264 Epoch 31/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4255 - mae: 3.8858 - val_loss: 2.5711 - val_mae: 3.0262 Epoch 32/40 282/282 [==============================] - 1s 5ms/step - loss: 3.4139 - mae: 3.8745 - val_loss: 2.5711 - val_mae: 3.0262 Epoch 33/40 282/282 [==============================] - 1s 5ms/step - loss: 3.4094 - mae: 3.8716 - val_loss: 2.5711 - val_mae: 3.0262 Epoch 34/40 282/282 [==============================] - 1s 5ms/step - loss: 3.4205 - mae: 3.8813 - val_loss: 2.5711 - val_mae: 3.0263 Epoch 35/40 282/282 [==============================] - ETA: 0s - loss: 3.4081 - mae: 3.8686 Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.000000248221113e-09. 282/282 [==============================] - 1s 5ms/step - loss: 3.4081 - mae: 3.8686 - val_loss: 2.5712 - val_mae: 3.0263 Epoch 36/40 282/282 [==============================] - 1s 5ms/step - loss: 3.4262 - mae: 3.8871 - val_loss: 2.5712 - val_mae: 3.0263 Epoch 37/40 282/282 [==============================] - 1s 5ms/step - loss: 3.4156 - mae: 3.8754 - val_loss: 2.5712 - val_mae: 3.0263 Epoch 38/40 282/282 [==============================] - 1s 5ms/step - loss: 3.4154 - mae: 3.8756 - val_loss: 2.5712 - val_mae: 3.0263 Epoch 39/40 282/282 [==============================] - 1s 4ms/step - loss: 3.4225 - mae: 3.8833 - val_loss: 2.5712 - val_mae: 3.0263 Epoch 40/40 278/282 [============================&gt;.] - ETA: 0s - loss: 3.4258 - mae: 3.8866 Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.000000248221113e-10. 282/282 [==============================] - 1s 4ms/step - loss: 3.4268 - mae: 3.8875 - val_loss: 2.5712 - val_mae: 3.0263 . plt.plot(history.history[&#39;mae&#39;], label=&#39;train&#39;) plt.plot(history.history[&#39;val_mae&#39;], label=&#39;test&#39;) plt.title(&#39;MAE&#39;) plt.legend() plt.show() . cnndeepLearningrmse=np.sqrt(1/len(X_test)*np.sum((model.predict(X_test).reshape(len(X_test),)-y_test)**2,axis=0)) cnndeepLearningmae=1/len(X_test)*np.sum(abs(model.predict(X_test).reshape(len(X_test),)-y_test),axis=0) . be.clear_session() model2 = Sequential() model2.add(GRU(30, activation=&#39;relu&#39;, recurrent_dropout=0.3,return_sequences=True, input_shape=(window_size, X_train.shape[2]))) model2.add(Attention()) model2.add(Dropout(0.3)) model2.add(Dense(1,&#39;linear&#39;)) model2.compile(loss=Huber(), optimizer=Adam(lr=3e-4),metrics=[&quot;mae&quot;]) model2.summary() history2 = model2.fit(X_train, y_train, epochs=30, batch_size=64,shuffle=True, validation_data=(X_valid,y_valid) ,callbacks =[reduceLR]) . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= gru (GRU) (None, 5, 30) 3960 _________________________________________________________________ attention_score_vec (Dense) (None, 5, 30) 900 _________________________________________________________________ last_hidden_state (Lambda) (None, 30) 0 _________________________________________________________________ attention_score (Dot) (None, 5) 0 _________________________________________________________________ attention_weight (Activation (None, 5) 0 _________________________________________________________________ context_vector (Dot) (None, 30) 0 _________________________________________________________________ attention_output (Concatenat (None, 60) 0 _________________________________________________________________ attention_vector (Dense) (None, 128) 7680 _________________________________________________________________ dropout (Dropout) (None, 128) 0 _________________________________________________________________ dense (Dense) (None, 1) 129 ================================================================= Total params: 12,669 Trainable params: 12,669 Non-trainable params: 0 _________________________________________________________________ Epoch 1/30 564/564 [==============================] - 4s 7ms/step - loss: 10.6730 - mae: 11.1558 - val_loss: 6.6292 - val_mae: 7.1103 Epoch 2/30 564/564 [==============================] - 4s 7ms/step - loss: 4.4253 - mae: 4.8908 - val_loss: 3.6135 - val_mae: 4.0713 Epoch 3/30 564/564 [==============================] - 4s 6ms/step - loss: 3.7659 - mae: 4.2257 - val_loss: 3.0014 - val_mae: 3.4523 Epoch 4/30 564/564 [==============================] - 4s 7ms/step - loss: 3.5577 - mae: 4.0163 - val_loss: 2.7535 - val_mae: 3.1967 Epoch 5/30 564/564 [==============================] - 4s 7ms/step - loss: 3.4494 - mae: 3.9078 - val_loss: 2.5712 - val_mae: 3.0116 Epoch 6/30 564/564 [==============================] - 4s 7ms/step - loss: 3.3806 - mae: 3.8378 - val_loss: 2.5105 - val_mae: 2.9511 Epoch 7/30 564/564 [==============================] - 4s 7ms/step - loss: 3.3318 - mae: 3.7892 - val_loss: 2.4667 - val_mae: 2.9064 Epoch 8/30 564/564 [==============================] - 4s 6ms/step - loss: 3.2760 - mae: 3.7326 - val_loss: 2.4401 - val_mae: 2.8831 Epoch 9/30 564/564 [==============================] - 4s 7ms/step - loss: 3.2568 - mae: 3.7136 - val_loss: 2.3749 - val_mae: 2.8134 Epoch 10/30 564/564 [==============================] - 4s 7ms/step - loss: 3.1981 - mae: 3.6536 - val_loss: 2.3262 - val_mae: 2.7623 Epoch 11/30 564/564 [==============================] - 4s 7ms/step - loss: 3.1597 - mae: 3.6151 - val_loss: 2.3485 - val_mae: 2.7869 Epoch 12/30 564/564 [==============================] - 4s 6ms/step - loss: 3.1562 - mae: 3.6124 - val_loss: 2.3169 - val_mae: 2.7543 Epoch 13/30 564/564 [==============================] - 4s 7ms/step - loss: 3.1261 - mae: 3.5808 - val_loss: 2.2989 - val_mae: 2.7356 Epoch 14/30 564/564 [==============================] - 4s 7ms/step - loss: 3.1200 - mae: 3.5746 - val_loss: 2.3032 - val_mae: 2.7384 Epoch 15/30 564/564 [==============================] - 4s 6ms/step - loss: 3.0942 - mae: 3.5483 - val_loss: 2.2762 - val_mae: 2.7147 Epoch 16/30 564/564 [==============================] - 4s 7ms/step - loss: 3.0971 - mae: 3.5516 - val_loss: 2.2439 - val_mae: 2.6784 Epoch 17/30 564/564 [==============================] - 4s 7ms/step - loss: 3.0863 - mae: 3.5414 - val_loss: 2.3091 - val_mae: 2.7511 Epoch 18/30 564/564 [==============================] - 4s 7ms/step - loss: 3.0653 - mae: 3.5191 - val_loss: 2.3983 - val_mae: 2.8483 Epoch 19/30 564/564 [==============================] - 4s 6ms/step - loss: 3.0462 - mae: 3.5000 - val_loss: 2.2226 - val_mae: 2.6574 Epoch 20/30 564/564 [==============================] - 4s 7ms/step - loss: 3.0245 - mae: 3.4770 - val_loss: 2.2538 - val_mae: 2.6915 Epoch 21/30 564/564 [==============================] - 4s 7ms/step - loss: 3.0125 - mae: 3.4654 - val_loss: 2.2919 - val_mae: 2.7321 Epoch 22/30 564/564 [==============================] - 4s 6ms/step - loss: 3.0107 - mae: 3.4649 - val_loss: 2.2204 - val_mae: 2.6541 Epoch 23/30 564/564 [==============================] - 4s 6ms/step - loss: 3.0105 - mae: 3.4639 - val_loss: 2.2624 - val_mae: 2.7002 Epoch 24/30 564/564 [==============================] - 4s 7ms/step - loss: 2.9889 - mae: 3.4424 - val_loss: 2.3169 - val_mae: 2.7607 Epoch 25/30 564/564 [==============================] - 4s 6ms/step - loss: 2.9979 - mae: 3.4500 - val_loss: 2.2116 - val_mae: 2.6454 Epoch 26/30 564/564 [==============================] - 4s 6ms/step - loss: 2.9782 - mae: 3.4311 - val_loss: 2.1603 - val_mae: 2.5890 Epoch 27/30 564/564 [==============================] - 4s 7ms/step - loss: 2.9734 - mae: 3.4255 - val_loss: 2.2009 - val_mae: 2.6314 Epoch 28/30 564/564 [==============================] - 4s 7ms/step - loss: 2.9614 - mae: 3.4133 - val_loss: 2.2249 - val_mae: 2.6595 Epoch 29/30 564/564 [==============================] - 4s 7ms/step - loss: 2.9460 - mae: 3.3988 - val_loss: 2.2541 - val_mae: 2.6927 Epoch 30/30 564/564 [==============================] - 4s 7ms/step - loss: 2.9502 - mae: 3.4026 - val_loss: 2.1841 - val_mae: 2.6167 . deepLearningrmse=np.sqrt(1/len(X_test)*np.sum((model2.predict(X_test).reshape(len(X_test),)-y_test)**2,axis=0)) deepLearningmae=1/len(X_test)*np.sum(abs(model2.predict(X_test).reshape(len(X_test),)-y_test),axis=0) . plt.plot(history2.history[&#39;mae&#39;], label=&#39;train&#39;) plt.plot(history2.history[&#39;val_mae&#39;], label=&#39;test&#39;) plt.title(&#39;MAE&#39;) plt.legend() plt.show() . data = {&#39;Models&#39;:[&#39;Dummy Model&#39;,&#39;Linear Regression&#39;, &#39;Random Forest&#39;, &#39;XGBoost&#39;,&#39;CNN&#39;, &#39;GRU with Attention&#39;], &#39;RMSE&#39;:[dummyrmse, linregrmse, rfrmse, xgboostrmse,cnndeepLearningrmse,deepLearningrmse], &#39;MAE&#39;:[dummymae, linregmae, rfmae, xgboostmae,cnndeepLearningmae,deepLearningmae]} df = pd.DataFrame(data) df.index=df.Models df.drop(&#39;Models&#39;,axis=1,inplace=True) . Looking at the scores we can somewhat conclude that linear regression works nice but xgboost (great because can be trained incrementally also) and deep learning models also perform really well. . df . RMSE MAE . Models . Dummy Model 18.179135 | 14.956636 | . Linear Regression 8.210400 | 4.230279 | . Random Forest 8.317027 | 3.779954 | . XGBoost 8.223609 | 1.920795 | . CNN 8.535649 | 3.918862 | . GRU with Attention 7.565562 | 3.312582 | .",
            "url": "https://ashish244co.github.io/blog/machine%20learning/deep%20learning/time%20series/jupyter/2020/10/30/Electric-pricing.html",
            "relUrl": "/machine%20learning/deep%20learning/time%20series/jupyter/2020/10/30/Electric-pricing.html",
            "date": " • Oct 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A data professional who is well-versed in Data Science, Data Engineering, Machine/Deep Learning, and Data Analysis. . . My competencies include: . Conceptual Skills . Statistics and Machine Learning . Data Analysis | A/B testing | Hypothesis testing | Regression Analysis | Supervised Learning | Unsupervised Learning | Feature Engineering | TIme series analysis and forecasting | . Natural Language Processing . Text Analysis | Topic Modeling | Text Summarisation | Named Entity Recognition | . Computer Vision . Image classification and clustering | Semantic segmentation | Object detection | . Tools . Analytical Tools . Python | SQL | Apache Spark | Google Analytics | Microsoft Excel | PowerBI | . Frameworks . Pytorch/Tensorflow | Statsmodel | Spacy | Numpy/Scipy | Pandas | Plotly | . ETL/Database Tools . Delta Lake | Azure Data Factory | Apache Airflow | Amazon Redshift | Apache Kafka | Neo4j | . MLOps tools . Docker | Kubernetes | MLFlow | Amazon Sagemaker | . Cloud Technologies . AWS | Microsoft Azure | Databricks | Salesforce Marketing Cloud | . Find more details here: LinkedIn .",
          "url": "https://ashish244co.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashish244co.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}